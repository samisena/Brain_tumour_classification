{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "wyIwW5TnyjlZ",
      "metadata": {
        "id": "wyIwW5TnyjlZ"
      },
      "source": [
        "# Importing Contigencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZEM2wpQNmyY2",
      "metadata": {
        "collapsed": true,
        "id": "ZEM2wpQNmyY2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hCbIVvFEneTA",
      "metadata": {
        "collapsed": true,
        "id": "hCbIVvFEneTA"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdba979-bc93-4446-98e7-27bc72966eac",
      "metadata": {
        "id": "2bdba979-bc93-4446-98e7-27bc72966eac"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hk_2wOvTzdbc",
      "metadata": {
        "id": "Hk_2wOvTzdbc"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbe62b3",
      "metadata": {
        "id": "2bbe62b3"
      },
      "source": [
        "# Data preprocessing\n",
        "## 1) Unzipping the MRI Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3dfc568",
      "metadata": {
        "id": "e3dfc568"
      },
      "outputs": [],
      "source": [
        "# Path to your zip file\n",
        "zip_path = '/content/drive/My Drive/Colab Notebooks/mri_images.zip'\n",
        "# Create a directory to extract the files\n",
        "extract_path = '/content/mri_images'\n",
        "os.makedirs(extract_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKm8-NADMa9o",
      "metadata": {
        "id": "bKm8-NADMa9o"
      },
      "outputs": [],
      "source": [
        "# Get the name of the zip file without the path\n",
        "zip_filename = os.path.basename(zip_path)\n",
        "\n",
        "# Get the name of the extracted folder (remove .zip extension)\n",
        "extracted_folder = os.path.splitext(zip_filename)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f5c440",
      "metadata": {
        "collapsed": true,
        "id": "b2f5c440"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(f\"Zip file: {zip_filename}\")\n",
        "print(f\"Extracted folder: {extracted_folder}\")\n",
        "print(f\"Files extracted to: {extract_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9925b00",
      "metadata": {
        "id": "f9925b00"
      },
      "outputs": [],
      "source": [
        "# Define the path to the glioma folder\n",
        "glioma_path = os.path.join('Testing', 'glioma')\n",
        "\n",
        "# Verify the path exists\n",
        "if os.path.exists(glioma_path):\n",
        "    print(f\"Successfully located the glioma folder at: {glioma_path}\")\n",
        "else:\n",
        "    print(\"Couldn't find the glioma folder. Please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4425e5",
      "metadata": {
        "id": "9e4425e5"
      },
      "source": [
        "#### Viewing a Random Image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be853e2e",
      "metadata": {
        "collapsed": true,
        "id": "be853e2e"
      },
      "outputs": [],
      "source": [
        "# Get a list of all image files in the glioma folder\n",
        "image_files = [f for f in os.listdir(glioma_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "if image_files:\n",
        "    # Select a random image\n",
        "    random_image = random.choice(image_files)\n",
        "    image_path = os.path.join(glioma_path, random_image)\n",
        "\n",
        "    # Open and display the image\n",
        "    img = Image.open(image_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample Glioma Image: {random_image}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No image files found in the glioma folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f7a8d6",
      "metadata": {
        "collapsed": true,
        "id": "f8f7a8d6"
      },
      "outputs": [],
      "source": [
        "# Get a list of all image files in the glioma folder\n",
        "image_files = [f for f in os.listdir(glioma_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "if image_files:\n",
        "    # Select a random image\n",
        "    random_image = random.choice(image_files)\n",
        "    image_path = os.path.join(glioma_path, random_image)\n",
        "\n",
        "    # Open the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img_array, cmap='gray')  # Use 'gray' colormap\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample Glioma Image: {random_image}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No image files found in the glioma folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575c4bb2",
      "metadata": {
        "id": "575c4bb2"
      },
      "source": [
        "## 2) Resizing all the images to ResNet-50 input size and Saving them in a new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92deaabc",
      "metadata": {
        "id": "92deaabc"
      },
      "outputs": [],
      "source": [
        "def resize_images(input_folder, output_folder, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Resize all images in the input folder and save them to the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the folder containing original images\n",
        "    output_folder (str): Path to the folder where resized images will be saved\n",
        "    target_size (tuple): The target size for the images (width, height)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "            try:\n",
        "                img = Image.open(os.path.join(input_folder, filename))\n",
        "                img = img.resize(target_size, Image.LANCZOS)\n",
        "                img.save(os.path.join(output_folder, filename))\n",
        "                print(f\"Resized {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "# Define the size you want for all images\n",
        "target_size = (224, 224)  # This is a common size for many CNN architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655a34c8",
      "metadata": {
        "collapsed": true,
        "id": "655a34c8"
      },
      "outputs": [],
      "source": [
        "# Define the base directories\n",
        "base_input_dir = 'Training'  # Adjust this if your folder structure is different\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Resize images for each tumor type\n",
        "for tumor_type in tumor_types:\n",
        "    input_folder = os.path.join(base_input_dir, tumor_type)\n",
        "    output_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "    print(f\"Resizing images in {tumor_type} folder...\")\n",
        "    resize_images(input_folder, output_folder, target_size)\n",
        "\n",
        "print(\"Image resizing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zC4nD9LUaOde",
      "metadata": {
        "collapsed": true,
        "id": "zC4nD9LUaOde"
      },
      "outputs": [],
      "source": [
        "# Define the base directories\n",
        "base_input_dir = 'Testing'  # Adjust this if your folder structure is different\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Resize images for each tumor type\n",
        "for tumor_type in tumor_types:\n",
        "    input_folder = os.path.join(base_input_dir, tumor_type)\n",
        "    output_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "    print(f\"Resizing images in {tumor_type} folder...\")\n",
        "    resize_images(input_folder, output_folder, target_size)\n",
        "\n",
        "print(\"Image resizing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UlXDjPjnaF0V",
      "metadata": {
        "id": "UlXDjPjnaF0V"
      },
      "source": [
        "### Loading the resized folders from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-p9pcBvbTcOT",
      "metadata": {
        "collapsed": true,
        "id": "-p9pcBvbTcOT"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if not already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Choose a random tumor type and image\n",
        "tumor_type = random.choice(tumor_types)\n",
        "resized_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(resized_folder):\n",
        "    print(f\"Error: The folder {resized_folder} does not exist in your Google Drive.\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(resized_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Error: No image files found in {resized_folder}\")\n",
        "    else:\n",
        "        random_image = random.choice(image_files)\n",
        "\n",
        "        # Display the image\n",
        "        img_path = os.path.join(resized_folder, random_image)\n",
        "        img = Image.open(img_path)\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img_array, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Resized {tumor_type} Image: {random_image}\\nSize: {img_array.shape}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Image size: {img_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SB008AFDbJ16",
      "metadata": {
        "collapsed": true,
        "id": "SB008AFDbJ16"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if not already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Choose a random tumor type and image\n",
        "tumor_type = random.choice(tumor_types)\n",
        "resized_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(resized_folder):\n",
        "    print(f\"Error: The folder {resized_folder} does not exist in your Google Drive.\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(resized_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Error: No image files found in {resized_folder}\")\n",
        "    else:\n",
        "        random_image = random.choice(image_files)\n",
        "\n",
        "        # Display the image\n",
        "        img_path = os.path.join(resized_folder, random_image)\n",
        "        img = Image.open(img_path)\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img_array, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Resized {tumor_type} Image: {random_image}\\nSize: {img_array.shape}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Image size: {img_array.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkbckuT3y-Jd",
      "metadata": {
        "id": "xkbckuT3y-Jd"
      },
      "source": [
        "# PyTorch Data Pre-Processing\n",
        "\n",
        "## 1) DataSet & DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ypBOqduzIQ-",
      "metadata": {
        "id": "6ypBOqduzIQ-"
      },
      "source": [
        "Importing PyTorch contigencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1kRaIX1pRqu",
      "metadata": {
        "id": "S1kRaIX1pRqu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MJujwySKzRis",
      "metadata": {
        "id": "MJujwySKzRis"
      },
      "source": [
        "Defining the data sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mc-By-K9pST7",
      "metadata": {
        "id": "Mc-By-K9pST7"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BES8ulaozTmz",
      "metadata": {
        "id": "BES8ulaozTmz"
      },
      "source": [
        "DataSet Class:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66863b94",
      "metadata": {
        "id": "66863b94"
      },
      "source": [
        "## 2) Data Augmentation\n",
        "\n",
        "1. Random Crop\n",
        "2. Random Flip\n",
        "3. Random Rotate\n",
        "4. Random Brightness\n",
        "5. Random Contrast\n",
        "6. On the fly augmentation during training (saves memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rd1WSxuFCMrx",
      "metadata": {
        "id": "rd1WSxuFCMrx"
      },
      "source": [
        "__Important__\n",
        "\n",
        "1) Avoid extreme rotations or flips that could change the anatomical orientation.\n",
        "\n",
        "2) Be cautious with color-based augmentations, as MRI intensity values often have specific meanings.\n",
        "\n",
        "3)Maintain the overall structure and proportions of the brain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AEm0XQZMCghb",
      "metadata": {
        "id": "AEm0XQZMCghb"
      },
      "source": [
        "1. Slight rotations (within Â±10 degrees)\n",
        "2. Small shifts (translations)\n",
        "3. Zoom in/out (within a small range)\n",
        "4. Minimal brightness and contrast adjustments\n",
        "5. Gaussian noise addition (to simulate image noise)\n",
        "6. Elastic deformations (subtle warping)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ePbW7TwvLv",
      "metadata": {
        "id": "c0ePbW7TwvLv"
      },
      "outputs": [],
      "source": [
        "class RandomMRIAugmentation(nn.Module):\n",
        "    def __init__(self, rotation_range=10, translation_range=0.1, zoom_range=0.1, noise_factor=0.05, p=0.5):\n",
        "        super().__init__()\n",
        "        self.rotation_range = rotation_range\n",
        "        self.translation_range = translation_range\n",
        "        self.zoom_range = zoom_range\n",
        "        self.noise_factor = noise_factor\n",
        "        self.p = p  # Probability of applying each augmentation\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Ensure input is a tensor\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img = TF.to_tensor(img)\n",
        "\n",
        "        # Random rotation\n",
        "        if random.random() < self.p:\n",
        "            angle = random.uniform(-self.rotation_range, self.rotation_range)\n",
        "            img = TF.rotate(img, angle)\n",
        "\n",
        "        # Random translation\n",
        "        if random.random() < self.p:\n",
        "            translate = [random.uniform(-self.translation_range, self.translation_range) for _ in range(2)]\n",
        "            img = TF.affine(img, angle=0, translate=translate, scale=1, shear=0)\n",
        "\n",
        "        # Random zoom\n",
        "        if random.random() < self.p:\n",
        "            scale = random.uniform(1-self.zoom_range, 1+self.zoom_range)\n",
        "            img = TF.affine(img, angle=0, translate=(0,0), scale=scale, shear=0)\n",
        "\n",
        "        # Add Gaussian noise\n",
        "        if random.random() < self.p:\n",
        "            noise = torch.randn_like(img) * self.noise_factor\n",
        "            img = img + noise\n",
        "            img = torch.clamp(img, 0, 1)\n",
        "\n",
        "        return img\n",
        "\n",
        "def elastic_transform(image, alpha=1000, sigma=30, alpha_affine=30):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003].\"\"\"\n",
        "    random_state = np.random.RandomState(None)\n",
        "\n",
        "    shape = image.shape\n",
        "    shape_size = shape[:2]\n",
        "\n",
        "    # Random affine\n",
        "    center_square = np.float32(shape_size) // 2\n",
        "    square_size = min(shape_size) // 3\n",
        "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
        "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
        "    M = cv2.getAffineTransform(pts1, pts2)\n",
        "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
        "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
        "\n",
        "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
        "\n",
        "class RandomElasticDeformation(nn.Module):\n",
        "    def __init__(self, p=0.2, alpha=1000, sigma=30, alpha_affine=30):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.alpha = alpha\n",
        "        self.sigma = sigma\n",
        "        self.alpha_affine = alpha_affine\n",
        "\n",
        "    def forward(self, img):\n",
        "        if random.random() < self.p:\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.numpy()\n",
        "            img = elastic_transform(img, self.alpha, self.sigma, self.alpha_affine)\n",
        "            img = torch.from_numpy(img)\n",
        "        return img\n",
        "\n",
        "def get_mri_augmentation(p_transform=0.5, p_elastic=0.2):\n",
        "    return transforms.Compose([\n",
        "        RandomMRIAugmentation(rotation_range=10, translation_range=0.1, zoom_range=0.1, noise_factor=0.05, p=p_transform),\n",
        "        RandomElasticDeformation(p=p_elastic),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust these values based on your MRI data statistics\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AE-Cp55NC_1Y",
      "metadata": {
        "id": "AE-Cp55NC_1Y"
      },
      "outputs": [],
      "source": [
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, folder_path, tumor_types, transform=None, augment=False):\n",
        "        self.folder_path = folder_path\n",
        "        self.tumor_types = tumor_types\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, tumor_type in enumerate(tumor_types):\n",
        "            tumor_folder = os.path.join(folder_path, tumor_type)\n",
        "            for img_name in os.listdir(tumor_folder):\n",
        "                self.image_paths.append(os.path.join(tumor_folder, img_name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        if self.augment:\n",
        "            self.aug_transform = get_mri_augmentation(p_transform=0.5, p_elastic=0.2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            image = self.aug_transform(image)\n",
        "        elif self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJiXsfrYy5z0",
      "metadata": {
        "id": "gJiXsfrYy5z0"
      },
      "outputs": [],
      "source": [
        "def create_mri_datasets(train_folder, test_folder, tumor_types, val_split=0.2, batch_size=32):\n",
        "    # Define base transform for validation and test sets\n",
        "    base_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust based on your MRI data\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    full_train_dataset = MRIDataset(train_folder, tumor_types, augment=True)\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=base_transform)\n",
        "\n",
        "    # Split the training dataset into train and validation\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(full_train_dataset)),\n",
        "        test_size=val_split,\n",
        "        stratify=full_train_dataset.labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(full_train_dataset, val_indices)\n",
        "\n",
        "    # Override the transform for the validation set\n",
        "    val_dataset.dataset.augment = False\n",
        "    val_dataset.dataset.transform = base_transform\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uihuQoeVy85r",
      "metadata": {
        "collapsed": true,
        "id": "uihuQoeVy85r"
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "train_loader, val_loader, test_loader = create_mri_datasets(train_folder, test_folder, tumor_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNets"
      ],
      "metadata": {
        "id": "ZYSUMhdwfOY-"
      },
      "id": "ZYSUMhdwfOY-"
    },
    {
      "cell_type": "markdown",
      "id": "vhIU243IzIoS",
      "metadata": {
        "id": "vhIU243IzIoS"
      },
      "source": [
        "\n",
        "## Model 1: ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k9DJnSA3G-Z9",
      "metadata": {
        "collapsed": true,
        "id": "k9DJnSA3G-Z9"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "INJfnB-SGEn_",
      "metadata": {
        "id": "INJfnB-SGEn_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kvtM9IHZ1DsW",
      "metadata": {
        "id": "kvtM9IHZ1DsW"
      },
      "source": [
        "Defining the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPjQzy9bCpol",
      "metadata": {
        "id": "TPjQzy9bCpol"
      },
      "outputs": [],
      "source": [
        "def initialize_resnet50_model(num_classes):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "             best_val_loss = val_loss\n",
        "             best_val_acc = val_acc\n",
        "             epochs_no_improve = 0\n",
        "             best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8x_TJ816mZj4",
      "metadata": {
        "id": "8x_TJ816mZj4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "def visualize_model_attention(model, input_tensor, target_class):\n",
        "    model.eval()\n",
        "    cam = GradCAM(model=model, target_layers=[model.layer4[-1]], use_cuda=torch.cuda.is_available())\n",
        "    grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0), target_category=target_class)\n",
        "    visualization = show_cam_on_image(input_tensor.permute(1, 2, 0).numpy(), grayscale_cam[0, :], use_rgb=True)\n",
        "    plt.imshow(visualization)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Grad-CAM for class {target_class}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DDdlzmlqlXv",
      "metadata": {
        "id": "0DDdlzmlqlXv"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6  # Size in MB\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "def create_metrics_dataframe(model, test_acc, precision, recall, f1, auc_roc, train_time, test_loss):\n",
        "    metrics = {\n",
        "        'Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "        'Value': [\n",
        "            test_acc,\n",
        "            f1,\n",
        "            test_loss,\n",
        "            train_time,\n",
        "            sum(p.numel() for p in model.parameters()),\n",
        "            get_model_size(model)\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(metrics)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thgX5dh7gc-f",
      "metadata": {
        "id": "thgX5dh7gc-f"
      },
      "outputs": [],
      "source": [
        "# Define the base path in your Google Drive\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(model, filename):\n",
        "    save_path = os.path.join(base_path, filename)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved successfully to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(model, filename):\n",
        "    load_path = os.path.join(base_path, filename)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(load_path))\n",
        "        print(f\"Model loaded successfully from {load_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rcyyQz2V1Sbb",
      "metadata": {
        "id": "rcyyQz2V1Sbb"
      },
      "source": [
        "Training Loop: GPU T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UP_z22M1OqN-",
      "metadata": {
        "id": "UP_z22M1OqN-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_resnet50_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'resnet50_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_mriresnet50_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "RQq5l4dx7IVn"
      },
      "id": "RQq5l4dx7IVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: ResNet101"
      ],
      "metadata": {
        "id": "rErMLCWIjJXo"
      },
      "id": "rErMLCWIjJXo"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchvision.models as models\n",
        "import time\n",
        "\n",
        "def initialize_model_resnet101(num_classes):\n",
        "    model = models.resnet101(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize criterion, optimizer, and scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Separate parameter groups for different learning rates\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.fc.parameters(), 'lr': 0.001},\n",
        "        {'params': model.layer4.parameters(), 'lr': 0.0001}\n",
        "    ])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping triggered!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Total training time: {training_time/60:.2f} minutes')\n",
        "\n",
        "    return model, best_val_acc, training_time"
      ],
      "metadata": {
        "id": "LFjDeloGjJzo"
      },
      "id": "LFjDeloGjJzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_model_resnet101(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'resnet101_model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model_resnet101(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'resnet101_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_resnet101_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete!\")\n",
        "\n",
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "def visualize_model_attention(model, input_tensor, target_class):\n",
        "    model.eval()\n",
        "    cam = GradCAM(model=model, target_layers=[model.layer4[-1]], use_cuda=torch.cuda.is_available())\n",
        "    grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0), target_category=target_class)\n",
        "    visualization = show_cam_on_image(input_tensor.permute(1, 2, 0).numpy(), grayscale_cam[0, :], use_rgb=True)\n",
        "    plt.imshow(visualization)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Grad-CAM for class {target_class}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wqWkFG9hk2PX",
        "collapsed": true
      },
      "id": "wqWkFG9hk2PX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNets"
      ],
      "metadata": {
        "id": "3u6u02DafgcF"
      },
      "id": "3u6u02DafgcF"
    },
    {
      "cell_type": "markdown",
      "id": "93PrO5bF1pHh",
      "metadata": {
        "id": "93PrO5bF1pHh"
      },
      "source": [
        "## Model 3: EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tIfH-g2j1pfD",
      "metadata": {
        "id": "tIfH-g2j1pfD"
      },
      "outputs": [],
      "source": [
        "def initialize_efficientnet0_model(num_classes):\n",
        "    model = models.efficientnet_b0(pretrained=True)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JeOu-qn0LZt3",
      "metadata": {
        "id": "JeOu-qn0LZt3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "             best_val_loss = val_loss\n",
        "             best_val_acc = val_acc\n",
        "             epochs_no_improve = 0\n",
        "             best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vN3yqs_dFHiv",
      "metadata": {
        "id": "vN3yqs_dFHiv"
      },
      "outputs": [],
      "source": [
        "def visualize_model_attention(model, input_tensor, target_class):\n",
        "    model.eval()\n",
        "    cam = GradCAM(model=model, target_layers=[model.features[-1]], use_cuda=torch.cuda.is_available())\n",
        "    grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0), target_category=target_class)\n",
        "    visualization = show_cam_on_image(input_tensor.permute(1, 2, 0).numpy(), grayscale_cam[0, :], use_rgb=True)\n",
        "    plt.imshow(visualization)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Grad-CAM for class {target_class}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vr3mgWHzFgw2",
      "metadata": {
        "id": "vr3mgWHzFgw2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_efficientnet0_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'efficientnet_b0_model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'efficientnet_b0_model_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_efficientnet_b0_mri_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete for EfficientNet-B0 model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7sFe-Abw2Ft4",
      "metadata": {
        "id": "7sFe-Abw2Ft4"
      },
      "source": [
        "##Remark:\n",
        "EfficienNet and ResNet are both CNNs that specialize in edge detection by nature. So it is normal to observe that model attention goes to the edge of tumours instead of directly hovering above it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: EfficientNetB1"
      ],
      "metadata": {
        "id": "H3xSWa50m-QU"
      },
      "id": "H3xSWa50m-QU"
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_efficientnetb1_model(num_classes):\n",
        "    model = models.efficientnet_b1(pretrained=True)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "             best_val_loss = val_loss\n",
        "             best_val_acc = val_acc\n",
        "             epochs_no_improve = 0\n",
        "             best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n",
        "\n"
      ],
      "metadata": {
        "id": "4oNewZxGnArz"
      },
      "id": "4oNewZxGnArz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'efficientnet_b1_model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_efficientnetb1_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'efficientnet_b1_model_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_efficientnet_b1_mri_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete for EfficientNet-B1 model!\")"
      ],
      "metadata": {
        "id": "HSr4W0WpnAvZ",
        "collapsed": true
      },
      "id": "HSr4W0WpnAvZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "YTJCtkV1whjz",
      "metadata": {
        "id": "YTJCtkV1whjz"
      },
      "source": [
        "## Comparaison of ResNet50 and EfficientNetB0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMxZz1ZQvYBN",
      "metadata": {
        "id": "pMxZz1ZQvYBN"
      },
      "outputs": [],
      "source": [
        "# Assuming the data is already in the format you provided\n",
        "data = {\n",
        "    'EfficientNet_Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "    'EfficientNet_Value': [9.847445e-01, 9.846879e-01, 5.018997e-02, 2.509030e+03, 4.012672e+06, 1.632957e+01],\n",
        "    'OtherModel_Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "    'OtherModel_Value': [9.938978e-01, 9.938897e-01, 2.039252e-02, 2.520090e+03, 2.351623e+07, 9.436445e+01]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Create separate bar charts for each metric\n",
        "metrics = df['EfficientNet_Metric']\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Comparison of Model Metrics', fontsize=16)\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i // 3, i % 3]\n",
        "\n",
        "    efficientnet_value = df.loc[df['EfficientNet_Metric'] == metric, 'EfficientNet_Value'].values[0]\n",
        "    othermodel_value = df.loc[df['OtherModel_Metric'] == metric, 'OtherModel_Value'].values[0]\n",
        "\n",
        "    ax.bar(['EfficientNet', 'Other Model'], [efficientnet_value, othermodel_value])\n",
        "    ax.set_title(metric)\n",
        "    ax.set_ylabel('Value')\n",
        "\n",
        "    # Format y-axis labels based on the metric\n",
        "    if 'Parameters' in metric or 'Time' in metric:\n",
        "        ax.set_yscale('log')\n",
        "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "    elif 'Accuracy' in metric or 'F1 Score' in metric:\n",
        "        ax.set_ylim(0.95, 1.0)  # Adjust as needed\n",
        "\n",
        "    # Add value labels on top of each bar\n",
        "    for j, v in enumerate([efficientnet_value, othermodel_value]):\n",
        "        ax.text(j, v, f'{v:.2e}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "plt.show()\n",
        "\n",
        "# Create a table to show the exact values\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.5)\n",
        "plt.title('Model Metrics Comparison Table')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bce9f6SBwnrM",
      "metadata": {
        "id": "Bce9f6SBwnrM"
      },
      "source": [
        "\n",
        "\n",
        "__It make sense to use EfficientNetB0, because it has a lot less parameters than ResNet50, yet achieves almost the same accuracy.__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT: Vision transformer"
      ],
      "metadata": {
        "id": "Bz3uQtXzrTOR"
      },
      "id": "Bz3uQtXzrTOR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5: ViT Small"
      ],
      "metadata": {
        "id": "WzlzElfhfrpo"
      },
      "id": "WzlzElfhfrpo"
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "available_models = timm.list_models('vit*')\n",
        "print(available_models)\n"
      ],
      "metadata": {
        "id": "sGTmzfN2xPzO"
      },
      "id": "sGTmzfN2xPzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import timm\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def initialize_vit_model(num_classes):\n",
        "    # Initialize ViT small model from timm\n",
        "    model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
        "    # Modify the head to match our number of classes\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Adjusted learning rate for ViT\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ccm2YvvGSPw"
      },
      "id": "_Ccm2YvvGSPw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Using ViT-specific preprocessing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # ViT standard normalization\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Rest of the code remains the same, just update the model names in save operations\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'vit_small_model_fold_{fold+1}.pth')\n",
        "\n",
        "    # Final training on entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the metrics\n",
        "    metrics_csv_path = os.path.join(base_path, 'vit_small_model_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_vit_small_mri_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete for ViT Small model!\")"
      ],
      "metadata": {
        "id": "8LQRGsxrGSTz",
        "collapsed": true
      },
      "id": "8LQRGsxrGSTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LeVits"
      ],
      "metadata": {
        "id": "2BXG1qief1Fx"
      },
      "id": "2BXG1qief1Fx"
    },
    {
      "cell_type": "markdown",
      "id": "ZqxUGTdOWIIV",
      "metadata": {
        "id": "ZqxUGTdOWIIV"
      },
      "source": [
        "## Model 6: Levit-256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JamnTZowC5Ob",
      "metadata": {
        "id": "JamnTZowC5Ob"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import timm\n",
        "import time\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1VhX8IwcVog",
      "metadata": {
        "id": "N1VhX8IwcVog"
      },
      "outputs": [],
      "source": [
        "class LeViT256Model(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(LeViT256Model, self).__init__()\n",
        "\n",
        "        # Load pretrained LeViT-256 model\n",
        "        self.levit = timm.create_model('levit_256', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Get the number of features from LeViT\n",
        "        levit_num_features = self.levit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(levit_num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through LeViT\n",
        "        features = self.levit(x)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_levit256_model(num_classes, pretrained=True):\n",
        "    return LeViT256Model(num_classes, pretrained)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "# Function to create metrics DataFrame\n",
        "def create_metrics_dataframe(model, test_acc, precision, recall, f1, auc_roc, train_time, test_loss):\n",
        "    metrics = {\n",
        "        'Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "        'Value': [\n",
        "            test_acc,\n",
        "            f1,\n",
        "            test_loss,\n",
        "            train_time,\n",
        "            sum(p.numel() for p in model.parameters()),\n",
        "            sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(metrics)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qbwe9hchh0rU",
      "metadata": {
        "id": "Qbwe9hchh0rU"
      },
      "outputs": [],
      "source": [
        "def train_levit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_levit_model(num_classes).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save the first model state or if we have a new best validation loss\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Final training on entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_levit_model(num_classes).to(device)\n",
        "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None  # Initialize best_model\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        # Validation phase\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save the first model state or if we have a new best validation loss\n",
        "        if best_model is None or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = final_model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Load the best model state\n",
        "    final_model.load_state_dict(best_model)\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_levit_256_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMtGcId4hbH_",
      "metadata": {
        "id": "cMtGcId4hbH_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable torch compile to avoid dynamo issues\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        # Print dataset sizes\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the LeViT-256 model\n",
        "        results, final_model = train_levit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_256_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_256_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_256_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_256_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()  # This will print the full error traceback\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nLeViT-256 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gKu7OREGTT56",
      "metadata": {
        "id": "gKu7OREGTT56"
      },
      "source": [
        "## Model 7: Levit-384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt4KntDS0lWz",
      "metadata": {
        "id": "Xt4KntDS0lWz"
      },
      "outputs": [],
      "source": [
        "class LeViT384Model(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(LeViT384Model, self).__init__()\n",
        "\n",
        "        # Load pretrained LeViT-384 model\n",
        "        self.levit = timm.create_model('levit_384', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Get the number of features from LeViT\n",
        "        levit_num_features = self.levit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(levit_num_features, 1024),  # Increased intermediate layer size for LeViT-384\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through LeViT\n",
        "        features = self.levit(x)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_levit384_model(num_classes, pretrained=True):\n",
        "    return LeViT384Model(num_classes, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YE2GXpUrcVtm",
      "metadata": {
        "collapsed": true,
        "id": "YE2GXpUrcVtm"
      },
      "outputs": [],
      "source": [
        "def train_levit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_levit_model(num_classes).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save the first model state or if we have a new best validation loss\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Final training on entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_levit_model(num_classes).to(device)\n",
        "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None  # Initialize best_model\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        # Validation phase\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save the first model state or if we have a new best validation loss\n",
        "        if best_model is None or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = final_model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Load the best model state\n",
        "    final_model.load_state_dict(best_model)\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_levit_384_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89n6xoKscVwG",
      "metadata": {
        "id": "89n6xoKscVwG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable torch compile to avoid dynamo issues\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        # Print dataset sizes\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the LeViT-384 model\n",
        "        results, final_model = train_levit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_384_training_history.csv')  # Changed filename\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        # Reduced batch size for LeViT-384 due to higher memory requirements\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)  # Reduced batch size\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_384_model_metrics.csv')  # Changed filename\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_384_model_final.pth')  # Changed filename\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_384_test_predictions.csv')  # Changed filename\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()  # This will print the full error traceback\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nLeViT-384 training, evaluation, and metrics logging complete!\")  # Updated message"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CoAtNets"
      ],
      "metadata": {
        "id": "pUr2dDUgk8nJ"
      },
      "id": "pUr2dDUgk8nJ"
    },
    {
      "cell_type": "markdown",
      "id": "6IG7TOH-KlTj",
      "metadata": {
        "id": "6IG7TOH-KlTj"
      },
      "source": [
        "## Model 8: CoAtNet-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9ixpJCxUSYK",
      "metadata": {
        "id": "i9ixpJCxUSYK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "import timm\n",
        "import time\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "import traceback\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class CoAtNet0Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CoAtNet0Model, self).__init__()\n",
        "\n",
        "        # Load pre-trained CoAtNet-0-RW model\n",
        "        self.coatnet = timm.create_model('coatnet_0_rw_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(self.coatnet, 'set_grad_checkpointing'):\n",
        "            self.coatnet.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from CoAtNet\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "\n",
        "        # Add final classification layers with reduced size for memory efficiency\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(coatnet_num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.coatnet(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_coatnet_model(num_classes):\n",
        "    return CoAtNet0Model(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CXJDXD-qQWnu",
      "metadata": {
        "id": "CXJDXD-qQWnu"
      },
      "outputs": [],
      "source": [
        "def train_coatnet_model(train_dataset, test_dataset, num_classes, num_epochs=50, patience=5, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Training with pre-trained weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Slightly larger batch size since we're fine-tuning\n",
        "    batch_size = 24\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_coatnet_model(num_classes).to(device)\n",
        "\n",
        "        # Use different learning rates for pre-trained layers and new layers\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': model.coatnet.parameters(), 'lr': 1e-5},  # Lower learning rate for pre-trained layers\n",
        "            {'params': model.fc.parameters(), 'lr': 1e-4}       # Higher learning rate for new layers\n",
        "        ])\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache periodically\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_pretrained_coatnet_0_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ysXYjXCUSdL",
      "metadata": {
        "id": "8ysXYjXCUSdL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the CoAtNet-0 model\n",
        "        results, final_model = train_coatnet_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'coatnet_0_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'coatnet_0_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'coatnet_0_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'coatnet_0_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCoAtNet-0 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kOIU17eceS9u",
      "metadata": {
        "id": "kOIU17eceS9u"
      },
      "source": [
        "Since the comp complexity a.k.a time to train is too long I will make the educated guess to not experiment with more complex co-at-net models based on the fact that research shows that accuracy doesn't improve all that much."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to it's complex hybrid architecture, CoAtNet is nearly impossible to interpret"
      ],
      "metadata": {
        "id": "b1J8rMFjRH-R"
      },
      "id": "b1J8rMFjRH-R"
    },
    {
      "cell_type": "markdown",
      "id": "J4whcLjVjszh",
      "metadata": {
        "id": "J4whcLjVjszh"
      },
      "source": [
        "## Model 9: CoAtNet1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kd1Pi3YPK3Jm"
      },
      "id": "kd1Pi3YPK3Jm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OSQGmAvHzMeC",
      "metadata": {
        "id": "OSQGmAvHzMeC"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class CoAtNet1Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CoAtNet1Model, self).__init__()\n",
        "\n",
        "        # Load pre-trained CoAtNet-1-RW model\n",
        "        self.coatnet = timm.create_model('coatnet_1_rw_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(self.coatnet, 'set_grad_checkpointing'):\n",
        "            self.coatnet.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from CoAtNet-1\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "\n",
        "        # Add final classification layers with reduced size for memory efficiency\n",
        "        # Note: CoAtNet-1 has more features than CoAtNet-0, so we add an extra reduction layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(coatnet_num_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.coatnet(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_coatnet_model(num_classes):\n",
        "    return CoAtNet1Model(num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MWzKT_Q1zMze",
      "metadata": {
        "id": "MWzKT_Q1zMze"
      },
      "outputs": [],
      "source": [
        "def train_coatnet_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Training with pre-trained CoAtNet-1 weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Reduced batch size for CoAtNet-1 since it's larger\n",
        "    batch_size = 16\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_coatnet_model(num_classes).to(device)\n",
        "\n",
        "        # Adjusted learning rates for CoAtNet-1\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': model.coatnet.parameters(), 'lr': 5e-6},  # Lower learning rate for pre-trained layers\n",
        "            {'params': model.fc.parameters(), 'lr': 5e-5}       # Lower learning rate for new layers\n",
        "        ])\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache more frequently for CoAtNet-1\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_pretrained_coatnet_1_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oDxicI-zNQS",
      "metadata": {
        "id": "5oDxicI-zNQS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    print(\"Initializing CoAtNet-1 training pipeline...\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    print(f\"Tumor types: {tumor_types}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for CoAtNet-1\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomAdjustSharpness(0.2),  # Added for CoAtNet-1\n",
        "            transforms.RandomAutocontrast(),        # Added for CoAtNet-1\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"\\nDataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the CoAtNet-1 model\n",
        "        print(\"\\nInitiating CoAtNet-1 training...\")\n",
        "        results, final_model = train_coatnet_model(\n",
        "            train_dataset,\n",
        "            test_dataset,\n",
        "            num_classes,\n",
        "            num_epochs=50,    # Adjust these parameters as needed\n",
        "            patience=5,\n",
        "            k_folds=5\n",
        "        )\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'coatnet_1_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"\\nTraining history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=16,  # Reduced batch size for CoAtNet-1\n",
        "            shuffle=False,\n",
        "            num_workers=2,   # Reduced workers for memory efficiency\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'coatnet_1_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save confusion matrix\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=tumor_types,\n",
        "                   yticklabels=tumor_types)\n",
        "        plt.title('CoAtNet-1 Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(base_path, 'coatnet_1_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'coatnet_1_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'coatnet_1_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "        # Print final summary\n",
        "        print(\"\\nFinal Summary:\")\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Average F1 Score: {np.mean(f1):.4f}\")\n",
        "        print(f\"Average AUC-ROC: {np.mean(auc_roc):.4f}\")\n",
        "        print(f\"Total Training Time: {results[-1]['Training Time (s)']:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCoAtNet-1 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XCiTs: Cross Covariance Image Transformers"
      ],
      "metadata": {
        "id": "nGRJ6aAilF3m"
      },
      "id": "nGRJ6aAilF3m"
    },
    {
      "cell_type": "markdown",
      "id": "bb6a89AsiwZK",
      "metadata": {
        "id": "bb6a89AsiwZK"
      },
      "source": [
        "## Model 10: XCiT_small_12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m9gloWBIl9Qu",
      "metadata": {
        "id": "m9gloWBIl9Qu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class XCiTModel(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(XCiTModel, self).__init__()\n",
        "\n",
        "        # Load XCiT small_12 model with pretrained parameter\n",
        "        self.xcit = timm.create_model('xcit_small_12_p8_224', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing for memory efficiency\n",
        "        self.xcit.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from XCiT\n",
        "        xcit_num_features = self.xcit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(xcit_num_features, 512),  # Reduced from 1024 to 512 for memory efficiency\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.xcit(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_xcit_model(num_classes, pretrained=True):\n",
        "    return XCiTModel(num_classes, pretrained=pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64Vtz1Ksl9mD",
      "metadata": {
        "id": "64Vtz1Ksl9mD"
      },
      "outputs": [],
      "source": [
        "def train_xcit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5, pretrained=True):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Training with {'pretrained' if pretrained else 'randomly initialized'} weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use smaller batch size to prevent OOM errors\n",
        "    batch_size = 16  # Reduced from 32 but can be larger than XCiT-24 since model is smaller\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_xcit_model(num_classes, pretrained=pretrained).to(device)\n",
        "\n",
        "        # Use different learning rates for pretrained vs non-pretrained\n",
        "        initial_lr = 0.0001 if pretrained else 0.001\n",
        "        optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache periodically\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results,\n",
        "        'pretrained': pretrained\n",
        "    }, 'final_xcit_small_12_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GISmuvECl-D1",
      "metadata": {
        "id": "GISmuvECl-D1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the XCiT model\n",
        "        results, final_model = train_xcit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'xcit_small_12_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'xcit_small_12_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'xcit_small_12_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'xcit_small_12_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nXCiT Small 12 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualising results"
      ],
      "metadata": {
        "id": "mq0E-Rwvmp76"
      },
      "id": "mq0E-Rwvmp76"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_metrics():\n",
        "    # Dictionary to store file paths\n",
        "    file_paths = {\n",
        "        'ResNet50': '/content/drive/MyDrive/Colab Notebooks/resnet50_metrics.csv',\n",
        "        'ResNet101': '/content/drive/MyDrive/Colab Notebooks/resnet101_metrics.csv',\n",
        "        'EfficientNetB0': '/content/drive/MyDrive/Colab Notebooks/efficientnet_b0_model_metrics.csv',\n",
        "        'EfficientNetB1': '/content/drive/MyDrive/Colab Notebooks/efficientnet_b1_model_metrics.csv',\n",
        "        'Vit': '/content/drive/MyDrive/Colab Notebooks/vit_small_model_metrics.csv',\n",
        "        'Levit256': '/content/drive/MyDrive/Colab Notebooks/levit_256_model_metrics.csv',\n",
        "        'Levit384': '/content/drive/MyDrive/Colab Notebooks/levit_384_model_metrics.csv',\n",
        "        'CoAtNet0': '/content/drive/MyDrive/Colab Notebooks/coatnet_0_model_metrics.csv',\n",
        "        'CoAtNet1': '/content/drive/MyDrive/Colab Notebooks/coatnet_1_model_metrics.csv',\n",
        "        'XCiT': '/content/drive/MyDrive/Colab Notebooks/xcit_small_12_model_metrics.csv'\n",
        "    }\n",
        "\n",
        "    # Dictionary to store dataframes\n",
        "    dataframes = {}\n",
        "\n",
        "    # Load each CSV file into a dataframe\n",
        "    for model_name, file_path in file_paths.items():\n",
        "        try:                                #error handling\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                dataframes[model_name] = df\n",
        "                print(f\"Successfully loaded {model_name} metrics\")\n",
        "            else:\n",
        "                print(f\"Warning: File not found for {model_name} at {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_name} metrics: {str(e)}\")\n",
        "\n",
        "    return dataframes\n",
        "\n",
        "# Load all model metrics\n",
        "model_metrics = load_model_metrics()\n",
        "\n",
        "# You can access individual dataframes like this:\n",
        "# resnet50_df = model_metrics['ResNet50']\n",
        "# efficientnet_b0_df = model_metrics['EfficientNetB0']\n",
        "# etc."
      ],
      "metadata": {
        "id": "HVPYims9SMZE"
      },
      "id": "HVPYims9SMZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_df = model_metrics['ResNet50']\n",
        "resnet50_df"
      ],
      "metadata": {
        "id": "nlWWATTTSof7"
      },
      "id": "nlWWATTTSof7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_model_metrics(model_metrics):\n",
        "    \"\"\"\n",
        "    Merge all model metrics dataframes into a single dataframe where each row\n",
        "    represents a model and columns are the different metrics.\n",
        "\n",
        "    Args:\n",
        "        model_metrics (dict): Dictionary of dataframes keyed by model name\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Combined dataframe with one row per model\n",
        "    \"\"\"\n",
        "    # Initialize list to store model data\n",
        "    all_model_data = []\n",
        "\n",
        "    # Process each model's dataframe\n",
        "    for model_name, df in model_metrics.items():\n",
        "        # Create a dictionary for this model's metrics\n",
        "        model_data = {'model': model_name}\n",
        "\n",
        "        # Add each metric to the dictionary\n",
        "        for _, row in df.iterrows():\n",
        "            metric_name = row['Metric'].lower().replace(' ', '_')\n",
        "            if '(' in metric_name:  # Handle metrics with units\n",
        "                metric_name = metric_name.split('(')[0].strip('_')\n",
        "            model_data[metric_name] = row['Value']\n",
        "\n",
        "        all_model_data.append(model_data)\n",
        "\n",
        "    # Create final dataframe\n",
        "    merged_df = pd.DataFrame(all_model_data)\n",
        "\n",
        "    # Ensure consistent column ordering\n",
        "    desired_columns = ['model', 'overall_accuracy', 'f1_score',\n",
        "                      'cross_entropy_loss', 'training_time',\n",
        "                      'number_of_parameters', 'model_size']\n",
        "\n",
        "    # Filter columns to only include those that exist\n",
        "    final_columns = ['model'] + [col for col in desired_columns[1:]\n",
        "                                if col in merged_df.columns]\n",
        "\n",
        "    merged_df = merged_df[final_columns]\n",
        "\n",
        "    print(f\"Successfully merged {len(model_metrics)} models\")\n",
        "    print(f\"Columns in final dataframe: {merged_df.columns.tolist()}\")\n",
        "\n",
        "    return merged_df"
      ],
      "metadata": {
        "id": "QbDz0_uyS6xR"
      },
      "id": "QbDz0_uyS6xR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the model_metrics dictionary from the previous code\n",
        "merged_metrics = merge_model_metrics(model_metrics)\n",
        "\n",
        "merged_metrics"
      ],
      "metadata": {
        "id": "9bdbz2Dze012"
      },
      "id": "9bdbz2Dze012",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model accuracies (bar chart)"
      ],
      "metadata": {
        "id": "EfEIDnBTnMum"
      },
      "id": "EfEIDnBTnMum"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort the dataframe by accuracy\n",
        "sorted_metrics = merged_metrics.sort_values('overall_accuracy')\n",
        "\n",
        "# Create figure with larger size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the bar plot with sorted data\n",
        "bars = plt.bar(sorted_metrics['model'], sorted_metrics['overall_accuracy'], color='cornflowerblue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Model Accuracy Comparison', pad=20, size=14)\n",
        "plt.xlabel('Model', labelpad=10)\n",
        "plt.ylabel('Accuracy', labelpad=10)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Set y-axis limits to focus on 85-100% range with extra space for labels\n",
        "plt.ylim(0.85, 1.01)\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
        "\n",
        "# Format y-axis as percentages\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
        "\n",
        "# Add value labels on top of each bar with padding\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
        "             f'{height:.1%}',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qJvZvgRijcQz"
      },
      "id": "qJvZvgRijcQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy against number of paramters (scatter plot)"
      ],
      "metadata": {
        "id": "kqmvm_p8nM2Q"
      },
      "id": "kqmvm_p8nM2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure with larger size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(merged_metrics['number_of_parameters'],\n",
        "           merged_metrics['overall_accuracy'],\n",
        "           alpha=0.6,\n",
        "           s=100)  # s controls point size\n",
        "\n",
        "# Add labels for each point\n",
        "for i, model in enumerate(merged_metrics['model']):\n",
        "    plt.annotate(model,\n",
        "                (merged_metrics['number_of_parameters'].iloc[i],\n",
        "                 merged_metrics['overall_accuracy'].iloc[i]),\n",
        "                xytext=(5, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Model Accuracy vs Number of Parameters', pad=20, size=14)\n",
        "plt.xlabel('Number of Parameters (millions)', labelpad=10)\n",
        "plt.ylabel('Accuracy', labelpad=10)\n",
        "\n",
        "# Format x-axis in millions\n",
        "plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.1f}M'.format(x/1e6)))\n",
        "\n",
        "# Format y-axis as percentages\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Set y-axis limits to focus on relevant range\n",
        "plt.ylim(0.85, 1.02)\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zjh_SPNgkEq3"
      },
      "id": "zjh_SPNgkEq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time to train bar chart"
      ],
      "metadata": {
        "id": "DPwORO7uqe2m"
      },
      "id": "DPwORO7uqe2m"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort the dataframe by training time\n",
        "sorted_metrics = merged_metrics.sort_values('training_time')\n",
        "\n",
        "# Create figure with larger size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the bar plot with sorted data\n",
        "bars = plt.bar(sorted_metrics['model'], sorted_metrics['training_time'], color='cornflowerblue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Model Training Time Comparison', pad=20, size=14)\n",
        "plt.xlabel('Model', labelpad=10)\n",
        "plt.ylabel('Training Time (hours)', labelpad=10)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
        "\n",
        "# Format y-axis in hours\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1f}h'.format(y/3600)))\n",
        "\n",
        "# Add value labels on top of each bar with padding\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 50,  # Added padding\n",
        "             f'{height/3600:.1f}h',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pQBM6WnCqcYE"
      },
      "id": "pQBM6WnCqcYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time to train against against number of parameters (scatter plot)"
      ],
      "metadata": {
        "id": "vXtyJBAonM88"
      },
      "id": "vXtyJBAonM88"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure with larger size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(merged_metrics['number_of_parameters'],\n",
        "           merged_metrics['training_time'],\n",
        "           alpha=0.6,\n",
        "           s=100)  # s controls point size\n",
        "\n",
        "# Add labels for each point\n",
        "for i, model in enumerate(merged_metrics['model']):\n",
        "    plt.annotate(model,\n",
        "                (merged_metrics['number_of_parameters'].iloc[i],\n",
        "                 merged_metrics['training_time'].iloc[i]),\n",
        "                xytext=(5, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Training Time vs Number of Parameters', pad=20, size=14)\n",
        "plt.xlabel('Number of Parameters (millions)', labelpad=10)\n",
        "plt.ylabel('Training Time (hours)', labelpad=10)\n",
        "\n",
        "# Format x-axis in millions\n",
        "plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.1f}M'.format(x/1e6)))\n",
        "\n",
        "# Format y-axis in hours\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1f}h'.format(y/3600)))\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kVwPjbNfqoTb"
      },
      "id": "kVwPjbNfqoTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No direct correlation between time to train and number of paramets, it all comes down to the architecture design itself."
      ],
      "metadata": {
        "id": "BT73WImNqtez"
      },
      "id": "BT73WImNqtez"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference time (bar chart)"
      ],
      "metadata": {
        "id": "tpfEdoHtnNA6"
      },
      "id": "tpfEdoHtnNA6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of parameters vs Size in GB"
      ],
      "metadata": {
        "id": "ML7SOLfKrkT2"
      },
      "id": "ML7SOLfKrkT2"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert model size from MB to GB\n",
        "model_size_gb = merged_metrics['model_size'] / 1024  # MB to GB\n",
        "\n",
        "# Create figure with larger size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(model_size_gb,\n",
        "           merged_metrics['number_of_parameters'],\n",
        "           alpha=0.6,\n",
        "           s=100)  # s controls point size\n",
        "\n",
        "# Add labels for each point\n",
        "for i, model in enumerate(merged_metrics['model']):\n",
        "    plt.annotate(model,\n",
        "                (model_size_gb.iloc[i],\n",
        "                 merged_metrics['number_of_parameters'].iloc[i]),\n",
        "                xytext=(5, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Model Size vs Number of Parameters', pad=20, size=14)\n",
        "plt.xlabel('Model Size (GB)', labelpad=10)\n",
        "plt.ylabel('Number of Parameters (millions)', labelpad=10)\n",
        "\n",
        "# Format axes\n",
        "plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.2f}'.format(x)))\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1f}M'.format(y/1e6)))\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vykME-pyrpds"
      },
      "id": "vykME-pyrpds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect correlation"
      ],
      "metadata": {
        "id": "h_DxHotCr3gs"
      },
      "id": "h_DxHotCr3gs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Interpretability"
      ],
      "metadata": {
        "id": "m_LOyYIqnNFJ"
      },
      "id": "m_LOyYIqnNFJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the models"
      ],
      "metadata": {
        "id": "XGzro_H_Y2bc"
      },
      "id": "XGzro_H_Y2bc"
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_model = initialize_resnet50_model(num_classes=len(tumor_types))\n",
        "resnet50_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_mriresnet50_model.pth'))\n",
        "resnet50_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F_9HlGU0ZntV"
      },
      "id": "F_9HlGU0ZntV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet101_model = initialize_model_resnet101(num_classes=len(tumor_types))\n",
        "resnet101_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_resnet101_classification_model.pth'))\n",
        "resnet101_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mie5N7blaLhU"
      },
      "id": "Mie5N7blaLhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnetb0_model = initialize_efficientnet0_model(num_classes=len(tumor_types))\n",
        "efficientnetb0_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_efficientnet_b0_mri_classification_model.pth'))\n",
        "efficientnetb0_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W8No90h1abIk"
      },
      "id": "W8No90h1abIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnetb1_model = initialize_efficientnetb1_model(num_classes=len(tumor_types))\n",
        "efficientnetb1_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_efficientnet_b1_mri_classification_model.pth'))\n",
        "efficientnetb1_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DTFFbGkybty-"
      },
      "id": "DTFFbGkybty-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = initialize_vit_model(num_classes=len(tumor_types))\n",
        "vit_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_vit_small_mri_classification_model.pth'))\n",
        "vit_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l8mVXfFwu_2T"
      },
      "id": "l8mVXfFwu_2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "levit384_model = initialize_levit384_model(num_classes=4)\n",
        "levit384_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_384_model_final.pth')['model_state_dict'])\n",
        "levit384_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V8C2_bB2vAIO"
      },
      "id": "V8C2_bB2vAIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "levit256_model = initialize_levit256_model(num_classes=4)\n",
        "levit256_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_256_model_final.pth')['model_state_dict'])\n",
        "levit256_model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0IcITGlUvASp"
      },
      "id": "0IcITGlUvASp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting the same pictures"
      ],
      "metadata": {
        "id": "O3TQdJ28tUIU"
      },
      "id": "O3TQdJ28tUIU"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_random_samples(test_folder, samples_per_category=10, seed=None):\n",
        "    \"\"\"\n",
        "    Displays random samples of MRI images from each tumor category.\n",
        "\n",
        "    Args:\n",
        "        test_folder (str): Path to the test folder containing tumor type subfolders\n",
        "        samples_per_category (int): Number of samples to display per category\n",
        "        seed (int, optional): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing selected image paths for each category\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # Get all tumor categories (subfolders)\n",
        "    tumor_categories = [d for d in os.listdir(test_folder)\n",
        "                       if os.path.isdir(os.path.join(test_folder, d))]\n",
        "\n",
        "    selected_samples = {}\n",
        "\n",
        "    # Select random samples from each category\n",
        "    for category in tumor_categories:\n",
        "        category_path = os.path.join(test_folder, category)\n",
        "        image_files = [f for f in os.listdir(category_path)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        # Select random samples\n",
        "        selected_images = random.sample(image_files,\n",
        "                                      min(samples_per_category, len(image_files)))\n",
        "        selected_samples[category] = selected_images\n",
        "\n",
        "        # Display the samples\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        plt.suptitle(f'Category: {category}')\n",
        "\n",
        "        for idx, img_name in enumerate(selected_images, 1):\n",
        "            img_path = os.path.join(category_path, img_name)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            plt.subplot(2, 5, idx)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f'File: {img_name}')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return selected_samples\n",
        "\n",
        "# Example usage:\n",
        "# selected_images = display_random_samples(test_folder='/content/drive/MyDrive/Colab Notebooks/Resized_Testing')"
      ],
      "metadata": {
        "id": "Rqu07lz6kjJU"
      },
      "id": "Rqu07lz6kjJU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_images = display_random_samples(test_folder='/content/drive/MyDrive/Colab Notebooks/Resized_Testing')"
      ],
      "metadata": {
        "id": "wZJbKL9l-q1P"
      },
      "id": "wZJbKL9l-q1P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#images :\n",
        "# Te-gl_0261\n",
        "# Te-gl_0037\n",
        "# Te-gl_0131\n",
        "#Te-me_0224\n",
        "#Te-me_0231\n",
        "#Te-me_0162\n",
        "#Te-pi_0205\n",
        "#Te-pi_0132\n",
        "#Te-pi_0110"
      ],
      "metadata": {
        "id": "uVD6FpYe-6eO"
      },
      "id": "uVD6FpYe-6eO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_images(test_folder, image_names):\n",
        "    # Mapping of short names to full folder names\n",
        "    folder_map = {\n",
        "        'gl': 'glioma',\n",
        "        'me': 'meningioma',\n",
        "        'pi': 'pituitary'\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, name in enumerate(image_names, 1):\n",
        "        # Get category (gl, me, or pi)\n",
        "        category_short = name.split('-')[1].split('_')[0]\n",
        "        category_full = folder_map[category_short]\n",
        "\n",
        "        # Construct full path\n",
        "        img_path = os.path.join(test_folder, category_full, name + '.jpg')  # or whatever extension you have\n",
        "\n",
        "        # Load and display image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        plt.subplot(3, 3, i)\n",
        "        plt.imshow(img)\n",
        "        plt.title(name)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Images to load\n",
        "images = [\n",
        "    'Te-gl_0261',\n",
        "    'Te-gl_0037',\n",
        "    'Te-gl_0131',\n",
        "    'Te-me_0224',\n",
        "    'Te-me_0231',\n",
        "    'Te-me_0162',\n",
        "    'Te-pi_0205',\n",
        "    'Te-pi_0132',\n",
        "    'Te-pi_0110'\n",
        "]\n",
        "\n",
        "# Use it like this:\n",
        "load_images('/content/drive/MyDrive/Colab Notebooks/Resized_Testing', images)"
      ],
      "metadata": {
        "id": "kZE6wafwC4-6"
      },
      "id": "kZE6wafwC4-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grad-Cam (Resnets, EfficientNets)"
      ],
      "metadata": {
        "id": "pcAwlEointHb"
      },
      "id": "pcAwlEointHb"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - take the 9 images\n",
        "# 2 - does their gradcam for each of the 4 models\n",
        "# 3 - plot them side by side"
      ],
      "metadata": {
        "id": "qg6kp6pEBPfS"
      },
      "id": "qg6kp6pEBPfS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "def get_model_specific_params(model_name):\n",
        "    \"\"\"Returns model-specific parameters for GradCAM\"\"\"\n",
        "    params = {\n",
        "        'resnet50': {\n",
        "            'target_layer': 'layer4',\n",
        "            'input_size': 224,\n",
        "            'layer_getter': lambda model: model.layer4[-1]\n",
        "        },\n",
        "        'resnet101': {\n",
        "            'target_layer': 'layer4',\n",
        "            'input_size': 224,\n",
        "            'layer_getter': lambda model: model.layer4[-1]\n",
        "        },\n",
        "        'efficientnetb0': {\n",
        "            'target_layer': 'features.8',\n",
        "            'input_size': 224,\n",
        "            'layer_getter': lambda model: model.features[8]\n",
        "        },\n",
        "        'efficientnetb1': {\n",
        "            'target_layer': 'features.8',\n",
        "            'input_size': 240,\n",
        "            'layer_getter': lambda model: model.features[8]\n",
        "        }\n",
        "    }\n",
        "    return params[model_name]\n",
        "\n",
        "def get_gradcam_for_image(model, model_name, image_path, training_order):\n",
        "    \"\"\"Generate GradCAM visualization for a single model and image\"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        model_params = get_model_specific_params(model_name)\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((model_params['input_size'], model_params['input_size'])),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Setup GradCAM\n",
        "        target_layer = model_params['layer_getter'](model)\n",
        "        grad_cam = GradCAM(\n",
        "            model=model,\n",
        "            target_layers=[target_layer],\n",
        "            reshape_transform=None\n",
        "        )\n",
        "\n",
        "        # Get prediction and confidence\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "            prediction = torch.argmax(output).item()\n",
        "            confidence = probabilities[0][prediction].item() * 100\n",
        "\n",
        "        # Generate GradCAM\n",
        "        targets = [ClassifierOutputTarget(prediction)]\n",
        "        grayscale_cam = grad_cam(input_tensor=input_tensor, targets=targets)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        rgb_img = np.array(image.resize((model_params['input_size'], model_params['input_size']))) / 255.0\n",
        "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        return visualization, confidence, prediction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path} with {model_name}: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "def visualize_multiple_models_gradcam(models_dict, image_list, test_folder, training_order):\n",
        "    \"\"\"\n",
        "    Visualize GradCAM for multiple images across different models\n",
        "    \"\"\"\n",
        "    folder_map = {\n",
        "        'gl': 'glioma',\n",
        "        'me': 'meningioma',\n",
        "        'pi': 'pituitary'\n",
        "    }\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n_images = len(image_list)\n",
        "    n_models = len(models_dict)\n",
        "\n",
        "    plt.figure(figsize=(4 * n_models, 3 * n_images))\n",
        "\n",
        "    for img_idx, img_name in enumerate(image_list):\n",
        "        # Get category and construct full path\n",
        "        category_short = img_name.split('-')[1].split('_')[0]\n",
        "        category_full = folder_map[category_short]\n",
        "        img_path = os.path.join(test_folder, category_full, img_name + '.jpg')\n",
        "\n",
        "        # Load original image for reference\n",
        "        original_img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Plot original image\n",
        "        plt.subplot(n_images, n_models + 1, img_idx * (n_models + 1) + 1)\n",
        "        plt.imshow(original_img)\n",
        "        plt.title(f'Original\\n{img_name}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Generate and plot GradCAM for each model\n",
        "        for model_idx, (model_name, model) in enumerate(models_dict.items(), 1):\n",
        "            vis, conf, pred = get_gradcam_for_image(model, model_name.lower(), img_path, training_order)\n",
        "\n",
        "            if vis is not None:\n",
        "                plt.subplot(n_images, n_models + 1, img_idx * (n_models + 1) + model_idx + 1)\n",
        "                plt.imshow(vis)\n",
        "                plt.title(f'{model_name}\\nConf: {conf:.1f}%\\nPred: {training_order[pred]}')\n",
        "                plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use it like this:\n",
        "models_dict = {\n",
        "    'ResNet50': resnet50_model,\n",
        "    'ResNet101': resnet101_model,\n",
        "    'EfficientNetB0': efficientnetb0_model,\n",
        "    'EfficientNetB1': efficientnetb1_model\n",
        "}\n",
        "\n",
        "images_to_process = [\n",
        "    'Te-gl_0261',\n",
        "    'Te-gl_0037',\n",
        "    'Te-gl_0131',\n",
        "    'Te-me_0224',\n",
        "    'Te-me_0231',\n",
        "    'Te-me_0162',\n",
        "    'Te-pi_0205',\n",
        "    'Te-pi_0132',\n",
        "    'Te-pi_0110'\n",
        "]\n",
        "\n",
        "training_order = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "visualize_multiple_models_gradcam(\n",
        "    models_dict=models_dict,\n",
        "    image_list=images_to_process,\n",
        "    test_folder='/content/drive/MyDrive/Colab Notebooks/Resized_Testing',\n",
        "    training_order=training_order\n",
        ")"
      ],
      "metadata": {
        "id": "5j4Qc2czILt4"
      },
      "id": "5j4Qc2czILt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME for ResNet"
      ],
      "metadata": {
        "id": "wfwziKUG3-td"
      },
      "id": "wfwziKUG3-td"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zND4U3UB-bhy"
      },
      "id": "zND4U3UB-bhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME for EfficientNets"
      ],
      "metadata": {
        "id": "0Fi2qLBY4Ci5"
      },
      "id": "0Fi2qLBY4Ci5"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5e5xSCl-cAI"
      },
      "id": "I5e5xSCl-cAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Roll Out for ViT"
      ],
      "metadata": {
        "id": "Cp_H0AsoCBrq"
      },
      "id": "Cp_H0AsoCBrq"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "class VitAttentionRollout:\n",
        "    # [Previous VitAttentionRollout class implementation remains the same]\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.attention_layers = []\n",
        "        self.hooks = []\n",
        "\n",
        "    def register_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                with torch.no_grad():\n",
        "                    B, N, C = output.shape\n",
        "                    num_heads = self.model.blocks[0].attn.num_heads\n",
        "                    head_dim = C // (3 * num_heads)\n",
        "\n",
        "                    qkv = output.reshape(B, N, 3, num_heads, head_dim)\n",
        "                    qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "                    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "                    attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "                    attn = torch.nn.functional.softmax(attn, dim=-1)\n",
        "                    self.attention_layers.append(attn.detach())\n",
        "            return hook\n",
        "\n",
        "        for block in self.model.blocks:\n",
        "            self.hooks.append(block.attn.qkv.register_forward_hook(hook_fn(\"attn\")))\n",
        "\n",
        "    def cleanup(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n",
        "\n",
        "    def get_attention_rollout(self, input_tensor):\n",
        "        self.attention_layers = []\n",
        "        self.cleanup()\n",
        "        self.register_hooks()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(input_tensor)\n",
        "\n",
        "        self.cleanup()\n",
        "\n",
        "        attention_maps = [attn.mean(dim=1) for attn in self.attention_layers]\n",
        "        flat_attn = attention_maps[0]\n",
        "        for attn in attention_maps[1:]:\n",
        "            flat_attn = torch.matmul(attn, flat_attn)\n",
        "\n",
        "        attention = flat_attn[0, 1:, 1:]\n",
        "        attention = attention.mean(dim=-1)\n",
        "\n",
        "        grid_size = int(math.sqrt(attention.shape[0]))\n",
        "        attention = attention.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "        attention = (attention - attention.min()) / (attention.max() - attention.min())\n",
        "\n",
        "        return attention\n",
        "\n",
        "def create_attention_overlay(image, attention_map, alpha=0.6):\n",
        "    \"\"\"Create a bright attention overlay with original jet colormap\"\"\"\n",
        "    # Convert attention map to RGB using jet colormap\n",
        "    heatmap = plt.cm.jet(attention_map)[:, :, :3]\n",
        "\n",
        "    # Enhance heatmap brightness\n",
        "    heatmap = np.clip(heatmap * 1.2, 0, 1)\n",
        "\n",
        "    # Create the overlay with enhanced brightness\n",
        "    overlay = image * (1 - alpha) + heatmap * alpha\n",
        "\n",
        "    # Ensure values are in valid range\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    return overlay\n",
        "\n",
        "def visualize_attention_rollout(model, image_path, training_order=None):\n",
        "    \"\"\"Visualize attention rollout with brighter overlay\"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get model prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "            prediction = torch.argmax(output).item()\n",
        "            confidence = probabilities[0][prediction].item() * 100\n",
        "\n",
        "        # Get attention map\n",
        "        attention_rollout = VitAttentionRollout(model, device=device)\n",
        "        attention_map = attention_rollout.get_attention_rollout(input_tensor)\n",
        "\n",
        "        # Resize attention map\n",
        "        attention_map = F.interpolate(\n",
        "            torch.tensor(attention_map).unsqueeze(0).unsqueeze(0),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze().numpy()\n",
        "\n",
        "        # Convert image for visualization\n",
        "        img_np = np.array(image.resize((224, 224))) / 255.0\n",
        "\n",
        "        # Create enhanced overlay\n",
        "        overlay = create_attention_overlay(img_np, attention_map)\n",
        "\n",
        "        # Visualizations\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(image)\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Attention heatmap\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(attention_map, cmap='jet')\n",
        "        if training_order:\n",
        "            plt.title(f'Attention Map\\nPred: {training_order[prediction]}\\nConf: {confidence:.1f}%')\n",
        "        else:\n",
        "            plt.title(f'Attention Map\\nConfidence: {confidence:.1f}%')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Enhanced overlay\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(overlay)\n",
        "        plt.title('Attention Overlay')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return attention_map, prediction, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "5nHJ_9v6QSg-"
      },
      "id": "5nHJ_9v6QSg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First clear any existing hooks\n",
        "for name, module in vit_model.named_modules():\n",
        "    if hasattr(module, '_forward_hooks'):\n",
        "        module._forward_hooks.clear()\n",
        "\n",
        "# Then try visualization\n",
        "img_path = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0261.jpg'\n",
        "attention_map, pred, conf = visualize_attention_rollout(vit_model, img_path, tumor_types)"
      ],
      "metadata": {
        "id": "NjIyYtegQSm-"
      },
      "id": "NjIyYtegQSm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Attribution for Vit"
      ],
      "metadata": {
        "id": "D0KROVZu32RD"
      },
      "id": "D0KROVZu32RD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Roll Out for LeVit256"
      ],
      "metadata": {
        "id": "bu7E71gWIckF"
      },
      "id": "bu7E71gWIckF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difficulties: Attention roll-out assumes a coninous flow of attention through transformer layers, and LeVit uses shrinking attention that progressively reduces spatial dimensions."
      ],
      "metadata": {
        "id": "qLuZpccSvPdN"
      },
      "id": "qLuZpccSvPdN"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}