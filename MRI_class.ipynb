{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "wyIwW5TnyjlZ",
      "metadata": {
        "id": "wyIwW5TnyjlZ"
      },
      "source": [
        "# Importing Contigencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZEM2wpQNmyY2",
      "metadata": {
        "collapsed": true,
        "id": "ZEM2wpQNmyY2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hCbIVvFEneTA",
      "metadata": {
        "collapsed": true,
        "id": "hCbIVvFEneTA"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdba979-bc93-4446-98e7-27bc72966eac",
      "metadata": {
        "id": "2bdba979-bc93-4446-98e7-27bc72966eac"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hk_2wOvTzdbc",
      "metadata": {
        "id": "Hk_2wOvTzdbc"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbe62b3",
      "metadata": {
        "id": "2bbe62b3"
      },
      "source": [
        "# Data preprocessing\n",
        "## 1) Unzipping the MRI Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3dfc568",
      "metadata": {
        "id": "e3dfc568"
      },
      "outputs": [],
      "source": [
        "# Path to your zip file\n",
        "zip_path = '/content/drive/My Drive/Colab Notebooks/mri_images.zip'\n",
        "# Create a directory to extract the files\n",
        "extract_path = '/content/mri_images'\n",
        "os.makedirs(extract_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKm8-NADMa9o",
      "metadata": {
        "id": "bKm8-NADMa9o"
      },
      "outputs": [],
      "source": [
        "# Get the name of the zip file without the path\n",
        "zip_filename = os.path.basename(zip_path)\n",
        "\n",
        "# Get the name of the extracted folder (remove .zip extension)\n",
        "extracted_folder = os.path.splitext(zip_filename)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f5c440",
      "metadata": {
        "collapsed": true,
        "id": "b2f5c440"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(f\"Zip file: {zip_filename}\")\n",
        "print(f\"Extracted folder: {extracted_folder}\")\n",
        "print(f\"Files extracted to: {extract_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9925b00",
      "metadata": {
        "id": "f9925b00"
      },
      "outputs": [],
      "source": [
        "# Define the path to the glioma folder\n",
        "glioma_path = os.path.join('Testing', 'glioma')\n",
        "\n",
        "# Verify the path exists\n",
        "if os.path.exists(glioma_path):\n",
        "    print(f\"Successfully located the glioma folder at: {glioma_path}\")\n",
        "else:\n",
        "    print(\"Couldn't find the glioma folder. Please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4425e5",
      "metadata": {
        "id": "9e4425e5"
      },
      "source": [
        "#### Viewing a Random Image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be853e2e",
      "metadata": {
        "collapsed": true,
        "id": "be853e2e"
      },
      "outputs": [],
      "source": [
        "# Get a list of all image files in the glioma folder\n",
        "image_files = [f for f in os.listdir(glioma_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "if image_files:\n",
        "    # Select a random image\n",
        "    random_image = random.choice(image_files)\n",
        "    image_path = os.path.join(glioma_path, random_image)\n",
        "\n",
        "    # Open and display the image\n",
        "    img = Image.open(image_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample Glioma Image: {random_image}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No image files found in the glioma folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f7a8d6",
      "metadata": {
        "collapsed": true,
        "id": "f8f7a8d6"
      },
      "outputs": [],
      "source": [
        "# Get a list of all image files in the glioma folder\n",
        "image_files = [f for f in os.listdir(glioma_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "if image_files:\n",
        "    # Select a random image\n",
        "    random_image = random.choice(image_files)\n",
        "    image_path = os.path.join(glioma_path, random_image)\n",
        "\n",
        "    # Open the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img_array, cmap='gray')  # Use 'gray' colormap\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample Glioma Image: {random_image}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No image files found in the glioma folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575c4bb2",
      "metadata": {
        "id": "575c4bb2"
      },
      "source": [
        "## 2) Resizing all the images to ResNet-50 input size and Saving them in a new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92deaabc",
      "metadata": {
        "id": "92deaabc"
      },
      "outputs": [],
      "source": [
        "def resize_images(input_folder, output_folder, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Resize all images in the input folder and save them to the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the folder containing original images\n",
        "    output_folder (str): Path to the folder where resized images will be saved\n",
        "    target_size (tuple): The target size for the images (width, height)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "            try:\n",
        "                img = Image.open(os.path.join(input_folder, filename))\n",
        "                img = img.resize(target_size, Image.LANCZOS)\n",
        "                img.save(os.path.join(output_folder, filename))\n",
        "                print(f\"Resized {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "# Define the size you want for all images\n",
        "target_size = (224, 224)  # This is a common size for many CNN architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655a34c8",
      "metadata": {
        "collapsed": true,
        "id": "655a34c8"
      },
      "outputs": [],
      "source": [
        "# Define the base directories\n",
        "base_input_dir = 'Training'  # Adjust this if your folder structure is different\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Resize images for each tumor type\n",
        "for tumor_type in tumor_types:\n",
        "    input_folder = os.path.join(base_input_dir, tumor_type)\n",
        "    output_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "    print(f\"Resizing images in {tumor_type} folder...\")\n",
        "    resize_images(input_folder, output_folder, target_size)\n",
        "\n",
        "print(\"Image resizing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zC4nD9LUaOde",
      "metadata": {
        "collapsed": true,
        "id": "zC4nD9LUaOde"
      },
      "outputs": [],
      "source": [
        "# Define the base directories\n",
        "base_input_dir = 'Testing'  # Adjust this if your folder structure is different\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Resize images for each tumor type\n",
        "for tumor_type in tumor_types:\n",
        "    input_folder = os.path.join(base_input_dir, tumor_type)\n",
        "    output_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "    print(f\"Resizing images in {tumor_type} folder...\")\n",
        "    resize_images(input_folder, output_folder, target_size)\n",
        "\n",
        "print(\"Image resizing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UlXDjPjnaF0V",
      "metadata": {
        "id": "UlXDjPjnaF0V"
      },
      "source": [
        "### Loading the resized folders from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-p9pcBvbTcOT",
      "metadata": {
        "collapsed": true,
        "id": "-p9pcBvbTcOT"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if not already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Choose a random tumor type and image\n",
        "tumor_type = random.choice(tumor_types)\n",
        "resized_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(resized_folder):\n",
        "    print(f\"Error: The folder {resized_folder} does not exist in your Google Drive.\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(resized_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Error: No image files found in {resized_folder}\")\n",
        "    else:\n",
        "        random_image = random.choice(image_files)\n",
        "\n",
        "        # Display the image\n",
        "        img_path = os.path.join(resized_folder, random_image)\n",
        "        img = Image.open(img_path)\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img_array, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Resized {tumor_type} Image: {random_image}\\nSize: {img_array.shape}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Image size: {img_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SB008AFDbJ16",
      "metadata": {
        "collapsed": true,
        "id": "SB008AFDbJ16"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if not already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory in Google Drive\n",
        "base_output_dir = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "\n",
        "# List of tumor types\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Choose a random tumor type and image\n",
        "tumor_type = random.choice(tumor_types)\n",
        "resized_folder = os.path.join(base_output_dir, tumor_type)\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(resized_folder):\n",
        "    print(f\"Error: The folder {resized_folder} does not exist in your Google Drive.\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(resized_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Error: No image files found in {resized_folder}\")\n",
        "    else:\n",
        "        random_image = random.choice(image_files)\n",
        "\n",
        "        # Display the image\n",
        "        img_path = os.path.join(resized_folder, random_image)\n",
        "        img = Image.open(img_path)\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img_array, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Resized {tumor_type} Image: {random_image}\\nSize: {img_array.shape}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Image size: {img_array.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkbckuT3y-Jd",
      "metadata": {
        "id": "xkbckuT3y-Jd"
      },
      "source": [
        "# PyTorch Data Pre-Processing\n",
        "\n",
        "## 1) DataSet & DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ypBOqduzIQ-",
      "metadata": {
        "id": "6ypBOqduzIQ-"
      },
      "source": [
        "Importing PyTorch contigencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1kRaIX1pRqu",
      "metadata": {
        "id": "S1kRaIX1pRqu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MJujwySKzRis",
      "metadata": {
        "id": "MJujwySKzRis"
      },
      "source": [
        "Defining the data sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mc-By-K9pST7",
      "metadata": {
        "id": "Mc-By-K9pST7"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BES8ulaozTmz",
      "metadata": {
        "id": "BES8ulaozTmz"
      },
      "source": [
        "DataSet Class:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66863b94",
      "metadata": {
        "id": "66863b94"
      },
      "source": [
        "## 2) Data Augmentation\n",
        "\n",
        "1. Random Crop\n",
        "2. Random Flip\n",
        "3. Random Rotate\n",
        "4. Random Brightness\n",
        "5. Random Contrast\n",
        "6. On the fly augmentation during training (saves memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rd1WSxuFCMrx",
      "metadata": {
        "id": "rd1WSxuFCMrx"
      },
      "source": [
        "__Important__\n",
        "\n",
        "1) Avoid extreme rotations or flips that could change the anatomical orientation.\n",
        "\n",
        "2) Be cautious with color-based augmentations, as MRI intensity values often have specific meanings.\n",
        "\n",
        "3)Maintain the overall structure and proportions of the brain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AEm0XQZMCghb",
      "metadata": {
        "id": "AEm0XQZMCghb"
      },
      "source": [
        "1. Slight rotations (within ±10 degrees)\n",
        "2. Small shifts (translations)\n",
        "3. Zoom in/out (within a small range)\n",
        "4. Minimal brightness and contrast adjustments\n",
        "5. Gaussian noise addition (to simulate image noise)\n",
        "6. Elastic deformations (subtle warping)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ePbW7TwvLv",
      "metadata": {
        "id": "c0ePbW7TwvLv"
      },
      "outputs": [],
      "source": [
        "class RandomMRIAugmentation(nn.Module):\n",
        "    def __init__(self, rotation_range=10, translation_range=0.1, zoom_range=0.1, noise_factor=0.05, p=0.5):\n",
        "        super().__init__()\n",
        "        self.rotation_range = rotation_range\n",
        "        self.translation_range = translation_range\n",
        "        self.zoom_range = zoom_range\n",
        "        self.noise_factor = noise_factor\n",
        "        self.p = p  # Probability of applying each augmentation\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Ensure input is a tensor\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img = TF.to_tensor(img)\n",
        "\n",
        "        # Random rotation\n",
        "        if random.random() < self.p:\n",
        "            angle = random.uniform(-self.rotation_range, self.rotation_range)\n",
        "            img = TF.rotate(img, angle)\n",
        "\n",
        "        # Random translation\n",
        "        if random.random() < self.p:\n",
        "            translate = [random.uniform(-self.translation_range, self.translation_range) for _ in range(2)]\n",
        "            img = TF.affine(img, angle=0, translate=translate, scale=1, shear=0)\n",
        "\n",
        "        # Random zoom\n",
        "        if random.random() < self.p:\n",
        "            scale = random.uniform(1-self.zoom_range, 1+self.zoom_range)\n",
        "            img = TF.affine(img, angle=0, translate=(0,0), scale=scale, shear=0)\n",
        "\n",
        "        # Add Gaussian noise\n",
        "        if random.random() < self.p:\n",
        "            noise = torch.randn_like(img) * self.noise_factor\n",
        "            img = img + noise\n",
        "            img = torch.clamp(img, 0, 1)\n",
        "\n",
        "        return img\n",
        "\n",
        "def elastic_transform(image, alpha=1000, sigma=30, alpha_affine=30):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003].\"\"\"\n",
        "    random_state = np.random.RandomState(None)\n",
        "\n",
        "    shape = image.shape\n",
        "    shape_size = shape[:2]\n",
        "\n",
        "    # Random affine\n",
        "    center_square = np.float32(shape_size) // 2\n",
        "    square_size = min(shape_size) // 3\n",
        "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
        "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
        "    M = cv2.getAffineTransform(pts1, pts2)\n",
        "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
        "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
        "\n",
        "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
        "\n",
        "class RandomElasticDeformation(nn.Module):\n",
        "    def __init__(self, p=0.2, alpha=1000, sigma=30, alpha_affine=30):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.alpha = alpha\n",
        "        self.sigma = sigma\n",
        "        self.alpha_affine = alpha_affine\n",
        "\n",
        "    def forward(self, img):\n",
        "        if random.random() < self.p:\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.numpy()\n",
        "            img = elastic_transform(img, self.alpha, self.sigma, self.alpha_affine)\n",
        "            img = torch.from_numpy(img)\n",
        "        return img\n",
        "\n",
        "def get_mri_augmentation(p_transform=0.5, p_elastic=0.2):\n",
        "    return transforms.Compose([\n",
        "        RandomMRIAugmentation(rotation_range=10, translation_range=0.1, zoom_range=0.1, noise_factor=0.05, p=p_transform),\n",
        "        RandomElasticDeformation(p=p_elastic),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust these values based on your MRI data statistics\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AE-Cp55NC_1Y",
      "metadata": {
        "id": "AE-Cp55NC_1Y"
      },
      "outputs": [],
      "source": [
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, folder_path, tumor_types, transform=None, augment=False):\n",
        "        self.folder_path = folder_path\n",
        "        self.tumor_types = tumor_types\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, tumor_type in enumerate(tumor_types):\n",
        "            tumor_folder = os.path.join(folder_path, tumor_type)\n",
        "            for img_name in os.listdir(tumor_folder):\n",
        "                self.image_paths.append(os.path.join(tumor_folder, img_name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        if self.augment:\n",
        "            self.aug_transform = get_mri_augmentation(p_transform=0.5, p_elastic=0.2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            image = self.aug_transform(image)\n",
        "        elif self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJiXsfrYy5z0",
      "metadata": {
        "id": "gJiXsfrYy5z0"
      },
      "outputs": [],
      "source": [
        "def create_mri_datasets(train_folder, test_folder, tumor_types, val_split=0.2, batch_size=32):\n",
        "    # Define base transform for validation and test sets\n",
        "    base_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust based on your MRI data\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    full_train_dataset = MRIDataset(train_folder, tumor_types, augment=True)\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=base_transform)\n",
        "\n",
        "    # Split the training dataset into train and validation\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(full_train_dataset)),\n",
        "        test_size=val_split,\n",
        "        stratify=full_train_dataset.labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(full_train_dataset, val_indices)\n",
        "\n",
        "    # Override the transform for the validation set\n",
        "    val_dataset.dataset.augment = False\n",
        "    val_dataset.dataset.transform = base_transform\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uihuQoeVy85r",
      "metadata": {
        "collapsed": true,
        "id": "uihuQoeVy85r"
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "train_loader, val_loader, test_loader = create_mri_datasets(train_folder, test_folder, tumor_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vhIU243IzIoS",
      "metadata": {
        "id": "vhIU243IzIoS"
      },
      "source": [
        "\n",
        "# Model 1: ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k9DJnSA3G-Z9",
      "metadata": {
        "collapsed": true,
        "id": "k9DJnSA3G-Z9"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "INJfnB-SGEn_",
      "metadata": {
        "id": "INJfnB-SGEn_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kvtM9IHZ1DsW",
      "metadata": {
        "id": "kvtM9IHZ1DsW"
      },
      "source": [
        "Defining the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPjQzy9bCpol",
      "metadata": {
        "id": "TPjQzy9bCpol"
      },
      "outputs": [],
      "source": [
        "def initialize_model(num_classes):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "             best_val_loss = val_loss\n",
        "             best_val_acc = val_acc\n",
        "             epochs_no_improve = 0\n",
        "             best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8x_TJ816mZj4",
      "metadata": {
        "id": "8x_TJ816mZj4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "def visualize_model_attention(model, input_tensor, target_class):\n",
        "    model.eval()\n",
        "    cam = GradCAM(model=model, target_layers=[model.layer4[-1]], use_cuda=torch.cuda.is_available())\n",
        "    grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0), target_category=target_class)\n",
        "    visualization = show_cam_on_image(input_tensor.permute(1, 2, 0).numpy(), grayscale_cam[0, :], use_rgb=True)\n",
        "    plt.imshow(visualization)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Grad-CAM for class {target_class}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DDdlzmlqlXv",
      "metadata": {
        "id": "0DDdlzmlqlXv"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6  # Size in MB\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "def create_metrics_dataframe(model, test_acc, precision, recall, f1, auc_roc, train_time, test_loss):\n",
        "    metrics = {\n",
        "        'Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "        'Value': [\n",
        "            test_acc,\n",
        "            f1,\n",
        "            test_loss,\n",
        "            train_time,\n",
        "            sum(p.numel() for p in model.parameters()),\n",
        "            get_model_size(model)\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(metrics)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thgX5dh7gc-f",
      "metadata": {
        "id": "thgX5dh7gc-f"
      },
      "outputs": [],
      "source": [
        "# Define the base path in your Google Drive\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# Function to save the model\n",
        "def save_model(model, filename):\n",
        "    save_path = os.path.join(base_path, filename)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved successfully to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(model, filename):\n",
        "    load_path = os.path.join(base_path, filename)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(load_path))\n",
        "        print(f\"Model loaded successfully from {load_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rcyyQz2V1Sbb",
      "metadata": {
        "id": "rcyyQz2V1Sbb"
      },
      "source": [
        "Training Loop: GPU T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UP_z22M1OqN-",
      "metadata": {
        "collapsed": true,
        "id": "UP_z22M1OqN-"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'model_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_mri_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s9FaNSwIjB73",
      "metadata": {
        "id": "s9FaNSwIjB73"
      },
      "source": [
        "### GradCam for ResNet:\n",
        "Load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9jYZyKzTnbIB",
      "metadata": {
        "collapsed": true,
        "id": "9jYZyKzTnbIB"
      },
      "outputs": [],
      "source": [
        "resnet_model = initialize_model(num_classes=len(tumor_types))\n",
        "resnet_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_mri_classification_model.pth'))\n",
        "resnet_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iRZFw09HmQc9",
      "metadata": {
        "id": "iRZFw09HmQc9"
      },
      "source": [
        "Grad-Cam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qKY0JsrX_ETw",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qKY0JsrX_ETw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_gradcam_visualization(model, image_path, training_order, target_layer_name='layer4'):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM visualization using the correct class order from training\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Use the same transforms as training\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Display names mapping\n",
        "        display_names = {\n",
        "            'glioma': 'Glioma',\n",
        "            'meningioma': 'Meningioma',\n",
        "            'notumor': 'Normal',\n",
        "            'pituitary': 'Pituitary'\n",
        "        }\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get target layer\n",
        "        target_layer = model.layer4[-1]\n",
        "\n",
        "        grad_cam = GradCAM(\n",
        "            model=model,\n",
        "            target_layers=[target_layer],\n",
        "            reshape_transform=None\n",
        "        )\n",
        "\n",
        "        # Get model prediction and confidence\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "            prediction = torch.argmax(output).item()\n",
        "            confidence = probabilities[0][prediction].item() * 100\n",
        "\n",
        "            predicted_class = training_order[prediction]\n",
        "            display_class = display_names[predicted_class]\n",
        "\n",
        "        targets = [ClassifierOutputTarget(prediction)]\n",
        "        grayscale_cam = grad_cam(input_tensor=input_tensor, targets=targets)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        rgb_img = np.array(image.resize((224, 224))) / 255.0\n",
        "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        return visualization, display_class, rgb_img, confidence, probabilities[0].cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "def visualize_correct_predictions(model, test_folder, training_order, samples_per_class=3):\n",
        "    \"\"\"\n",
        "    Visualize Grad-CAM for correctly predicted images using original training order\n",
        "    \"\"\"\n",
        "    display_names = {\n",
        "        'glioma': 'Glioma',\n",
        "        'meningioma': 'Meningioma',\n",
        "        'notumor': 'Normal',\n",
        "        'pituitary': 'Pituitary'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Store correct predictions for each class\n",
        "        correct_predictions = defaultdict(list)\n",
        "\n",
        "        # First pass: collect correct predictions\n",
        "        for class_name in training_order:\n",
        "            folder_path = os.path.join(test_folder, class_name)\n",
        "\n",
        "            if not os.path.exists(folder_path):\n",
        "                print(f\"Skipping {class_name} - folder not found: {folder_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {display_names[class_name]}...\")\n",
        "            image_files = [f for f in os.listdir(folder_path)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "            # Process all images in the folder\n",
        "            for img_file in image_files:\n",
        "                img_path = os.path.join(folder_path, img_file)\n",
        "                vis, pred_class, original, confidence, probs = get_gradcam_visualization(\n",
        "                    model, img_path, training_order)\n",
        "\n",
        "                # Store correct predictions\n",
        "                if vis is not None and pred_class == display_names[class_name]:\n",
        "                    correct_predictions[class_name].append({\n",
        "                        'path': img_path,\n",
        "                        'visualization': vis,\n",
        "                        'original': original,\n",
        "                        'confidence': confidence,\n",
        "                        'probabilities': probs\n",
        "                    })\n",
        "\n",
        "        # Create visualization for correct predictions\n",
        "        plt.figure(figsize=(15, 3 * len(training_order)))\n",
        "\n",
        "        for i, class_name in enumerate(training_order):\n",
        "            predictions = correct_predictions[class_name]\n",
        "\n",
        "            # Sort by confidence and take top k\n",
        "            predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "            selected_predictions = predictions[:samples_per_class]\n",
        "\n",
        "            print(f\"\\nShowing top {len(selected_predictions)} correct predictions for {display_names[class_name]}\")\n",
        "            print(f\"Total correct predictions: {len(predictions)}\")\n",
        "\n",
        "            for j, pred in enumerate(selected_predictions):\n",
        "                plt.subplot(len(training_order), samples_per_class * 2, i * samples_per_class * 2 + j * 2 + 1)\n",
        "                plt.imshow(pred['original'])\n",
        "                plt.title(f'Original\\n{display_names[class_name]}\\nConf: {pred[\"confidence\"]:.1f}%')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(len(training_order), samples_per_class * 2, i * samples_per_class * 2 + j * 2 + 2)\n",
        "                plt.imshow(pred['visualization'])\n",
        "                plt.title('Grad-CAM')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Print probabilities for each class\n",
        "                print(f\"\\nProbabilities for {display_names[class_name]} image {j+1}:\")\n",
        "                for cls, prob in zip(training_order, pred['probabilities']):\n",
        "                    print(f\"{display_names[cls]}: {prob*100:.2f}%\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(\"\\nSummary of correct predictions:\")\n",
        "        for class_name in training_order:\n",
        "            total = len(correct_predictions[class_name])\n",
        "            print(f\"{display_names[class_name]}: {total} correct predictions\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during visualization: {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "training_order = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "print(\"Generating Grad-CAM visualizations for correctly classified images...\")\n",
        "visualize_correct_predictions(\n",
        "    model=resnet_model,\n",
        "    test_folder=test_folder,\n",
        "    training_order=training_order,\n",
        "    samples_per_class=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93PrO5bF1pHh",
      "metadata": {
        "id": "93PrO5bF1pHh"
      },
      "source": [
        "# Model 2: EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tIfH-g2j1pfD",
      "metadata": {
        "id": "tIfH-g2j1pfD"
      },
      "outputs": [],
      "source": [
        "def initialize_model(num_classes):\n",
        "    model = models.efficientnet_b0(pretrained=True)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JeOu-qn0LZt3",
      "metadata": {
        "id": "JeOu-qn0LZt3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "             best_val_loss = val_loss\n",
        "             best_val_acc = val_acc\n",
        "             epochs_no_improve = 0\n",
        "             best_model = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    return model, best_val_acc, training_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vN3yqs_dFHiv",
      "metadata": {
        "id": "vN3yqs_dFHiv"
      },
      "outputs": [],
      "source": [
        "def visualize_model_attention(model, input_tensor, target_class):\n",
        "    model.eval()\n",
        "    cam = GradCAM(model=model, target_layers=[model.features[-1]], use_cuda=torch.cuda.is_available())\n",
        "    grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0), target_category=target_class)\n",
        "    visualization = show_cam_on_image(input_tensor.permute(1, 2, 0).numpy(), grayscale_cam[0, :], use_rgb=True)\n",
        "    plt.imshow(visualization)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Grad-CAM for class {target_class}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vr3mgWHzFgw2",
      "metadata": {
        "collapsed": true,
        "id": "vr3mgWHzFgw2"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the dataset\n",
        "    full_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "    results = []\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "        model = initialize_model(num_classes)\n",
        "        model, val_acc, train_time = train_model(model, train_loader, val_loader)\n",
        "\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "        # Save the model for this fold\n",
        "        save_model(model, f'efficientnet_b0_model_fold_{fold+1}.pth')\n",
        "\n",
        "    # After k-fold cross-validation, train on the entire training set\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_model = initialize_model(num_classes)\n",
        "    final_model, final_val_acc, final_train_time = train_model(final_model, train_loader, val_loader)\n",
        "\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Validation Accuracy': final_val_acc,\n",
        "        'Training Time (s)': final_train_time\n",
        "    })\n",
        "\n",
        "    # Create and display the summary table\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    test_acc, precision, recall, f1, auc_roc, test_loss, _, _ = evaluate_model(final_model, test_loader, tumor_types)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Create and display the metrics DataFrame\n",
        "    metrics_df = create_metrics_dataframe(final_model, test_acc, precision, recall, f1, auc_roc, final_train_time, test_loss)\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    # Save the DataFrame\n",
        "    metrics_csv_path = os.path.join(base_path, 'efficientnet_b0_model_metrics.csv')\n",
        "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "    print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "    # Save the final model\n",
        "    save_model(final_model, 'final_efficientnet_b0_mri_classification_model.pth')\n",
        "\n",
        "    print(\"Training, evaluation, and metrics logging complete for EfficientNet-B0 model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99p_lbwf0Hq4",
      "metadata": {
        "id": "99p_lbwf0Hq4"
      },
      "source": [
        "### Grad-Cam for EfficientNet\n",
        "Load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KvLSPiqp0G_W",
      "metadata": {
        "collapsed": true,
        "id": "KvLSPiqp0G_W"
      },
      "outputs": [],
      "source": [
        "efficientnet_model = initialize_model(num_classes=len(tumor_types))\n",
        "efficientnet_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_efficientnet_b0_mri_classification_model.pth'))\n",
        "efficientnet_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0QYYQ9yh0fbw",
      "metadata": {
        "id": "0QYYQ9yh0fbw"
      },
      "source": [
        "Grad-cam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "taCL7Lf_0N6U",
      "metadata": {
        "id": "taCL7Lf_0N6U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_gradcam_visualization(model, image_path, training_order, target_layer_name='features.8'):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM visualization for EfficientNetB0 using the correct class order from training\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Use the same transforms as training\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Display names mapping\n",
        "        display_names = {\n",
        "            'glioma': 'Glioma',\n",
        "            'meningioma': 'Meningioma',\n",
        "            'notumor': 'Normal',\n",
        "            'pituitary': 'Pituitary'\n",
        "        }\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get target layer - for EfficientNetB0, we use the final MBConv block\n",
        "        target_layer = model.features[8]\n",
        "\n",
        "        grad_cam = GradCAM(\n",
        "            model=model,\n",
        "            target_layers=[target_layer],\n",
        "            reshape_transform=None\n",
        "        )\n",
        "\n",
        "        # Get model prediction and confidence\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "            prediction = torch.argmax(output).item()\n",
        "            confidence = probabilities[0][prediction].item() * 100\n",
        "\n",
        "            predicted_class = training_order[prediction]\n",
        "            display_class = display_names[predicted_class]\n",
        "\n",
        "        targets = [ClassifierOutputTarget(prediction)]\n",
        "        grayscale_cam = grad_cam(input_tensor=input_tensor, targets=targets)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        rgb_img = np.array(image.resize((224, 224))) / 255.0\n",
        "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        return visualization, display_class, rgb_img, confidence, probabilities[0].cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "def visualize_correct_predictions(model, test_folder, training_order, samples_per_class=3):\n",
        "    \"\"\"\n",
        "    Visualize Grad-CAM for correctly predicted images using original training order\n",
        "    \"\"\"\n",
        "    display_names = {\n",
        "        'glioma': 'Glioma',\n",
        "        'meningioma': 'Meningioma',\n",
        "        'notumor': 'Normal',\n",
        "        'pituitary': 'Pituitary'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Store correct predictions for each class\n",
        "        correct_predictions = defaultdict(list)\n",
        "\n",
        "        # First pass: collect correct predictions\n",
        "        for class_name in training_order:\n",
        "            folder_path = os.path.join(test_folder, class_name)\n",
        "\n",
        "            if not os.path.exists(folder_path):\n",
        "                print(f\"Skipping {class_name} - folder not found: {folder_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {display_names[class_name]}...\")\n",
        "            image_files = [f for f in os.listdir(folder_path)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "            # Process all images in the folder\n",
        "            for img_file in image_files:\n",
        "                img_path = os.path.join(folder_path, img_file)\n",
        "                vis, pred_class, original, confidence, probs = get_gradcam_visualization(\n",
        "                    model, img_path, training_order)\n",
        "\n",
        "                # Store correct predictions\n",
        "                if vis is not None and pred_class == display_names[class_name]:\n",
        "                    correct_predictions[class_name].append({\n",
        "                        'path': img_path,\n",
        "                        'visualization': vis,\n",
        "                        'original': original,\n",
        "                        'confidence': confidence,\n",
        "                        'probabilities': probs\n",
        "                    })\n",
        "\n",
        "        # Create visualization for correct predictions\n",
        "        plt.figure(figsize=(15, 3 * len(training_order)))\n",
        "\n",
        "        for i, class_name in enumerate(training_order):\n",
        "            predictions = correct_predictions[class_name]\n",
        "\n",
        "            # Sort by confidence and take top k\n",
        "            predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "            selected_predictions = predictions[:samples_per_class]\n",
        "\n",
        "            print(f\"\\nShowing top {len(selected_predictions)} correct predictions for {display_names[class_name]}\")\n",
        "            print(f\"Total correct predictions: {len(predictions)}\")\n",
        "\n",
        "            for j, pred in enumerate(selected_predictions):\n",
        "                plt.subplot(len(training_order), samples_per_class * 2, i * samples_per_class * 2 + j * 2 + 1)\n",
        "                plt.imshow(pred['original'])\n",
        "                plt.title(f'Original\\n{display_names[class_name]}\\nConf: {pred[\"confidence\"]:.1f}%')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(len(training_order), samples_per_class * 2, i * samples_per_class * 2 + j * 2 + 2)\n",
        "                plt.imshow(pred['visualization'])\n",
        "                plt.title('Grad-CAM')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Print probabilities for each class\n",
        "                print(f\"\\nProbabilities for {display_names[class_name]} image {j+1}:\")\n",
        "                for cls, prob in zip(training_order, pred['probabilities']):\n",
        "                    print(f\"{display_names[cls]}: {prob*100:.2f}%\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(\"\\nSummary of correct predictions:\")\n",
        "        for class_name in training_order:\n",
        "            total = len(correct_predictions[class_name])\n",
        "            print(f\"{display_names[class_name]}: {total} correct predictions\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during visualization: {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "print(\"Generating Grad-CAM visualizations for correctly classified images...\")\n",
        "visualize_correct_predictions(\n",
        "    model=efficientnet_model,\n",
        "    test_folder=test_folder,\n",
        "    training_order=tumor_types,\n",
        "    samples_per_class=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7sFe-Abw2Ft4",
      "metadata": {
        "id": "7sFe-Abw2Ft4"
      },
      "source": [
        "##Remark:\n",
        "EfficienNet and ResNet are both CNNs that specialize in edge detection by nature. So it is normal to observe that model attention goes to the edge of tumours instead of directly hovering above it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YTJCtkV1whjz",
      "metadata": {
        "id": "YTJCtkV1whjz"
      },
      "source": [
        "## Comparaison of ResNet50 and EfficientNetB0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMxZz1ZQvYBN",
      "metadata": {
        "id": "pMxZz1ZQvYBN"
      },
      "outputs": [],
      "source": [
        "# Assuming the data is already in the format you provided\n",
        "data = {\n",
        "    'EfficientNet_Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "    'EfficientNet_Value': [9.847445e-01, 9.846879e-01, 5.018997e-02, 2.509030e+03, 4.012672e+06, 1.632957e+01],\n",
        "    'OtherModel_Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "    'OtherModel_Value': [9.938978e-01, 9.938897e-01, 2.039252e-02, 2.520090e+03, 2.351623e+07, 9.436445e+01]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Create separate bar charts for each metric\n",
        "metrics = df['EfficientNet_Metric']\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Comparison of Model Metrics', fontsize=16)\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i // 3, i % 3]\n",
        "\n",
        "    efficientnet_value = df.loc[df['EfficientNet_Metric'] == metric, 'EfficientNet_Value'].values[0]\n",
        "    othermodel_value = df.loc[df['OtherModel_Metric'] == metric, 'OtherModel_Value'].values[0]\n",
        "\n",
        "    ax.bar(['EfficientNet', 'Other Model'], [efficientnet_value, othermodel_value])\n",
        "    ax.set_title(metric)\n",
        "    ax.set_ylabel('Value')\n",
        "\n",
        "    # Format y-axis labels based on the metric\n",
        "    if 'Parameters' in metric or 'Time' in metric:\n",
        "        ax.set_yscale('log')\n",
        "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "    elif 'Accuracy' in metric or 'F1 Score' in metric:\n",
        "        ax.set_ylim(0.95, 1.0)  # Adjust as needed\n",
        "\n",
        "    # Add value labels on top of each bar\n",
        "    for j, v in enumerate([efficientnet_value, othermodel_value]):\n",
        "        ax.text(j, v, f'{v:.2e}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "plt.show()\n",
        "\n",
        "# Create a table to show the exact values\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.5)\n",
        "plt.title('Model Metrics Comparison Table')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bce9f6SBwnrM",
      "metadata": {
        "id": "Bce9f6SBwnrM"
      },
      "source": [
        "\n",
        "\n",
        "__It make sense to use EfficientNetB0, because it has a lot less parameters than ResNet50, yet achieves almost the same accuracy.__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZqxUGTdOWIIV",
      "metadata": {
        "id": "ZqxUGTdOWIIV"
      },
      "source": [
        "# Model 3: Levit-256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JamnTZowC5Ob",
      "metadata": {
        "id": "JamnTZowC5Ob"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import timm\n",
        "import time\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1VhX8IwcVog",
      "metadata": {
        "id": "N1VhX8IwcVog"
      },
      "outputs": [],
      "source": [
        "class LeViT256Model(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(LeViT256Model, self).__init__()\n",
        "\n",
        "        # Load pretrained LeViT-256 model\n",
        "        self.levit = timm.create_model('levit_256', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Get the number of features from LeViT\n",
        "        levit_num_features = self.levit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(levit_num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through LeViT\n",
        "        features = self.levit(x)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_levit_model(num_classes, pretrained=True):\n",
        "    return LeViT256Model(num_classes, pretrained)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "# Function to create metrics DataFrame\n",
        "def create_metrics_dataframe(model, test_acc, precision, recall, f1, auc_roc, train_time, test_loss):\n",
        "    metrics = {\n",
        "        'Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "        'Value': [\n",
        "            test_acc,\n",
        "            f1,\n",
        "            test_loss,\n",
        "            train_time,\n",
        "            sum(p.numel() for p in model.parameters()),\n",
        "            sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(metrics)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qbwe9hchh0rU",
      "metadata": {
        "id": "Qbwe9hchh0rU"
      },
      "outputs": [],
      "source": [
        "def train_levit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_levit_model(num_classes).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save the first model state or if we have a new best validation loss\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Final training on entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_levit_model(num_classes).to(device)\n",
        "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None  # Initialize best_model\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        # Validation phase\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save the first model state or if we have a new best validation loss\n",
        "        if best_model is None or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = final_model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Load the best model state\n",
        "    final_model.load_state_dict(best_model)\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_levit_256_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMtGcId4hbH_",
      "metadata": {
        "id": "cMtGcId4hbH_"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable torch compile to avoid dynamo issues\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        # Print dataset sizes\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the LeViT-256 model\n",
        "        results, final_model = train_levit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_256_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_256_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_256_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_256_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()  # This will print the full error traceback\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nLeViT-256 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DRYKigg42pbu",
      "metadata": {
        "id": "DRYKigg42pbu"
      },
      "source": [
        "## New GradCam Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AcOWiqUT5CkF",
      "metadata": {
        "collapsed": true,
        "id": "AcOWiqUT5CkF"
      },
      "outputs": [],
      "source": [
        "levit256_model = initialize_levit_model(num_classes=4)\n",
        "levit256_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_256_model_final.pth')['model_state_dict'])\n",
        "levit256_model = levit256_model.cuda()\n",
        "levit256_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5E1pkczPvGq",
      "metadata": {
        "id": "h5E1pkczPvGq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Vq84Lo0e2poF",
      "metadata": {
        "id": "Vq84Lo0e2poF"
      },
      "source": [
        "## Old GradCam Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u_a3wG7Z0lCc",
      "metadata": {
        "id": "u_a3wG7Z0lCc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def visualize_gradcam(model, image_path, transform, tumor_types, target_class=None, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Visualize Grad-CAM for a given image and model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained LeViT model\n",
        "        image_path: Path to the image file\n",
        "        transform: Transformation pipeline\n",
        "        tumor_types: List of tumor type labels\n",
        "        target_class: Specific class to visualize (optional)\n",
        "        alpha: Transparency of the heatmap overlay\n",
        "    \"\"\"\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(img).unsqueeze(0)\n",
        "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
        "\n",
        "    # Get the target layer (last convolutional layer in LeViT)\n",
        "    target_layer = model.levit.stages[-1][-1].norm  # Adjust this based on LeViT architecture\n",
        "\n",
        "    # Initialize GradCAM\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    # If target class is not specified, use predicted class\n",
        "    if target_class is None:\n",
        "        target_class = predicted_class\n",
        "\n",
        "    # Generate heatmap\n",
        "    heatmap = grad_cam.get_cam(input_tensor, target_class)\n",
        "\n",
        "    # Convert image to numpy array\n",
        "    orig_img = np.array(img)\n",
        "\n",
        "    # Resize heatmap to match image size\n",
        "    heatmap = cv2.resize(heatmap, (orig_img.shape[1], orig_img.shape[0]))\n",
        "\n",
        "    # Convert heatmap to RGB\n",
        "    heatmap_rgb = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create superimposed image\n",
        "    superimposed = cv2.addWeighted(orig_img, 1-alpha, heatmap_rgb, alpha, 0)\n",
        "\n",
        "    # Plot the results\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image\n",
        "    ax[0].imshow(orig_img)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Heatmap\n",
        "    ax[1].imshow(heatmap_rgb)\n",
        "    ax[1].set_title('Grad-CAM Heatmap')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    # Superimposed\n",
        "    ax[2].imshow(superimposed)\n",
        "    ax[2].set_title('Superimposed')\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    # Add prediction information\n",
        "    pred_prob = probabilities[0][predicted_class].item() * 100\n",
        "    fig.suptitle(f'Prediction: {tumor_types[predicted_class]} ({pred_prob:.2f}%)', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return heatmap, predicted_class, probabilities[0].cpu().numpy()\n",
        "\n",
        "def visualize_batch_gradcam(model, image_paths, transform, tumor_types, output_dir=None):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM visualizations for a batch of images.\n",
        "\n",
        "    Args:\n",
        "        model: The trained LeViT model\n",
        "        image_paths: List of paths to images\n",
        "        transform: Transformation pipeline\n",
        "        tumor_types: List of tumor type labels\n",
        "        output_dir: Directory to save visualizations (optional)\n",
        "    \"\"\"\n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        print(f\"\\nProcessing image {idx+1}/{len(image_paths)}\")\n",
        "        print(f\"Image path: {img_path}\")\n",
        "\n",
        "        heatmap, pred_class, probs = visualize_gradcam(model, img_path, transform, tumor_types)\n",
        "\n",
        "        # Print prediction probabilities for each class\n",
        "        print(\"\\nPrediction probabilities:\")\n",
        "        for i, (tumor_type, prob) in enumerate(zip(tumor_types, probs)):\n",
        "            print(f\"{tumor_type}: {prob*100:.2f}%\")\n",
        "\n",
        "        if output_dir:\n",
        "            plt.savefig(os.path.join(output_dir, f'gradcam_{idx}.png'))\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "# Example usage function\n",
        "def analyze_model_attention(model, test_dataset, tumor_types, num_samples=5):\n",
        "    \"\"\"\n",
        "    Analyze model attention on random test samples.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LeViT model\n",
        "        test_dataset: Test dataset\n",
        "        tumor_types: List of tumor type labels\n",
        "        num_samples: Number of random samples to analyze\n",
        "    \"\"\"\n",
        "    # Get random indices\n",
        "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "\n",
        "    print(f\"\\nAnalyzing model attention on {num_samples} random test samples...\")\n",
        "\n",
        "    for idx in indices:\n",
        "        # Get image path and true label\n",
        "        img_path = test_dataset.image_paths[idx]\n",
        "        true_label = test_dataset.labels[idx]\n",
        "\n",
        "        print(f\"\\nAnalyzing image {idx}\")\n",
        "        print(f\"True label: {tumor_types[true_label]}\")\n",
        "\n",
        "        # Generate Grad-CAM visualization\n",
        "        heatmap, pred_class, probs = visualize_gradcam(\n",
        "            model,\n",
        "            img_path,\n",
        "            test_dataset.transform,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        print(f\"Predicted label: {tumor_types[pred_class]}\")\n",
        "        print(\"\\nClass probabilities:\")\n",
        "        for tumor_type, prob in zip(tumor_types, probs):\n",
        "            print(f\"{tumor_type}: {prob*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JsE0nHhO2gZT",
      "metadata": {
        "id": "JsE0nHhO2gZT"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.target_layer = model.levit.stages[-1].blocks[-1].attn\n",
        "        self.gradients = None\n",
        "        self.activation = None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activation = output.detach()\n",
        "            return output\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate_cam(self, input_tensor, target_class=None):\n",
        "        b, c, h, w = input_tensor.shape\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.set_grad_enabled(True):\n",
        "            model_output = self.model(input_tensor)\n",
        "\n",
        "            if target_class is None:\n",
        "                target_class = model_output.argmax(dim=1).item()\n",
        "\n",
        "            # Zero grads\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            # Target for backprop\n",
        "            one_hot = torch.zeros_like(model_output)\n",
        "            one_hot[0][target_class] = 1\n",
        "\n",
        "            # Backward pass with retain_graph\n",
        "            model_output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        # Generate attention map\n",
        "        if self.gradients is None or self.activation is None:\n",
        "            return np.zeros((h, w)), model_output\n",
        "\n",
        "        # Get the attention map\n",
        "        attention_map = self.activation.detach()  # Shape: [B, N, D]\n",
        "\n",
        "        # Reshape attention map to 2D\n",
        "        attention_size = int(np.sqrt(attention_map.size(1)))\n",
        "        attention_map = attention_map.mean(dim=2)  # Average over features\n",
        "        attention_map = attention_map.reshape(1, 1, attention_size, attention_size)\n",
        "\n",
        "        # Interpolate to input size\n",
        "        with torch.no_grad():\n",
        "            attention_map = F.interpolate(attention_map,\n",
        "                                        size=(h, w),\n",
        "                                        mode='bilinear',\n",
        "                                        align_corners=False)\n",
        "\n",
        "        # Normalize\n",
        "        attention_map = attention_map - attention_map.min()\n",
        "        attention_map = attention_map / (attention_map.max() + 1e-8)\n",
        "\n",
        "        # Detach and convert to numpy\n",
        "        attention_map = attention_map.squeeze().detach().cpu().numpy()\n",
        "\n",
        "        return attention_map, model_output.detach()\n",
        "\n",
        "def visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3):\n",
        "    \"\"\"\n",
        "    Visualize attention maps for specific samples from each class\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    grad_cam = GradCAM(model)\n",
        "\n",
        "    # Organize samples by class\n",
        "    samples_by_class = {}\n",
        "    for idx in range(len(test_dataset)):\n",
        "        _, label = test_dataset[idx]\n",
        "        if label not in samples_by_class:\n",
        "            samples_by_class[label] = []\n",
        "        samples_by_class[label].append(idx)\n",
        "\n",
        "    # Process samples from each class\n",
        "    for class_idx in range(len(tumor_types)):\n",
        "        print(f\"\\nProcessing class: {tumor_types[class_idx]}\")\n",
        "\n",
        "        # Get random samples for this class\n",
        "        class_samples = random.sample(samples_by_class[class_idx], min(samples_per_class, len(samples_by_class[class_idx])))\n",
        "\n",
        "        for sample_idx in class_samples:\n",
        "            try:\n",
        "                # Get image and create input tensor\n",
        "                image, label = test_dataset[sample_idx]\n",
        "                input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "                # Generate attention map\n",
        "                with torch.no_grad():\n",
        "                    attention_map, output = grad_cam.generate_cam(input_tensor)\n",
        "                    probs = F.softmax(output, dim=1)\n",
        "                    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "                # Get original image\n",
        "                original_image = cv2.imread(test_dataset.image_paths[sample_idx])\n",
        "                original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create heatmap\n",
        "                heatmap = np.uint8(255 * attention_map)\n",
        "                heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
        "                heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "                # Superimpose heatmap on original image\n",
        "                superimposed = cv2.addWeighted(original_image, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "                # Create figure with subplots\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "                # Plot original image\n",
        "                axes[0].imshow(original_image)\n",
        "                axes[0].set_title('Original Image')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Plot heatmap\n",
        "                axes[1].imshow(cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB))\n",
        "                axes[1].set_title('Attention Map')\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                # Plot superimposed image\n",
        "                axes[2].imshow(superimposed)\n",
        "                axes[2].set_title('Superimposed')\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add prediction information\n",
        "                true_class = tumor_types[label]\n",
        "                pred_class = tumor_types[predicted_class]\n",
        "                pred_prob = probs[0][predicted_class].item() * 100\n",
        "\n",
        "                plt.suptitle(f'True: {true_class} | Predicted: {pred_class} ({pred_prob:.2f}%)\\n'\n",
        "                            f'Image: {test_dataset.image_paths[sample_idx].split(\"/\")[-1]}',\n",
        "                            fontsize=12)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image at index {sample_idx}: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vYPHUk7V5uuf",
      "metadata": {
        "id": "vYPHUk7V5uuf"
      },
      "outputs": [],
      "source": [
        "def create_smooth_heatmap(attention_map, original_image, colormap=cv2.COLORMAP_INFERNO, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Create a smooth, visually appealing heatmap overlay\n",
        "\n",
        "    Args:\n",
        "        attention_map: The attention map from Grad-CAM\n",
        "        original_image: Original input image\n",
        "        colormap: OpenCV colormap to use (default: COLORMAP_INFERNO)\n",
        "        alpha: Transparency of the heatmap overlay (default: 0.4)\n",
        "    \"\"\"\n",
        "    # Apply Gaussian blur to smooth the attention map\n",
        "    attention_map = cv2.GaussianBlur(attention_map, (3, 3), 0)\n",
        "\n",
        "    # Resize attention map to match original image size\n",
        "    heatmap = cv2.resize(attention_map, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Normalize to 0-255 range\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
        "\n",
        "    # Convert to RGB\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create superimposed image with smoother blending\n",
        "    superimposed = cv2.addWeighted(original_image, 1 - alpha, heatmap, alpha, 0)\n",
        "\n",
        "    # Enhance contrast of the superimposed image\n",
        "    superimposed = cv2.convertScaleAbs(superimposed, alpha=1.2, beta=10)\n",
        "\n",
        "    return heatmap, superimposed\n",
        "\n",
        "def visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3):\n",
        "    \"\"\"\n",
        "    Visualize attention maps for specific samples from each class\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    grad_cam = GradCAM(model)\n",
        "\n",
        "    # Process samples from each class\n",
        "    for class_idx in range(len(tumor_types)):\n",
        "        print(f\"\\nProcessing class: {tumor_types[class_idx]}\")\n",
        "\n",
        "        # Get samples for this class\n",
        "        class_samples = [idx for idx, (_, label) in enumerate(test_dataset) if label == class_idx]\n",
        "        selected_samples = random.sample(class_samples, min(samples_per_class, len(class_samples)))\n",
        "\n",
        "        for sample_idx in selected_samples:\n",
        "            try:\n",
        "                # Get image and create input tensor\n",
        "                image, label = test_dataset[sample_idx]\n",
        "                input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "                # Generate attention map\n",
        "                with torch.no_grad():\n",
        "                    attention_map, output = grad_cam.generate_cam(input_tensor)\n",
        "                    probs = F.softmax(output, dim=1)\n",
        "                    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "                # Get original image\n",
        "                original_image = cv2.imread(test_dataset.image_paths[sample_idx])\n",
        "                original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create smooth heatmap visualization\n",
        "                heatmap, superimposed = create_smooth_heatmap(\n",
        "                    attention_map,\n",
        "                    original_image,\n",
        "                    colormap=cv2.COLORMAP_MAGMA,  # Using MAGMA colormap for better visualization\n",
        "                    alpha=0.5\n",
        "                )\n",
        "\n",
        "                # Create figure with subplots\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "                # Plot original image\n",
        "                axes[0].imshow(original_image)\n",
        "                axes[0].set_title('Original Image', fontsize=12)\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Plot heatmap\n",
        "                axes[1].imshow(heatmap)\n",
        "                axes[1].set_title('Attention Map', fontsize=12)\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                # Plot superimposed image\n",
        "                axes[2].imshow(superimposed)\n",
        "                axes[2].set_title('Superimposed', fontsize=12)\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add prediction information\n",
        "                true_class = tumor_types[label]\n",
        "                pred_class = tumor_types[predicted_class]\n",
        "                pred_prob = probs[0][predicted_class].item() * 100\n",
        "\n",
        "                plt.suptitle(\n",
        "                    f'True: {true_class} | Predicted: {pred_class} ({pred_prob:.2f}%)\\n'\n",
        "                    f'Image: {test_dataset.image_paths[sample_idx].split(\"/\")[-1]}',\n",
        "                    fontsize=13,\n",
        "                    y=1.02\n",
        "                )\n",
        "\n",
        "                # Adjust layout\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Add a white background\n",
        "                fig.patch.set_facecolor('white')\n",
        "\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image at index {sample_idx}: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iJLfn-Lj63Cq",
      "metadata": {
        "id": "iJLfn-Lj63Cq"
      },
      "outputs": [],
      "source": [
        "def create_refined_heatmap(attention_map, original_image):\n",
        "    \"\"\"\n",
        "    Create a refined heatmap visualization with better color mapping\n",
        "    \"\"\"\n",
        "    # Smooth the attention map\n",
        "    attention_map = cv2.GaussianBlur(attention_map, (3, 3), 0)\n",
        "\n",
        "    # Resize to match original image\n",
        "    heatmap = cv2.resize(attention_map, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Create colormap (red -> yellow -> green -> blue)\n",
        "    colors = []\n",
        "    for i in range(256):\n",
        "        if i < 64:  # blue to cyan\n",
        "            colors.append([0, i*4, 255])\n",
        "        elif i < 128:  # cyan to green\n",
        "            j = i - 64\n",
        "            colors.append([0, 255, 255 - j*4])\n",
        "        elif i < 192:  # green to yellow\n",
        "            j = i - 128\n",
        "            colors.append([j*4, 255, 0])\n",
        "        else:  # yellow to red\n",
        "            j = i - 192\n",
        "            colors.append([255, 255 - j*4, 0])\n",
        "\n",
        "    colors = np.array(colors, dtype=np.uint8)\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap_normalized = np.uint8(255 * heatmap)\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
        "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create superimposed image with better blending\n",
        "    superimposed = np.float32(original_image.copy())\n",
        "\n",
        "    # Enhanced alpha blending\n",
        "    alpha = 0.6\n",
        "    mask = heatmap > 0.2  # Only show significant activations\n",
        "    superimposed[mask] = superimposed[mask] * (1 - alpha) + heatmap_colored[mask] * alpha\n",
        "\n",
        "    return heatmap_normalized, np.uint8(superimposed)\n",
        "\n",
        "def visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    grad_cam = GradCAM(model)\n",
        "\n",
        "    plt.style.use('default')\n",
        "\n",
        "    for class_idx in range(len(tumor_types)):\n",
        "        print(f\"\\nProcessing class: {tumor_types[class_idx]}\")\n",
        "\n",
        "        class_samples = [idx for idx, (_, label) in enumerate(test_dataset) if label == class_idx]\n",
        "        selected_samples = random.sample(class_samples, min(samples_per_class, len(class_samples)))\n",
        "\n",
        "        for sample_idx in selected_samples:\n",
        "            try:\n",
        "                # Get image and predict\n",
        "                image, label = test_dataset[sample_idx]\n",
        "                input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "                # Generate attention map\n",
        "                with torch.no_grad():\n",
        "                    attention_map, output = grad_cam.generate_cam(input_tensor)\n",
        "                    probs = F.softmax(output, dim=1)\n",
        "                    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "                # Get original image\n",
        "                original_image = cv2.imread(test_dataset.image_paths[sample_idx])\n",
        "                original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create visualization\n",
        "                heatmap, superimposed = create_refined_heatmap(attention_map, original_image)\n",
        "\n",
        "                # Create figure\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "                fig.patch.set_facecolor('white')\n",
        "                plt.subplots_adjust(wspace=0.3)\n",
        "\n",
        "                # Plot original image\n",
        "                axes[0].imshow(original_image, cmap='gray')\n",
        "                axes[0].set_title('Original Image')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Plot attention map\n",
        "                im = axes[1].imshow(heatmap, cmap='jet')\n",
        "                axes[1].set_title('Attention Map')\n",
        "                axes[1].axis('off')\n",
        "                cbar = plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "                cbar.set_ticks([0, 64, 128, 192, 255])\n",
        "                cbar.set_ticklabels(['0.0', '0.2', '0.4', '0.6', '0.8'])\n",
        "\n",
        "                # Plot superimposed\n",
        "                axes[2].imshow(superimposed)\n",
        "                axes[2].set_title('CNN GradCAM')\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add prediction information\n",
        "                true_class = tumor_types[label]\n",
        "                pred_class = tumor_types[predicted_class]\n",
        "                pred_prob = probs[0][predicted_class].item() * 100\n",
        "\n",
        "                plt.suptitle(\n",
        "                    f'True: {true_class}\\nPred: {pred_class} ({pred_prob:.2f}%)',\n",
        "                    y=1.05,\n",
        "                    fontsize=12\n",
        "                )\n",
        "\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image at index {sample_idx}: {str(e)}\")\n",
        "                traceback.print_exc()\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bn0KU-yi0lUF",
      "metadata": {
        "id": "Bn0KU-yi0lUF"
      },
      "outputs": [],
      "source": [
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 2. Create dataset\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "\n",
        "    # 3. Load model\n",
        "    model = initialize_levit_model(num_classes=4)\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_256_model_final.pth')['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "\n",
        "    # 4. Visualize attention maps\n",
        "    visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gKu7OREGTT56",
      "metadata": {
        "id": "gKu7OREGTT56"
      },
      "source": [
        "# Model 4: Levit-384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt4KntDS0lWz",
      "metadata": {
        "id": "Xt4KntDS0lWz"
      },
      "outputs": [],
      "source": [
        "class LeViT384Model(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(LeViT384Model, self).__init__()\n",
        "\n",
        "        # Load pretrained LeViT-384 model\n",
        "        self.levit = timm.create_model('levit_384', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Get the number of features from LeViT\n",
        "        levit_num_features = self.levit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(levit_num_features, 1024),  # Increased intermediate layer size for LeViT-384\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through LeViT\n",
        "        features = self.levit(x)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_levit_model(num_classes, pretrained=True):\n",
        "    return LeViT384Model(num_classes, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YE2GXpUrcVtm",
      "metadata": {
        "collapsed": true,
        "id": "YE2GXpUrcVtm"
      },
      "outputs": [],
      "source": [
        "def train_levit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_levit_model(num_classes).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save the first model state or if we have a new best validation loss\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Final training on entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_levit_model(num_classes).to(device)\n",
        "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None  # Initialize best_model\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        # Validation phase\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save the first model state or if we have a new best validation loss\n",
        "        if best_model is None or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = final_model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Load the best model state\n",
        "    final_model.load_state_dict(best_model)\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_levit_384_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89n6xoKscVwG",
      "metadata": {
        "id": "89n6xoKscVwG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable torch compile to avoid dynamo issues\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        # Print dataset sizes\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the LeViT-384 model\n",
        "        results, final_model = train_levit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_384_training_history.csv')  # Changed filename\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        # Reduced batch size for LeViT-384 due to higher memory requirements\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)  # Reduced batch size\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_384_model_metrics.csv')  # Changed filename\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_384_model_final.pth')  # Changed filename\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_384_test_predictions.csv')  # Changed filename\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()  # This will print the full error traceback\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nLeViT-384 training, evaluation, and metrics logging complete!\")  # Updated message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fz4WngE6USIe",
      "metadata": {
        "id": "Fz4WngE6USIe"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.nn.functional import interpolate\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.target_layer = model.levit.stages[-1].blocks[-1].mlp\n",
        "        self.gradients = None\n",
        "        self.activation = None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activation = output[0] if isinstance(output, tuple) else output\n",
        "            self.activation = self.activation.detach()\n",
        "            return output\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate_cam(self, input_tensor, target_class=None):\n",
        "        b, c, h, w = input_tensor.shape\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.set_grad_enabled(True):\n",
        "            model_output = self.model(input_tensor)\n",
        "\n",
        "            if target_class is None:\n",
        "                target_class = model_output.argmax(dim=1).item()\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            one_hot = torch.zeros_like(model_output)\n",
        "            one_hot[0][target_class] = 1\n",
        "            model_output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        if self.gradients is None or self.activation is None:\n",
        "            return np.zeros((h, w)), model_output\n",
        "\n",
        "        # Print shapes for debugging\n",
        "        print(f\"Activation shape: {self.activation.shape}\")\n",
        "        print(f\"Gradients shape: {self.gradients.shape}\")\n",
        "\n",
        "        # Handle transformer-style output (B, N, C)\n",
        "        if len(self.activation.shape) == 3:\n",
        "            B, N, C = self.activation.shape\n",
        "\n",
        "            # Calculate spatial dimensions\n",
        "            H = W = int(np.sqrt(N))\n",
        "\n",
        "            # Reshape activation and gradients to (B, C, H, W)\n",
        "            activation = self.activation.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "            gradients = self.gradients.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "\n",
        "            # Calculate channel-wise weights\n",
        "            weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
        "\n",
        "            # Weight the activation maps\n",
        "            cam = (weights * activation).sum(dim=1, keepdim=True)\n",
        "\n",
        "        else:\n",
        "            # For regular convolutional output (B, C, H, W)\n",
        "            weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
        "            cam = (weights * self.activation).sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Apply ReLU and normalize\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Resize to input size\n",
        "        cam = F.interpolate(\n",
        "            cam,\n",
        "            size=(h, w),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Normalize\n",
        "        with torch.no_grad():\n",
        "            cam_min = cam.min()\n",
        "            cam_max = cam.max()\n",
        "            if cam_max - cam_min != 0:\n",
        "                cam = (cam - cam_min) / (cam_max - cam_min)\n",
        "\n",
        "        return cam[0, 0].cpu().numpy(), model_output.detach()\n",
        "\n",
        "def create_refined_heatmap(attention_map, original_image):\n",
        "    \"\"\"\n",
        "    Create a refined heatmap visualization with better color mapping\n",
        "    \"\"\"\n",
        "    # Ensure attention_map is a numpy array\n",
        "    if torch.is_tensor(attention_map):\n",
        "        attention_map = attention_map.cpu().numpy()\n",
        "\n",
        "    # Ensure attention_map is 2D\n",
        "    if len(attention_map.shape) > 2:\n",
        "        attention_map = attention_map.squeeze()\n",
        "\n",
        "    # Normalize if needed\n",
        "    if attention_map.max() > 1:\n",
        "        attention_map = attention_map / attention_map.max()\n",
        "\n",
        "    # Smooth the attention map\n",
        "    attention_map = cv2.GaussianBlur(attention_map.astype(np.float32), (3, 3), 0)\n",
        "\n",
        "    # Resize to match original image\n",
        "    heatmap = cv2.resize(attention_map, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap_normalized = np.uint8(255 * heatmap)\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
        "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create superimposed image with better blending\n",
        "    superimposed = np.float32(original_image.copy())\n",
        "    alpha = 0.6\n",
        "    mask = heatmap > 0.2\n",
        "    superimposed[mask] = superimposed[mask] * (1 - alpha) + heatmap_colored[mask] * alpha\n",
        "\n",
        "    return heatmap_normalized, np.uint8(superimposed)\n",
        "\n",
        "def visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    grad_cam = GradCAM(model)\n",
        "\n",
        "    plt.style.use('default')\n",
        "\n",
        "    for class_idx in range(len(tumor_types)):\n",
        "        print(f\"\\nProcessing class: {tumor_types[class_idx]}\")\n",
        "\n",
        "        class_samples = [idx for idx, (_, label) in enumerate(test_dataset) if label == class_idx]\n",
        "        selected_samples = random.sample(class_samples, min(samples_per_class, len(class_samples)))\n",
        "\n",
        "        for sample_idx in selected_samples:\n",
        "            try:\n",
        "                # Get image and predict\n",
        "                image, label = test_dataset[sample_idx]\n",
        "                input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "                # Generate attention map\n",
        "                attention_map, output = grad_cam.generate_cam(input_tensor)\n",
        "                probs = F.softmax(output, dim=1)\n",
        "                predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "                # Get original image\n",
        "                original_image = cv2.imread(test_dataset.image_paths[sample_idx])\n",
        "                original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create visualization\n",
        "                heatmap, superimposed = create_refined_heatmap(attention_map, original_image)\n",
        "\n",
        "                # Create figure\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "                fig.patch.set_facecolor('white')\n",
        "                plt.subplots_adjust(wspace=0.3)\n",
        "\n",
        "                # Plot visualizations\n",
        "                axes[0].imshow(original_image)\n",
        "                axes[0].set_title('Original Image')\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                im = axes[1].imshow(heatmap, cmap='jet')\n",
        "                axes[1].set_title('Attention Map')\n",
        "                axes[1].axis('off')\n",
        "                cbar = plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "                cbar.set_ticks([0, 64, 128, 192, 255])\n",
        "                cbar.set_ticklabels(['0.0', '0.2', '0.4', '0.6', '0.8'])\n",
        "\n",
        "                axes[2].imshow(superimposed)\n",
        "                axes[2].set_title('Grad-CAM Overlay')\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add prediction information\n",
        "                true_class = tumor_types[label]\n",
        "                pred_class = tumor_types[predicted_class]\n",
        "                pred_prob = probs[0][predicted_class].item() * 100\n",
        "\n",
        "                plt.suptitle(\n",
        "                    f'True: {true_class}\\nPredicted: {pred_class} ({pred_prob:.2f}%)',\n",
        "                    y=1.05,\n",
        "                    fontsize=12\n",
        "                )\n",
        "\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image at index {sample_idx}: {str(e)}\")\n",
        "                print(f\"Activation shape: {grad_cam.activation.shape if grad_cam.activation is not None else 'None'}\")\n",
        "                print(f\"Gradients shape: {grad_cam.gradients.shape if grad_cam.gradients is not None else 'None'}\")\n",
        "                traceback.print_exc()\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ju2eyAC3USOk",
      "metadata": {
        "id": "ju2eyAC3USOk"
      },
      "outputs": [],
      "source": [
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 2. Create dataset\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "\n",
        "    # 3. Load model\n",
        "    model = initialize_levit_model(num_classes=4)\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_384_model_final.pth')['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # 4. Visualize attention maps\n",
        "    visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6IG7TOH-KlTj",
      "metadata": {
        "id": "6IG7TOH-KlTj"
      },
      "source": [
        "# Model 5: CoAtNet-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9ixpJCxUSYK",
      "metadata": {
        "id": "i9ixpJCxUSYK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "import timm\n",
        "import time\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "import traceback\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class CoAtNet0Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CoAtNet0Model, self).__init__()\n",
        "\n",
        "        # Load pre-trained CoAtNet-0-RW model\n",
        "        self.coatnet = timm.create_model('coatnet_0_rw_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(self.coatnet, 'set_grad_checkpointing'):\n",
        "            self.coatnet.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from CoAtNet\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "\n",
        "        # Add final classification layers with reduced size for memory efficiency\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(coatnet_num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.coatnet(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_coatnet_model(num_classes):\n",
        "    return CoAtNet0Model(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CXJDXD-qQWnu",
      "metadata": {
        "id": "CXJDXD-qQWnu"
      },
      "outputs": [],
      "source": [
        "def train_coatnet_model(train_dataset, test_dataset, num_classes, num_epochs=50, patience=5, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Training with pre-trained weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Slightly larger batch size since we're fine-tuning\n",
        "    batch_size = 24\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_coatnet_model(num_classes).to(device)\n",
        "\n",
        "        # Use different learning rates for pre-trained layers and new layers\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': model.coatnet.parameters(), 'lr': 1e-5},  # Lower learning rate for pre-trained layers\n",
        "            {'params': model.fc.parameters(), 'lr': 1e-4}       # Higher learning rate for new layers\n",
        "        ])\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache periodically\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_pretrained_coatnet_0_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ysXYjXCUSdL",
      "metadata": {
        "id": "8ysXYjXCUSdL"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the CoAtNet-0 model\n",
        "        results, final_model = train_coatnet_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'coatnet_0_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'coatnet_0_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'coatnet_0_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'coatnet_0_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCoAtNet-0 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kOIU17eceS9u",
      "metadata": {
        "id": "kOIU17eceS9u"
      },
      "source": [
        "Since the comp complexity a.k.a time to train is too long I will make the educated guess to not experiment with more complex co-at-net models based on the fact that research shows that accuracy doesn't improve all that much."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J4whcLjVjszh",
      "metadata": {
        "id": "J4whcLjVjszh"
      },
      "source": [
        "# CoAtNet1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OSQGmAvHzMeC",
      "metadata": {
        "id": "OSQGmAvHzMeC"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class CoAtNet1Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CoAtNet1Model, self).__init__()\n",
        "\n",
        "        # Load pre-trained CoAtNet-1-RW model\n",
        "        self.coatnet = timm.create_model('coatnet_1_rw_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(self.coatnet, 'set_grad_checkpointing'):\n",
        "            self.coatnet.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from CoAtNet-1\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "\n",
        "        # Add final classification layers with reduced size for memory efficiency\n",
        "        # Note: CoAtNet-1 has more features than CoAtNet-0, so we add an extra reduction layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(coatnet_num_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.coatnet(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_coatnet_model(num_classes):\n",
        "    return CoAtNet1Model(num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MWzKT_Q1zMze",
      "metadata": {
        "id": "MWzKT_Q1zMze"
      },
      "outputs": [],
      "source": [
        "def train_coatnet_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Training with pre-trained CoAtNet-1 weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Reduced batch size for CoAtNet-1 since it's larger\n",
        "    batch_size = 16\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_coatnet_model(num_classes).to(device)\n",
        "\n",
        "        # Adjusted learning rates for CoAtNet-1\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': model.coatnet.parameters(), 'lr': 5e-6},  # Lower learning rate for pre-trained layers\n",
        "            {'params': model.fc.parameters(), 'lr': 5e-5}       # Lower learning rate for new layers\n",
        "        ])\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache more frequently for CoAtNet-1\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results\n",
        "    }, 'final_pretrained_coatnet_1_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oDxicI-zNQS",
      "metadata": {
        "id": "5oDxicI-zNQS"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    print(\"Initializing CoAtNet-1 training pipeline...\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    print(f\"Tumor types: {tumor_types}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for CoAtNet-1\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomAdjustSharpness(0.2),  # Added for CoAtNet-1\n",
        "            transforms.RandomAutocontrast(),        # Added for CoAtNet-1\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"\\nDataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the CoAtNet-1 model\n",
        "        print(\"\\nInitiating CoAtNet-1 training...\")\n",
        "        results, final_model = train_coatnet_model(\n",
        "            train_dataset,\n",
        "            test_dataset,\n",
        "            num_classes,\n",
        "            num_epochs=50,    # Adjust these parameters as needed\n",
        "            patience=5,\n",
        "            k_folds=5\n",
        "        )\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'coatnet_1_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"\\nTraining history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=16,  # Reduced batch size for CoAtNet-1\n",
        "            shuffle=False,\n",
        "            num_workers=2,   # Reduced workers for memory efficiency\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'coatnet_1_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save confusion matrix\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=tumor_types,\n",
        "                   yticklabels=tumor_types)\n",
        "        plt.title('CoAtNet-1 Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(base_path, 'coatnet_1_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'coatnet_1_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'coatnet_1_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "        # Print final summary\n",
        "        print(\"\\nFinal Summary:\")\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Average F1 Score: {np.mean(f1):.4f}\")\n",
        "        print(f\"Average AUC-ROC: {np.mean(auc_roc):.4f}\")\n",
        "        print(f\"Total Training Time: {results[-1]['Training Time (s)']:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCoAtNet-1 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MQ38p10LzNaY",
      "metadata": {
        "id": "MQ38p10LzNaY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bb6a89AsiwZK",
      "metadata": {
        "id": "bb6a89AsiwZK"
      },
      "source": [
        "# Model 9: XCiT_small_12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m9gloWBIl9Qu",
      "metadata": {
        "id": "m9gloWBIl9Qu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "class XCiTModel(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(XCiTModel, self).__init__()\n",
        "\n",
        "        # Load XCiT small_12 model with pretrained parameter\n",
        "        self.xcit = timm.create_model('xcit_small_12_p8_224', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Enable gradient checkpointing for memory efficiency\n",
        "        self.xcit.set_grad_checkpointing(enable=True)\n",
        "\n",
        "        # Get the number of features from XCiT\n",
        "        xcit_num_features = self.xcit.num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(xcit_num_features, 512),  # Reduced from 1024 to 512 for memory efficiency\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.xcit(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "def initialize_xcit_model(num_classes, pretrained=True):\n",
        "    return XCiTModel(num_classes, pretrained=pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64Vtz1Ksl9mD",
      "metadata": {
        "id": "64Vtz1Ksl9mD"
      },
      "outputs": [],
      "source": [
        "def train_xcit_model(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5, pretrained=True):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Training with {'pretrained' if pretrained else 'randomly initialized'} weights\")\n",
        "\n",
        "    # Set memory-efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use smaller batch size to prevent OOM errors\n",
        "    batch_size = 16  # Reduced from 32 but can be larger than XCiT-24 since model is smaller\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = initialize_xcit_model(num_classes, pretrained=pretrained).to(device)\n",
        "\n",
        "        # Use different learning rates for pretrained vs non-pretrained\n",
        "        initial_lr = 0.0001 if pretrained else 0.001\n",
        "        optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Use mixed precision training\n",
        "                    with autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Scale gradients and optimize\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    train_total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Clear cache periodically\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        with autocast():\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        val_total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses and accuracies\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if best_model is None or val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "            # Clear cache after each epoch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'epoch': len(results),\n",
        "        'model_state_dict': best_model,\n",
        "        'results': results,\n",
        "        'pretrained': pretrained\n",
        "    }, 'final_xcit_small_12_model.pth')\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GISmuvECl-D1",
      "metadata": {
        "id": "GISmuvECl-D1"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the datasets\n",
        "        train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "        test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "        print(\"Dataset sizes:\")\n",
        "        print(f\"Training: {len(train_dataset)}\")\n",
        "        print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "        # Train the XCiT model\n",
        "        results, final_model = train_xcit_model(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'xcit_small_12_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display metrics\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'xcit_small_12_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save final model\n",
        "        model_path = os.path.join(base_path, 'xcit_small_12_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'xcit_small_12_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nXCiT Small 12 training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PiN7-0dTWHHK",
      "metadata": {
        "id": "PiN7-0dTWHHK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b0QCfc0h21cC",
      "metadata": {
        "id": "b0QCfc0h21cC"
      },
      "source": [
        "#LevitHybrid From timm Library:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XGFcpAYiGrqZ",
      "metadata": {
        "id": "XGFcpAYiGrqZ"
      },
      "source": [
        "Importing Contigencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6GCxklFMGr6P",
      "metadata": {
        "id": "6GCxklFMGr6P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import timm\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076A2Wsw20Z8",
      "metadata": {
        "id": "076A2Wsw20Z8"
      },
      "outputs": [],
      "source": [
        "class LeViTEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, levit_model='levit_384', pretrained=True):\n",
        "        super(LeViTEfficientNetHybrid, self).__init__()\n",
        "\n",
        "        # Load pretrained LeViT model\n",
        "        self.levit = timm.create_model(levit_model, pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Load pretrained EfficientNet-B0\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Remove the final classification layers from EfficientNet\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        # Get the number of features from both models\n",
        "        levit_num_features = self.levit.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Combine features\n",
        "        self.combined_features = levit_num_features + efficientnet_num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through LeViT\n",
        "        levit_output = self.levit(x)\n",
        "\n",
        "        # Process input through EfficientNet\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat((levit_output, efficientnet_output), dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_levit_efficientnet_hybrid(num_classes, levit_model='levit_384', pretrained=True):\n",
        "    return LeViTEfficientNetHybrid(num_classes, levit_model, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0EPYu6wU2-ea",
      "metadata": {
        "id": "0EPYu6wU2-ea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def train_levit_efficientnet_hybrid(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5, levit_model='levit_384'):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_levit_efficientnet_hybrid(num_classes, levit_model).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            val_acc = correct / total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model = model.state_dict()\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Train on the entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_levit_efficientnet_hybrid(num_classes, levit_model).to(device)\n",
        "    final_optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    final_scheduler = ReduceLROnPlateau(final_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                final_optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                final_optimizer.step()\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        final_scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_model = final_model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save(best_model, 'final_custom_levit_efficientnet_hybrid_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TkvnGQ213uGA",
      "metadata": {
        "id": "TkvnGQ213uGA"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, tumor_types):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "    print(f'Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tumor_types, yticklabels=tumor_types)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc_roc, avg_loss, all_preds, all_labels\n",
        "\n",
        "# Function to create metrics DataFrame\n",
        "def create_metrics_dataframe(model, test_acc, precision, recall, f1, auc_roc, train_time, test_loss):\n",
        "    metrics = {\n",
        "        'Metric': ['Overall Accuracy', 'F1 Score', 'Cross Entropy Loss', 'Training Time (s)', 'Number of Parameters', 'Model Size (MB)'],\n",
        "        'Value': [\n",
        "            test_acc,\n",
        "            f1,\n",
        "            test_loss,\n",
        "            train_time,\n",
        "            sum(p.numel() for p in model.parameters()),\n",
        "            sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(metrics)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MAGrQjjN3fGv",
      "metadata": {
        "collapsed": true,
        "id": "MAGrQjjN3fGv"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the datasets\n",
        "    train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "    # Print dataset sizes\n",
        "    print(\"Dataset sizes:\")\n",
        "    print(f\"Training: {len(train_dataset)}\")\n",
        "    print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "    try:\n",
        "        # Train the Custom LeViT-ResNet model\n",
        "        results, final_model = train_levit_efficientnet_hybrid(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_efficientnet_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_efficientnet_hybrid_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_efficientnet_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCustom LeViT-EffcientNet training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PXRdtsu6jjj7",
      "metadata": {
        "id": "PXRdtsu6jjj7"
      },
      "source": [
        "# LeVit compatible with GradCam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kuDoU177jpj0",
      "metadata": {
        "id": "kuDoU177jpj0"
      },
      "outputs": [],
      "source": [
        "class LeViTEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, levit_model='levit_384', pretrained=True):\n",
        "        super(LeViTEfficientNetHybrid, self).__init__()\n",
        "\n",
        "        self.levit = timm.create_model(levit_model, pretrained=pretrained, num_classes=0)\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        levit_num_features = self.levit.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "        self.combined_features = levit_num_features + efficientnet_num_features\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        self.attention_maps = []\n",
        "        self.attention_gradients = []\n",
        "\n",
        "        def save_attention(name):\n",
        "            def hook(module, input, output):\n",
        "                self.attention_maps.append(output)\n",
        "                # Register gradient hook for this output\n",
        "                output.register_hook(lambda grad: self.attention_gradients.append(grad))\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.levit.named_modules():\n",
        "            if 'attn' in name:\n",
        "                module.register_forward_hook(save_attention(name))\n",
        "\n",
        "    def get_attention_maps(self):\n",
        "        return self.attention_maps\n",
        "\n",
        "    def get_attention_gradients(self):\n",
        "        return self.attention_gradients\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.attention_maps = []\n",
        "        self.attention_gradients = []\n",
        "\n",
        "        levit_output = self.levit(x)\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "        combined = torch.cat((levit_output, efficientnet_output), dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "def get_class_specific_attention(model, image, target_class, img_size=224):\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass with gradient\n",
        "    image = image.clone().detach().requires_grad_(True)\n",
        "    output = model(image)\n",
        "\n",
        "    # Zero all gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Get the score for target class\n",
        "    score = output[0, target_class]\n",
        "\n",
        "    # Backward pass to get gradients\n",
        "    score.backward()\n",
        "\n",
        "    # Get attention maps and gradients\n",
        "    attention_maps = model.get_attention_maps()\n",
        "    attention_gradients = model.get_attention_gradients()\n",
        "\n",
        "    # Get final stage attention (layer 89)\n",
        "    final_attn = attention_maps[89]\n",
        "    final_grad = attention_gradients[89] if len(attention_gradients) > 89 else torch.ones_like(final_attn)\n",
        "\n",
        "    # Weight attention by gradients\n",
        "    weighted_attn = (final_attn * final_grad).mean(dim=2)\n",
        "\n",
        "    # Reshape to square\n",
        "    attn_map = weighted_attn.reshape(4, 4)\n",
        "\n",
        "    # Normalize\n",
        "    attn_map = F.normalize(attn_map.view(-1), dim=0).view(4, 4)\n",
        "\n",
        "    # Upsample to original image size\n",
        "    attn_map = attn_map.unsqueeze(0).unsqueeze(0)\n",
        "    attn_map = F.interpolate(attn_map, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "    attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
        "\n",
        "    return attn_map\n",
        "\n",
        "def initialize_levit_efficientnet_hybrid(num_classes, levit_model='levit_384', pretrained=True):\n",
        "    return LeViTEfficientNetHybrid(num_classes, levit_model, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iv-ZEQqbf5Ww",
      "metadata": {
        "collapsed": true,
        "id": "Iv-ZEQqbf5Ww"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the datasets\n",
        "    train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "    # Print dataset sizes\n",
        "    print(\"Dataset sizes:\")\n",
        "    print(f\"Training: {len(train_dataset)}\")\n",
        "    print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "    try:\n",
        "        # Train the Custom LeViT-ResNet model\n",
        "        results, final_model = train_levit_efficientnet_hybrid(train_dataset, test_dataset, num_classes)\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'levit_efficientnet_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save the metrics DataFrame\n",
        "        metrics_csv_path = os.path.join(base_path, 'levit_efficientnet_hybrid_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'levit_efficientnet_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'levit_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCustom LeViT-EffcientNet training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2929IJl_jsKw",
      "metadata": {
        "id": "2929IJl_jsKw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def visualize_attention(model, image_path, tumor_types):\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Move to GPU if available\n",
        "    device = next(model.parameters()).device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Get prediction and attention maps\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        attention_maps = model.get_attention_maps()\n",
        "\n",
        "    # Get predicted class\n",
        "    pred = torch.argmax(output).item()\n",
        "    pred_class = tumor_types[pred]\n",
        "\n",
        "    # Process attention maps\n",
        "    # We'll average attention across heads and create heatmaps\n",
        "    attention_layers = []\n",
        "    for attn in attention_maps:\n",
        "        # Average across heads if necessary\n",
        "        if len(attn.shape) > 3:\n",
        "            attn = attn.mean(dim=1)\n",
        "        attention_layers.append(attn.cpu().numpy())\n",
        "\n",
        "    # Plotting\n",
        "    n_layers = len(attention_layers)\n",
        "    fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(15, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original Image\\nPredicted: ' + pred_class)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Attention maps\n",
        "    for idx, attn in enumerate(attention_layers):\n",
        "        # Reshape attention to square if necessary\n",
        "        attn_map = attn[0]  # First batch\n",
        "        h = int(np.sqrt(attn_map.shape[-1]))\n",
        "        attn_map = attn_map.reshape(h, h)\n",
        "\n",
        "        axes[idx+1].imshow(attn_map, cmap='viridis')\n",
        "        axes[idx+1].set_title(f'Attention Layer {idx+1}')\n",
        "        axes[idx+1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_class, attention_layers\n",
        "\n",
        "# Example usage:\n",
        "def analyze_sample_images(model, test_dataset, tumor_types, num_samples=5):\n",
        "    \"\"\"\n",
        "    Analyze multiple sample images from the test dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        true_class = tumor_types[label.item()]\n",
        "\n",
        "        # Get prediction and attention maps\n",
        "        with torch.no_grad():\n",
        "            output = model(image.cuda())\n",
        "            attention_maps = model.get_attention_maps()\n",
        "\n",
        "        pred = torch.argmax(output).item()\n",
        "        pred_class = tumor_types[pred]\n",
        "\n",
        "        # Process attention maps\n",
        "        attention_layers = []\n",
        "        for attn in attention_maps:\n",
        "            if len(attn.shape) > 3:\n",
        "                attn = attn.mean(dim=1)\n",
        "            attention_layers.append(attn.cpu().numpy())\n",
        "\n",
        "        # Plotting\n",
        "        n_layers = len(attention_layers)\n",
        "        fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(15, 8))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Original image\n",
        "        axes[0].imshow(image.squeeze().permute(1,2,0))\n",
        "        axes[0].set_title(f'Original Image\\nTrue: {true_class}\\nPred: {pred_class}')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Attention maps\n",
        "        for idx, attn in enumerate(attention_layers):\n",
        "            attn_map = attn[0]\n",
        "            h = int(np.sqrt(attn_map.shape[-1]))\n",
        "            attn_map = attn_map.reshape(h, h)\n",
        "\n",
        "            axes[idx+1].imshow(attn_map, cmap='viridis')\n",
        "            axes[idx+1].set_title(f'Attention Layer {idx+1}')\n",
        "            axes[idx+1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zHl_miK_dSKR",
      "metadata": {
        "id": "zHl_miK_dSKR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def analyze_sample_images_highres(model, test_dataset, tumor_types, num_samples=5, img_size=224):\n",
        "    \"\"\"\n",
        "    Analyze samples with high resolution attention maps\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        if tumor_types[label.item()] == 'notumor':\n",
        "            continue  # Skip no-tumor cases\n",
        "\n",
        "        true_class = tumor_types[label.item()]\n",
        "\n",
        "        # Get prediction and attention maps\n",
        "        with torch.no_grad():\n",
        "            output = model(image.cuda())\n",
        "            attention_maps = model.get_attention_maps()\n",
        "\n",
        "        pred = torch.argmax(output).item()\n",
        "        pred_class = tumor_types[pred]\n",
        "\n",
        "        # Select key layers from each stage\n",
        "        key_layers = [0, 39, 89]  # First layer of each stage\n",
        "\n",
        "        # Process attention maps\n",
        "        processed_maps = []\n",
        "        for layer_idx in key_layers:\n",
        "            attn = attention_maps[layer_idx]\n",
        "\n",
        "            # Remove batch dimension if present\n",
        "            if len(attn.shape) == 3:\n",
        "                attn = attn.squeeze(0)\n",
        "\n",
        "            # Get number of tokens\n",
        "            num_tokens = attn.shape[0]\n",
        "\n",
        "            if num_tokens == 196:\n",
        "                size = 14\n",
        "            elif num_tokens == 49:\n",
        "                size = 7\n",
        "            elif num_tokens == 16:\n",
        "                size = 4\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Average attention patterns\n",
        "            attn = attn.mean(dim=-1)  # Average over all attention heads\n",
        "            attn = attn.reshape(size, size)\n",
        "\n",
        "            # Upsample to original image size\n",
        "            attn = torch.tensor(attn).unsqueeze(0).unsqueeze(0)\n",
        "            attn = F.interpolate(attn, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "            attn = attn.squeeze().cpu().numpy()\n",
        "\n",
        "            processed_maps.append(attn)\n",
        "\n",
        "        # Plotting\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "        # Original image\n",
        "        img_display = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "        # Denormalize\n",
        "        img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        axes[0].imshow(img_display, cmap='gray')\n",
        "        axes[0].set_title(f'Original Image\\nTrue: {true_class}\\nPred: {pred_class}')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Attention maps\n",
        "        stage_names = ['Stage 1 Attention', 'Stage 2 Attention', 'Stage 3 Attention']\n",
        "        for idx, (attn, stage_name) in enumerate(zip(processed_maps, stage_names)):\n",
        "            axes[idx+1].imshow(img_display, cmap='gray', alpha=0.6)\n",
        "            im = axes[idx+1].imshow(attn, cmap='jet', alpha=0.4)\n",
        "            axes[idx+1].set_title(stage_name)\n",
        "            axes[idx+1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        if true_class != pred_class:\n",
        "            print(\"⚠️ Misclassified case!\")\n",
        "        print(f\"True class: {true_class}\")\n",
        "        print(f\"Predicted class: {pred_class}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Only show one tumor case\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "geI9DASX6mVN",
      "metadata": {
        "id": "geI9DASX6mVN"
      },
      "outputs": [],
      "source": [
        "def create_smooth_heatmap(attention_map, original_image, colormap=cv2.COLORMAP_INFERNO, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Create a smooth, visually appealing heatmap overlay\n",
        "\n",
        "    Args:\n",
        "        attention_map: The attention map from Grad-CAM\n",
        "        original_image: Original input image\n",
        "        colormap: OpenCV colormap to use (default: COLORMAP_INFERNO)\n",
        "        alpha: Transparency of the heatmap overlay (default: 0.4)\n",
        "    \"\"\"\n",
        "    # Apply Gaussian blur to smooth the attention map\n",
        "    attention_map = cv2.GaussianBlur(attention_map, (3, 3), 0)\n",
        "\n",
        "    # Resize attention map to match original image size\n",
        "    heatmap = cv2.resize(attention_map, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Normalize to 0-255 range\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
        "\n",
        "    # Convert to RGB\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create superimposed image with smoother blending\n",
        "    superimposed = cv2.addWeighted(original_image, 1 - alpha, heatmap, alpha, 0)\n",
        "\n",
        "    # Enhance contrast of the superimposed image\n",
        "    superimposed = cv2.convertScaleAbs(superimposed, alpha=1.2, beta=10)\n",
        "\n",
        "    return heatmap, superimposed\n",
        "\n",
        "def visualize_class_specific_attention(model, test_dataset, tumor_types, samples_per_class=3):\n",
        "    \"\"\"\n",
        "    Visualize attention maps for specific samples from each class\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    grad_cam = GradCAM(model)\n",
        "\n",
        "    # Process samples from each class\n",
        "    for class_idx in range(len(tumor_types)):\n",
        "        print(f\"\\nProcessing class: {tumor_types[class_idx]}\")\n",
        "\n",
        "        # Get samples for this class\n",
        "        class_samples = [idx for idx, (_, label) in enumerate(test_dataset) if label == class_idx]\n",
        "        selected_samples = random.sample(class_samples, min(samples_per_class, len(class_samples)))\n",
        "\n",
        "        for sample_idx in selected_samples:\n",
        "            try:\n",
        "                # Get image and create input tensor\n",
        "                image, label = test_dataset[sample_idx]\n",
        "                input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "                # Generate attention map\n",
        "                with torch.no_grad():\n",
        "                    attention_map, output = grad_cam.generate_cam(input_tensor)\n",
        "                    probs = F.softmax(output, dim=1)\n",
        "                    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "                # Get original image\n",
        "                original_image = cv2.imread(test_dataset.image_paths[sample_idx])\n",
        "                original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create smooth heatmap visualization\n",
        "                heatmap, superimposed = create_smooth_heatmap(\n",
        "                    attention_map,\n",
        "                    original_image,\n",
        "                    colormap=cv2.COLORMAP_MAGMA,  # Using MAGMA colormap for better visualization\n",
        "                    alpha=0.5\n",
        "                )\n",
        "\n",
        "                # Create figure with subplots\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "                # Plot original image\n",
        "                axes[0].imshow(original_image)\n",
        "                axes[0].set_title('Original Image', fontsize=12)\n",
        "                axes[0].axis('off')\n",
        "\n",
        "                # Plot heatmap\n",
        "                axes[1].imshow(heatmap)\n",
        "                axes[1].set_title('Attention Map', fontsize=12)\n",
        "                axes[1].axis('off')\n",
        "\n",
        "                # Plot superimposed image\n",
        "                axes[2].imshow(superimposed)\n",
        "                axes[2].set_title('Superimposed', fontsize=12)\n",
        "                axes[2].axis('off')\n",
        "\n",
        "                # Add prediction information\n",
        "                true_class = tumor_types[label]\n",
        "                pred_class = tumor_types[predicted_class]\n",
        "                pred_prob = probs[0][predicted_class].item() * 100\n",
        "\n",
        "                plt.suptitle(\n",
        "                    f'True: {true_class} | Predicted: {pred_class} ({pred_prob:.2f}%)\\n'\n",
        "                    f'Image: {test_dataset.image_paths[sample_idx].split(\"/\")[-1]}',\n",
        "                    fontsize=13,\n",
        "                    y=1.02\n",
        "                )\n",
        "\n",
        "                # Adjust layout\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Add a white background\n",
        "                fig.patch.set_facecolor('white')\n",
        "\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image at index {sample_idx}: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76CAjNJq1MHQ",
      "metadata": {
        "id": "76CAjNJq1MHQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0il_ZCvm1vpG",
      "metadata": {
        "id": "0il_ZCvm1vpG"
      },
      "outputs": [],
      "source": [
        "# 1. Setup transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Create dataset\n",
        "test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "\n",
        "# 3. Load model\n",
        "model = initialize_levit_efficientnet_hybrid(num_classes=4, levit_model='levit_384')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_efficientnet_model_final.pth')['model_state_dict'])\n",
        "model = model.cuda()\n",
        "\n",
        "#4\n",
        "visualize_class_specific_attention(model, test_dataset, tumor_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sPO3t8Nw7RVV",
      "metadata": {
        "id": "sPO3t8Nw7RVV"
      },
      "source": [
        "# CoAtNet with EffcientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nfg8_20c7QYT",
      "metadata": {
        "id": "Nfg8_20c7QYT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import models\n",
        "\n",
        "class CoAtNetEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, coatnet_model='coatnet_0_rw_224.sw_in1k', pretrained=True):\n",
        "        super(CoAtNetEfficientNetHybrid, self).__init__()\n",
        "\n",
        "        # Load pretrained CoAtNet model\n",
        "        self.coatnet = timm.create_model(coatnet_model, pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Load pretrained EfficientNet-B0\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Remove the final classification layers from EfficientNet\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        # Get the number of features from both models\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Combine features\n",
        "        self.combined_features = coatnet_num_features + efficientnet_num_features\n",
        "\n",
        "        # Add final classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through CoAtNet\n",
        "        coatnet_output = self.coatnet(x)\n",
        "\n",
        "        # Process input through EfficientNet\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat((coatnet_output, efficientnet_output), dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "# Function to initialize the model\n",
        "def initialize_coatnet_efficientnet_hybrid(num_classes, coatnet_model='coatnet_0_rw_224.sw_in1k', pretrained=True):\n",
        "    return CoAtNetEfficientNetHybrid(num_classes, coatnet_model, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o6nF0AblSdM5",
      "metadata": {
        "id": "o6nF0AblSdM5"
      },
      "outputs": [],
      "source": [
        "def train_coatnet_efficientnet_hybrid(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5, coatnet_model='coatnet_0_rw_224.sw_in1k'):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = initialize_coatnet_efficientnet_hybrid(num_classes, coatnet_model).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item() * inputs.size(0)\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        continue\n",
        "\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            val_acc = correct / total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model = model.state_dict()\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Train on the entire dataset\n",
        "    print('FINAL TRAINING')\n",
        "    print('--------------------------------')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    final_model = initialize_coatnet_efficientnet_hybrid(num_classes, coatnet_model).to(device)\n",
        "    final_optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    final_scheduler = ReduceLROnPlateau(final_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                final_optimizer.zero_grad()\n",
        "                outputs = final_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                final_optimizer.step()\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError in final training loop: {e}\")\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                continue\n",
        "\n",
        "        final_model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = final_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in final validation loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    continue\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        final_scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_model = final_model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    final_training_time = end_time - start_time\n",
        "    results.append({\n",
        "        'Fold': 'Final',\n",
        "        'Best Validation Loss': best_val_loss,\n",
        "        'Training Time (s)': final_training_time\n",
        "    })\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save(best_model, 'final_custom_coatnet_efficientnet_hybrid_model.pth')\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y-Mc0QDm3xQg",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "y-Mc0QDm3xQg"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Specify the CoAtNet model to use\n",
        "    coatnet_model = 'coatnet_0_rw_224.sw_in1k'\n",
        "    print(f\"Using model: {coatnet_model}\")\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/testing\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the datasets\n",
        "    train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "    # Print dataset sizes\n",
        "    print(\"Dataset sizes:\")\n",
        "    print(f\"Training: {len(train_dataset)}\")\n",
        "    print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "    try:\n",
        "        # Train the Custom CoAtNet-EfficientNet model\n",
        "        results, final_model = train_coatnet_efficientnet_hybrid(\n",
        "            train_dataset,\n",
        "            test_dataset,\n",
        "            num_classes,\n",
        "            coatnet_model=coatnet_model\n",
        "        )\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'coatnet_efficientnet_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Create test dataloader for final evaluation\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=128,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and display the metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'coatnet_efficientnet_hybrid_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model with all relevant information\n",
        "        model_path = os.path.join(base_path, 'coatnet_efficientnet_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'coatnet_model': coatnet_model,  # Save the specific CoAtNet model used\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'coatnet_test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nCustom CoAtNet-EfficientNet training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Y0rNJGLGO1B",
      "metadata": {
        "id": "1Y0rNJGLGO1B"
      },
      "source": [
        "# CoAtNet with GradCam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hBI7g0tIJIdy",
      "metadata": {
        "id": "hBI7g0tIJIdy"
      },
      "outputs": [],
      "source": [
        "# Function to initialize the model\n",
        "def initialize_coatnet_efficientnet_hybrid(num_classes, coatnet_model='coatnet_0_rw_224.sw_in1k', pretrained=True):\n",
        "    return CoAtNetEfficientNetHybrid(num_classes, coatnet_model, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7XZ2zoxGZA7",
      "metadata": {
        "id": "g7XZ2zoxGZA7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CoAtNetEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, coatnet_model='coatnet_0_rw_224.sw_in1k', pretrained=True):\n",
        "        super(CoAtNetEfficientNetHybrid, self).__init__()\n",
        "\n",
        "        self.coatnet = timm.create_model(coatnet_model, pretrained=pretrained, num_classes=0)\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "        self.combined_features = coatnet_num_features + efficientnet_num_features\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Store activation and gradient\n",
        "        self.last_conv_output = None\n",
        "        self.last_conv_gradient = None\n",
        "\n",
        "        # Register hooks\n",
        "        def forward_hook(module, input, output):\n",
        "            self.last_conv_output = output\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.last_conv_gradient = grad_output[0]\n",
        "\n",
        "        # Find and register hooks for the last convolutional layer in CoAtNet\n",
        "        for name, module in self.coatnet.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                module.register_forward_hook(forward_hook)\n",
        "                module.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    def forward(self, x):\n",
        "        coatnet_output = self.coatnet(x)\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "        combined = torch.cat((coatnet_output, efficientnet_output), dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "    def get_activation_gradient(self):\n",
        "        return self.last_conv_gradient\n",
        "\n",
        "    def get_activation(self):\n",
        "        return self.last_conv_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qQ6tYYvDIJFm",
      "metadata": {
        "id": "qQ6tYYvDIJFm"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "\n",
        "def modify_inplace_operations(model):\n",
        "    \"\"\"Disable inplace operations in the model\"\"\"\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.ReLU):\n",
        "            module.inplace = False\n",
        "        elif hasattr(module, 'act') and isinstance(module.act, nn.ReLU):\n",
        "            module.act.inplace = False\n",
        "\n",
        "\n",
        "class ModifiedCoAtNetEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, coatnet_model='coatnet_0_rw_224.sw_in1k', pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.coatnet = timm.create_model(coatnet_model, pretrained=pretrained, num_classes=0)\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        coatnet_num_features = self.coatnet.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "        self.combined_features = coatnet_num_features + efficientnet_num_features\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Register hooks for feature extraction\n",
        "        def hook_fn(module, input, output):\n",
        "            module.latest_features = output.clone()\n",
        "\n",
        "        def grad_hook_fn(module, grad_input, grad_output):\n",
        "            module.latest_gradients = grad_output[0].clone()\n",
        "\n",
        "        for module in self.coatnet.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                module.register_forward_hook(hook_fn)\n",
        "                module.register_full_backward_hook(grad_hook_fn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.clone()  # Clone input to avoid inplace modifications\n",
        "        coatnet_output = self.coatnet(x)\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "        combined = torch.cat((coatnet_output, efficientnet_output), dim=1)\n",
        "        return self.fc(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O2U6mEofIdrW",
      "metadata": {
        "id": "O2U6mEofIdrW"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    \"\"\"Helper class to extract features and gradients\"\"\"\n",
        "    def __init__(self):\n",
        "        self.features = None\n",
        "        self.gradients = None\n",
        "\n",
        "    def save_features(self, module, input, output):\n",
        "        self.features = output.detach()\n",
        "\n",
        "    def save_gradients(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "def get_last_conv_layer(model):\n",
        "    \"\"\"Get the last convolutional layer in the model\"\"\"\n",
        "    last_conv_layer = None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_conv_layer = module\n",
        "    return last_conv_layer\n",
        "\n",
        "def visualize_test_folder(test_folder, model_path, num_samples=5):\n",
        "    \"\"\"Wrapper function for visualization\"\"\"\n",
        "    # Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "\n",
        "    # Initialize the base model\n",
        "    model = initialize_coatnet_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "\n",
        "    # Load state dict\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cuda', weights_only=True)\n",
        "    except:\n",
        "        print(\"Warning: Failed with weights_only=True, trying regular load...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cuda')\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Turn off all inplace operations\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.ReLU):\n",
        "            module.inplace = False\n",
        "\n",
        "    visualize_hybrid_attention(model, test_dataset, tumor_types, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q4l9JbRVJGCf",
      "metadata": {
        "id": "Q4l9JbRVJGCf"
      },
      "outputs": [],
      "source": [
        "def visualize_hybrid_attention(model, test_dataset, tumor_types, num_samples=5):\n",
        "    \"\"\"Visualize attention maps\"\"\"\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Move to CUDA and create a copy for attention computation\n",
        "            image_display = image.clone()  # For display\n",
        "            image = image.cuda()  # For computation\n",
        "            true_class = tumor_types[label.item()]\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                output = model(image)\n",
        "            pred = torch.argmax(output).item()\n",
        "            pred_class = tumor_types[pred]\n",
        "\n",
        "            # Get attention maps\n",
        "            coatnet_attn = get_coatnet_attention(model, image, label.item())\n",
        "            cnn_gradcam = get_gradcam(model, image, label.item())\n",
        "\n",
        "            # Create visualization\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Original image - use image_display instead of image\n",
        "            img_display = image_display.squeeze().permute(1, 2, 0).detach().numpy()\n",
        "            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "            # Plot original\n",
        "            axes[0].imshow(img_display)\n",
        "            axes[0].set_title(f'Original Image\\nTrue: {true_class}\\nPred: {pred_class}')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Plot CoAtNet attention\n",
        "            axes[1].imshow(img_display, alpha=0.6)\n",
        "            im1 = axes[1].imshow(coatnet_attn, cmap='jet', alpha=0.4)\n",
        "            axes[1].set_title('CoAtNet Attention')\n",
        "            axes[1].axis('off')\n",
        "            plt.colorbar(im1, ax=axes[1])\n",
        "\n",
        "            # Plot EfficientNet GradCAM\n",
        "            axes[2].imshow(img_display, alpha=0.6)\n",
        "            im2 = axes[2].imshow(cnn_gradcam, cmap='jet', alpha=0.4)\n",
        "            axes[2].set_title('EfficientNet GradCAM')\n",
        "            axes[2].axis('off')\n",
        "            plt.colorbar(im2, ax=axes[2])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            if true_class != pred_class:\n",
        "                print(\"⚠️ Misclassified case!\")\n",
        "            print(f\"True class: {true_class}\")\n",
        "            print(f\"Predicted class: {pred_class}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "def get_coatnet_attention(model, input_image, class_idx):\n",
        "    \"\"\"Generate attention map for CoAtNet\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create feature extractor\n",
        "    feature_extractor = FeatureExtractor()\n",
        "\n",
        "    # Get last conv layer\n",
        "    last_conv = get_last_conv_layer(model.coatnet)\n",
        "    if last_conv is None:\n",
        "        raise ValueError(\"Could not find convolutional layer in model\")\n",
        "\n",
        "    # Register hooks\n",
        "    forward_handle = last_conv.register_forward_hook(feature_extractor.save_features)\n",
        "    backward_handle = last_conv.register_full_backward_hook(feature_extractor.save_gradients)\n",
        "\n",
        "    try:\n",
        "        # Forward pass\n",
        "        input_tensor = input_image.clone().detach().requires_grad_(True)\n",
        "        output = model(input_tensor)\n",
        "\n",
        "        if output.dim() == 1:\n",
        "            output = output.unsqueeze(0)\n",
        "\n",
        "        # One-hot encoding for the target class\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, class_idx] = 1\n",
        "\n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        output.backward(gradient=one_hot)\n",
        "\n",
        "        # Get feature maps and gradients\n",
        "        features = feature_extractor.features\n",
        "        gradients = feature_extractor.gradients\n",
        "\n",
        "        if features is None or gradients is None:\n",
        "            raise ValueError(\"Features or gradients were not captured\")\n",
        "\n",
        "        # Calculate attention weights\n",
        "        pooled_gradients = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
        "        attention = features * pooled_gradients\n",
        "        attention = torch.sum(attention, dim=1, keepdim=True)\n",
        "\n",
        "        # Apply ReLU and normalize\n",
        "        attention = F.relu(attention)\n",
        "        attention = F.interpolate(attention,\n",
        "                                size=(224, 224),\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False)\n",
        "\n",
        "        attention = attention.squeeze().cpu().detach().numpy()\n",
        "        attention = (attention - np.min(attention)) / (np.max(attention) - np.min(attention) + 1e-8)\n",
        "\n",
        "        return attention\n",
        "\n",
        "    finally:\n",
        "        # Remove hooks\n",
        "        forward_handle.remove()\n",
        "        backward_handle.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3r7COuYCIfHd",
      "metadata": {
        "id": "3r7COuYCIfHd"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/coatnet_efficientnet_model_final.pth'\n",
        "\n",
        "# Run visualization\n",
        "visualize_test_folder(test_folder, model_path, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2tBPsDCQm54",
      "metadata": {
        "id": "m2tBPsDCQm54"
      },
      "outputs": [],
      "source": [
        "def get_fc_gradcam(model, image, target_class):\n",
        "    \"\"\"Get GradCAM-like visualization for the final fully connected layers\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Store activations and gradients\n",
        "    activations = {}\n",
        "    gradients = {}\n",
        "\n",
        "    def save_activation(name):\n",
        "        def hook(module, input, output):\n",
        "            activations[name] = output\n",
        "        return hook\n",
        "\n",
        "    def save_gradient(name):\n",
        "        def hook(module, grad_in, grad_out):\n",
        "            gradients[name] = grad_out[0]\n",
        "        return hook\n",
        "\n",
        "    # Register hooks for both CNN and Transformer features before they're combined\n",
        "    model.coatnet.register_forward_hook(save_activation('coatnet'))\n",
        "    model.efficientnet_features.register_forward_hook(save_activation('efficientnet'))\n",
        "\n",
        "    # Forward pass\n",
        "    image.requires_grad_()\n",
        "    output = model(image)\n",
        "    score = output[0, target_class]\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    score.backward()\n",
        "\n",
        "    # Get the combined importance from both branches\n",
        "    coatnet_importance = None\n",
        "    if 'coatnet' in activations and 'coatnet' in gradients:\n",
        "        coatnet_act = activations['coatnet']\n",
        "        coatnet_grad = gradients['coatnet']\n",
        "        coatnet_importance = (coatnet_act * coatnet_grad).sum(dim=1, keepdim=True)\n",
        "\n",
        "    efficientnet_importance = None\n",
        "    if 'efficientnet' in activations and 'efficientnet' in gradients:\n",
        "        eff_act = activations['efficientnet']\n",
        "        eff_grad = gradients['efficientnet']\n",
        "        efficientnet_importance = (eff_act * eff_grad).sum(dim=1, keepdim=True)\n",
        "\n",
        "    # Combine importances\n",
        "    if coatnet_importance is not None and efficientnet_importance is not None:\n",
        "        combined_importance = coatnet_importance + efficientnet_importance\n",
        "    else:\n",
        "        combined_importance = coatnet_importance if coatnet_importance is not None else efficientnet_importance\n",
        "\n",
        "    # Apply ReLU to focus on features that positively influence the target class\n",
        "    combined_importance = F.relu(combined_importance)\n",
        "\n",
        "    # Interpolate to original image size\n",
        "    combined_importance = F.interpolate(\n",
        "        combined_importance,\n",
        "        size=(224, 224),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # Normalize\n",
        "    combined_importance = combined_importance.squeeze().detach().cpu().numpy()\n",
        "    combined_importance = (combined_importance - np.min(combined_importance)) / \\\n",
        "                         (np.max(combined_importance) - np.min(combined_importance) + 1e-8)\n",
        "\n",
        "    return combined_importance\n",
        "\n",
        "def visualize_all_cams(model, image, label, tumor_types):\n",
        "    \"\"\"Visualize original GradCAM, FC GradCAM, and combined view\"\"\"\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "    pred = torch.argmax(output).item()\n",
        "\n",
        "    # Get different attention maps\n",
        "    gradcam = get_gradcam(model, image, label.item())\n",
        "    fc_gradcam = get_fc_gradcam(model, image, label.item())\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Original image\n",
        "    img_display = image.cpu().squeeze().permute(1, 2, 0).detach().numpy()\n",
        "    img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "    # Plot original\n",
        "    axes[0].imshow(img_display)\n",
        "    axes[0].set_title(f'Original Image\\nTrue: {tumor_types[label.item()]}\\nPred: {tumor_types[pred]}')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plot CNN GradCAM\n",
        "    axes[1].imshow(img_display, alpha=0.6)\n",
        "    im1 = axes[1].imshow(gradcam, cmap='jet', alpha=0.4)\n",
        "    axes[1].set_title('CNN GradCAM')\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im1, ax=axes[1])\n",
        "\n",
        "    # Plot FC GradCAM\n",
        "    axes[2].imshow(img_display, alpha=0.6)\n",
        "    im2 = axes[2].imshow(fc_gradcam, cmap='jet', alpha=0.4)\n",
        "    axes[2].set_title('FC GradCAM')\n",
        "    axes[2].axis('off')\n",
        "    plt.colorbar(im2, ax=axes[2])\n",
        "\n",
        "    # Plot Combined (average of both)\n",
        "    combined = (gradcam + fc_gradcam) / 2\n",
        "    axes[3].imshow(img_display, alpha=0.6)\n",
        "    im3 = axes[3].imshow(combined, cmap='jet', alpha=0.4)\n",
        "    axes[3].set_title('Combined View')\n",
        "    axes[3].axis('off')\n",
        "    plt.colorbar(im3, ax=axes[3])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_test_folder_all_cams(test_folder, model_path, num_samples=5):\n",
        "    \"\"\"Wrapper function to visualize all attention maps from test folder\"\"\"\n",
        "    # Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "\n",
        "    # Initialize model\n",
        "    model = initialize_coatnet_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "\n",
        "    # Load state dict\n",
        "    checkpoint = torch.load(model_path, map_location='cuda')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Create data loader\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Visualize samples\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            image = image.cuda()\n",
        "            visualize_all_cams(model, image, label, tumor_types)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aKTst6lTReHg",
      "metadata": {
        "id": "aKTst6lTReHg"
      },
      "outputs": [],
      "source": [
        "def disable_inplace_operations(model):\n",
        "    \"\"\"Disable all inplace operations in the model\"\"\"\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, 'inplace'):\n",
        "            module.inplace = False\n",
        "        # Handle squeeze-excite layers specifically\n",
        "        if hasattr(module, 'act') and hasattr(module.act, 'inplace'):\n",
        "            module.act.inplace = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQtdSC0fSLEQ",
      "metadata": {
        "id": "YQtdSC0fSLEQ"
      },
      "outputs": [],
      "source": [
        "def get_fc_gradcam(model, input_image, target_class):\n",
        "    \"\"\"Generate attention map focusing on the last convolutional layer before FC\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Store the activations and gradients\n",
        "    activations = {}\n",
        "    gradients = {}\n",
        "\n",
        "    def save_activation(module, input, output):\n",
        "        activations['features'] = output.detach()\n",
        "\n",
        "    def save_gradient(module, grad_input, grad_output):\n",
        "        gradients['features'] = grad_output[0].detach()\n",
        "\n",
        "    # Find the last convolutional layer before the FC layers\n",
        "    last_conv = None\n",
        "    for module in model.efficientnet.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_conv = module\n",
        "\n",
        "    if last_conv is None:\n",
        "        raise ValueError(\"Could not find convolutional layer\")\n",
        "\n",
        "    # Register hooks\n",
        "    handle1 = last_conv.register_forward_hook(save_activation)\n",
        "    handle2 = last_conv.register_full_backward_hook(save_gradient)\n",
        "\n",
        "    try:\n",
        "        # Forward pass\n",
        "        input_tensor = input_image.clone().requires_grad_()\n",
        "        output = model(input_tensor)\n",
        "\n",
        "        if not activations['features'].requires_grad:\n",
        "            activations['features'].requires_grad = True\n",
        "\n",
        "        # Target for backprop\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, target_class] = 1\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        # Get weights based on global average pooling of gradients\n",
        "        weights = torch.mean(gradients['features'], dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # Get weighted combination of forward activation maps\n",
        "        cam = torch.mul(activations['features'], weights).sum(dim=1, keepdim=True)\n",
        "\n",
        "        # ReLU\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Interpolate to image size\n",
        "        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Convert to numpy and normalize - with proper detachment\n",
        "        cam = cam.squeeze().detach().cpu().numpy()\n",
        "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
        "\n",
        "        return cam\n",
        "\n",
        "    finally:\n",
        "        handle1.remove()\n",
        "        handle2.remove()\n",
        "\n",
        "def visualize_test_folder_all_cams(test_folder, model_path, num_samples=5):\n",
        "    \"\"\"Wrapper function to visualize all attention maps from test folder\"\"\"\n",
        "    # Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset and loader\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = initialize_coatnet_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "\n",
        "    # Disable inplace operations\n",
        "    disable_inplace_operations(model)\n",
        "\n",
        "    # Load state dict\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cuda', weights_only=True)\n",
        "    except:\n",
        "        print(\"Warning: Failed with weights_only=True, trying regular load...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cuda')\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            image = image.cuda()\n",
        "            true_class = tumor_types[label.item()]\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                output = model(image)\n",
        "            pred = torch.argmax(output).item()\n",
        "            pred_class = tumor_types[pred]\n",
        "\n",
        "            # Get attention maps\n",
        "            gradcam = get_gradcam(model, image.clone(), label.item())\n",
        "            fc_gradcam = get_fc_gradcam(model, image.clone(), label.item())\n",
        "\n",
        "            # Plotting\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Original image - with proper detachment\n",
        "            img_display = image.cpu().squeeze().permute(1, 2, 0).detach().numpy()\n",
        "            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "            # Plot original\n",
        "            axes[0].imshow(img_display)\n",
        "            axes[0].set_title(f'Original Image\\nTrue: {true_class}\\nPred: {pred_class}')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Plot CNN GradCAM\n",
        "            axes[1].imshow(img_display, alpha=0.6)\n",
        "            im1 = axes[1].imshow(gradcam, cmap='jet', alpha=0.4)\n",
        "            axes[1].set_title('CNN GradCAM')\n",
        "            axes[1].axis('off')\n",
        "            plt.colorbar(im1, ax=axes[1])\n",
        "\n",
        "            # Plot FC GradCAM\n",
        "            axes[2].imshow(img_display, alpha=0.6)\n",
        "            im2 = axes[2].imshow(fc_gradcam, cmap='jet', alpha=0.4)\n",
        "            axes[2].set_title('FC GradCAM')\n",
        "            axes[2].axis('off')\n",
        "            plt.colorbar(im2, ax=axes[2])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            if true_class != pred_class:\n",
        "                print(\"⚠️ Misclassified case!\")\n",
        "            print(f\"True class: {true_class}\")\n",
        "            print(f\"Predicted class: {pred_class}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yguL5UisQoMX",
      "metadata": {
        "id": "yguL5UisQoMX"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/coatnet_efficientnet_model_final.pth'\n",
        "visualize_test_folder_all_cams(test_folder, model_path, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ofeWNcpMTeYp",
      "metadata": {
        "id": "ofeWNcpMTeYp"
      },
      "outputs": [],
      "source": [
        "def visualize_test_folder_all_cams(test_folder, model_path, num_samples=5):\n",
        "    \"\"\"Enhanced visualization with multiple color schemes\"\"\"\n",
        "    # Setup transforms\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset and loader\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = initialize_coatnet_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "    disable_inplace_operations(model)\n",
        "\n",
        "    # Load state dict\n",
        "    checkpoint = torch.load(model_path, map_location='cuda')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Different colormaps to try\n",
        "    colormaps = ['jet', 'hot', 'plasma']\n",
        "\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            image = image.cuda()\n",
        "            true_class = tumor_types[label.item()]\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                output = model(image)\n",
        "            pred = torch.argmax(output).item()\n",
        "            pred_class = tumor_types[pred]\n",
        "\n",
        "            # Get attention maps\n",
        "            gradcam = get_gradcam(model, image.clone(), label.item())\n",
        "            fc_gradcam = get_fc_gradcam(model, image.clone(), label.item())\n",
        "\n",
        "            # Create multiple visualizations with different parameters\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "            # Original image\n",
        "            img_display = image.cpu().squeeze().permute(1, 2, 0).detach().numpy()\n",
        "            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "            # Row 1: Original colormaps\n",
        "            axes[0, 0].imshow(img_display, cmap='gray')\n",
        "            axes[0, 0].set_title(f'Original Image\\nTrue: {true_class}\\nPred: {pred_class}')\n",
        "            axes[0, 0].axis('off')\n",
        "\n",
        "            # CNN GradCAM with jet colormap\n",
        "            axes[0, 1].imshow(img_display, cmap='gray', alpha=0.7)\n",
        "            im1 = axes[0, 1].imshow(gradcam, cmap='jet', alpha=0.5)\n",
        "            axes[0, 1].set_title('CNN GradCAM (jet)')\n",
        "            axes[0, 1].axis('off')\n",
        "            plt.colorbar(im1, ax=axes[0, 1])\n",
        "\n",
        "            # FC GradCAM with jet colormap\n",
        "            axes[0, 2].imshow(img_display, cmap='gray', alpha=0.7)\n",
        "            im2 = axes[0, 2].imshow(fc_gradcam, cmap='jet', alpha=0.5)\n",
        "            axes[0, 2].set_title('FC GradCAM (jet)')\n",
        "            axes[0, 2].axis('off')\n",
        "            plt.colorbar(im2, ax=axes[0, 2])\n",
        "\n",
        "            # Row 2: Alternative visualizations\n",
        "            # Heatmap only - CNN GradCAM\n",
        "            axes[1, 0].imshow(gradcam, cmap='hot')\n",
        "            axes[1, 0].set_title('CNN GradCAM Heatmap Only (hot)')\n",
        "            axes[1, 0].axis('off')\n",
        "\n",
        "            # Heatmap only - FC GradCAM\n",
        "            axes[1, 1].imshow(fc_gradcam, cmap='plasma')\n",
        "            axes[1, 1].set_title('FC GradCAM Heatmap Only (plasma)')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            # Combined visualization\n",
        "            combined = (gradcam + fc_gradcam) / 2\n",
        "            axes[1, 2].imshow(img_display, cmap='gray', alpha=0.7)\n",
        "            im3 = axes[1, 2].imshow(combined, cmap='viridis', alpha=0.5)\n",
        "            axes[1, 2].set_title('Combined Attention')\n",
        "            axes[1, 2].axis('off')\n",
        "            plt.colorbar(im3, ax=axes[1, 2])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Print additional information\n",
        "            print(f\"True class: {true_class}\")\n",
        "            print(f\"Predicted class: {pred_class}\")\n",
        "            print(f\"Max CNN attention value: {np.max(gradcam):.3f}\")\n",
        "            print(f\"Max FC attention value: {np.max(fc_gradcam):.3f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcQ2jbS2TfQ6",
      "metadata": {
        "id": "dcQ2jbS2TfQ6"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/coatnet_efficientnet_model_final.pth'\n",
        "visualize_test_folder_all_cams(test_folder, model_path, num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w_er4mAPfnhz",
      "metadata": {
        "id": "w_er4mAPfnhz"
      },
      "source": [
        "#Cross-Covariance Image Transformers w/ EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W78j7T_UfrZD",
      "metadata": {
        "id": "W78j7T_UfrZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import models\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "class XCiTEfficientNetHybrid(nn.Module):\n",
        "    def __init__(self, num_classes, xcit_model='xcit_small_24_p8_224', pretrained=True):\n",
        "        super(XCiTEfficientNetHybrid, self).__init__()\n",
        "\n",
        "        # Load pretrained XCiT model\n",
        "        self.xcit = timm.create_model(xcit_model, pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Load pretrained EfficientNet-B0\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Remove the final classification layers from EfficientNet\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        # Get the number of features from both models\n",
        "        xcit_num_features = self.xcit.num_features\n",
        "        efficientnet_num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Combine features\n",
        "        self.combined_features = xcit_num_features + efficientnet_num_features\n",
        "\n",
        "        # Add final classification layers with modified batch normalization\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.combined_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through XCiT\n",
        "        xcit_output = self.xcit(x)\n",
        "\n",
        "        # Process input through EfficientNet\n",
        "        efficientnet_output = self.efficientnet_features(x)\n",
        "        efficientnet_output = torch.flatten(efficientnet_output, 1)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat((xcit_output, efficientnet_output), dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "def initialize_xcit_efficientnet_hybrid(num_classes, xcit_model='xcit_small_24_p8_224', pretrained=True):\n",
        "    return XCiTEfficientNetHybrid(num_classes, xcit_model, pretrained)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oQRNcncS6TDE",
      "metadata": {
        "id": "oQRNcncS6TDE"
      },
      "outputs": [],
      "source": [
        "def train_xcit_efficientnet_hybrid(train_dataset, test_dataset, num_classes, num_epochs=100, patience=10, k_folds=5, xcit_model='xcit_small_24_p8_224'):\n",
        "    results = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Memory optimization settings\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Correct GradScaler initialization\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    # Adjusted batch sizes and accumulation\n",
        "    batch_size = 16\n",
        "    accumulation_steps = 2\n",
        "\n",
        "    # K-Fold Cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        print(f'FOLD {fold+1}')\n",
        "        print('--------------------------------')\n",
        "\n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=val_subsampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        model = initialize_xcit_efficientnet_hybrid(num_classes, xcit_model).to(device)\n",
        "\n",
        "        # Fixed gradient checkpointing call\n",
        "        if hasattr(model.xcit, 'set_grad_checkpointing'):\n",
        "            model.xcit.set_grad_checkpointing(True)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(train_loader):\n",
        "                try:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    # Updated autocast usage\n",
        "                    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss = loss / accumulation_steps\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "\n",
        "                    if (i + 1) % accumulation_steps == 0:\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                    train_loss += loss.item() * inputs.size(0) * accumulation_steps\n",
        "\n",
        "                    del outputs, loss\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"RuntimeError in training loop: {e}\")\n",
        "                    print(f\"Input shape: {inputs.shape}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    try:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                        del outputs, loss\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"RuntimeError in validation loop: {e}\")\n",
        "                        print(f\"Input shape: {inputs.shape}\")\n",
        "                        torch.cuda.empty_cache()\n",
        "                        continue\n",
        "\n",
        "            if total > 0:\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                val_loss = val_loss / total\n",
        "                val_acc = correct / total\n",
        "\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "                print(f'Train Loss: {train_loss:.4f}')\n",
        "                print(f'Val Loss: {val_loss:.4f}')\n",
        "                print(f'Val Acc: {val_acc:.4f}')\n",
        "\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    epochs_no_improve = 0\n",
        "                    best_model = model.state_dict()\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "\n",
        "                if epochs_no_improve == patience:\n",
        "                    print('Early stopping!')\n",
        "                    break\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "        results.append({\n",
        "            'Fold': fold+1,\n",
        "            'Best Validation Loss': best_val_loss,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "        # Clean up memory after each fold\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Load the final model for return\n",
        "    final_model = initialize_xcit_efficientnet_hybrid(num_classes, xcit_model).to(device)\n",
        "    final_model.load_state_dict(best_model)\n",
        "\n",
        "    return results, final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wA8qPvitXNbj",
      "metadata": {
        "id": "wA8qPvitXNbj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Training'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "    tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "    num_classes = len(tumor_types)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = os.path.join(base_path, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Specify the XCiT model to use\n",
        "    xcit_model = 'xcit_small_24_p8_224'\n",
        "    print(f\"Using model: {xcit_model}\")\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # Create the datasets\n",
        "    train_dataset = MRIDataset(train_folder, tumor_types, transform=data_transforms['train'])\n",
        "    test_dataset = MRIDataset(test_folder, tumor_types, transform=data_transforms['val'])\n",
        "\n",
        "    # Print dataset sizes\n",
        "    print(\"Dataset sizes:\")\n",
        "    print(f\"Training: {len(train_dataset)}\")\n",
        "    print(f\"Testing: {len(test_dataset)}\")\n",
        "\n",
        "    try:\n",
        "        # Train the model\n",
        "        results, final_model = train_xcit_efficientnet_hybrid(\n",
        "            train_dataset,\n",
        "            test_dataset,\n",
        "            num_classes,\n",
        "            xcit_model=xcit_model\n",
        "        )\n",
        "\n",
        "        # Display training results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTraining Results:\")\n",
        "        print(results_df)\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(base_path, 'xcit_efficientnet_training_history.csv')\n",
        "        results_df.to_csv(history_path, index=False)\n",
        "        print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "        # Create test dataloader for final evaluation\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=128,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        test_acc, precision, recall, f1, auc_roc, test_loss, predictions, true_labels = evaluate_model(\n",
        "            final_model,\n",
        "            test_loader,\n",
        "            tumor_types\n",
        "        )\n",
        "\n",
        "        # Create and save metrics DataFrame\n",
        "        metrics_df = create_metrics_dataframe(\n",
        "            final_model,\n",
        "            test_acc,\n",
        "            precision,\n",
        "            recall,\n",
        "            f1,\n",
        "            auc_roc,\n",
        "            results[-1]['Training Time (s)'],\n",
        "            test_loss\n",
        "        )\n",
        "        print(\"\\nModel Metrics:\")\n",
        "        print(metrics_df.to_string())\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_csv_path = os.path.join(base_path, 'xcit_efficientnet_model_metrics.csv')\n",
        "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "        print(f\"\\nMetrics saved to {metrics_csv_path}\")\n",
        "\n",
        "        # Save the final model\n",
        "        model_path = os.path.join(base_path, 'xcit_efficientnet_model_final.pth')\n",
        "        torch.save({\n",
        "            'epoch': len(results),\n",
        "            'model_state_dict': final_model.state_dict(),\n",
        "            'results': results,\n",
        "            'test_metrics': {\n",
        "                'accuracy': test_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc_roc': auc_roc,\n",
        "                'test_loss': test_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "        print(f\"\\nModel saved to {model_path}\")\n",
        "\n",
        "        # Save predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'True_Label': [tumor_types[i] for i in true_labels],\n",
        "            'Predicted_Label': [tumor_types[i] for i in predictions]\n",
        "        })\n",
        "        predictions_path = os.path.join(base_path, 'test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"Test predictions saved to {predictions_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    print(\"\\nXCiT-EfficientNet training, evaluation, and metrics logging complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lBSaMmWsesyN",
      "metadata": {
        "id": "lBSaMmWsesyN"
      },
      "source": [
        "# Attention Visualization:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XhlJEGkR_NlA",
      "metadata": {
        "id": "XhlJEGkR_NlA"
      },
      "source": [
        "### Loading the models we trained :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PLu_mHu0rhnt",
      "metadata": {
        "collapsed": true,
        "id": "PLu_mHu0rhnt"
      },
      "outputs": [],
      "source": [
        "# Efficient Net B0\n",
        "\n",
        "cnn _model = initialize_model(num_classes=len(tumor_types))\n",
        "cnn_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/final_efficientnet_b0_mri_classification_model.pth'))\n",
        "cnn_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "StLI1j_W35R2",
      "metadata": {
        "collapsed": true,
        "id": "StLI1j_W35R2"
      },
      "outputs": [],
      "source": [
        "# LeVit\n",
        "\n",
        "levit_model = initialize_levit_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "levit_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/levit_efficientnet_model_final.pth')['model_state_dict'])\n",
        "levit_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ItA4ghxb3Lzt",
      "metadata": {
        "collapsed": true,
        "id": "ItA4ghxb3Lzt"
      },
      "outputs": [],
      "source": [
        "#CoAtNet\n",
        "\n",
        "coatnet_model = initialize_coatnet_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "coatnet_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/coatnet_efficientnet_model_final.pth')['model_state_dict'])\n",
        "coatnet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FfQ1VyfW1EjU",
      "metadata": {
        "collapsed": true,
        "id": "FfQ1VyfW1EjU"
      },
      "outputs": [],
      "source": [
        "#XCIT\n",
        "\n",
        "xcit_model = initialize_xcit_efficientnet_hybrid(num_classes=len(tumor_types))\n",
        "xcit_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/xcit_efficientnet_model_final.pth')['model_state_dict'])\n",
        "xcit_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sQXA4TTg5DRv",
      "metadata": {
        "id": "sQXA4TTg5DRv"
      },
      "source": [
        "## Grad-Cam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PxwivU2xDTXd",
      "metadata": {
        "id": "PxwivU2xDTXd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class BaseGradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "\n",
        "        # Using register_full_backward_hook instead of register_backward_hook\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    def get_cam(self, input_tensor):\n",
        "        # Forward pass\n",
        "        self.model.zero_grad()\n",
        "        output = self.model(input_tensor)\n",
        "        pred = output.argmax(dim=1).item()\n",
        "\n",
        "        # Get class activation\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0][pred] = 1\n",
        "\n",
        "        # Backward pass\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        return pred, output.softmax(dim=1)[0].detach().cpu().numpy()\n",
        "\n",
        "class EfficientNetGradCAM(BaseGradCAM):\n",
        "    def __init__(self, model):\n",
        "        super().__init__(model, model.features[7][0].block[1])\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        pred, class_probs = self.get_cam(input_tensor)\n",
        "\n",
        "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam.squeeze().cpu().numpy(), pred, class_probs\n",
        "\n",
        "class LeViTGradCAM(BaseGradCAM):\n",
        "    def __init__(self, model):\n",
        "        super().__init__(model, model.levit.stages[-1].blocks[-1].attn)\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        pred, class_probs = self.get_cam(input_tensor)\n",
        "\n",
        "        # Handle attention maps\n",
        "        b, n, c = self.activations.shape\n",
        "        h = w = int(np.sqrt(n - 1))  # Subtract 1 for CLS token\n",
        "\n",
        "        # Remove CLS token and reshape\n",
        "        activations = self.activations[:, 1:, :].reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        gradients = self.gradients[:, 1:, :].reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam.squeeze().cpu().numpy(), pred, class_probs\n",
        "\n",
        "class CoAtNetGradCAM(BaseGradCAM):\n",
        "    def __init__(self, model):\n",
        "        super().__init__(model, model.coatnet.stages[-1].blocks[-1].mlp)\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        pred, class_probs = self.get_cam(input_tensor)\n",
        "\n",
        "        # For CoAtNet, we need to reshape the activations to spatial dimensions\n",
        "        b, n, c = self.activations.shape\n",
        "        h = w = int(np.sqrt(n))\n",
        "\n",
        "        activations = self.activations.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        gradients = self.gradients.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam.squeeze().cpu().numpy(), pred, class_probs\n",
        "\n",
        "class XCiTGradCAM(BaseGradCAM):\n",
        "    def __init__(self, model):\n",
        "        super().__init__(model, model.xcit.blocks[-1].local_mp)\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        pred, class_probs = self.get_cam(input_tensor)\n",
        "\n",
        "        # For XCiT, handle the local attention features\n",
        "        if len(self.gradients.shape) == 3:\n",
        "            b, n, c = self.gradients.shape\n",
        "            h = w = int(np.sqrt(n))\n",
        "\n",
        "            activations = self.activations.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "            gradients = self.gradients.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            activations = self.activations\n",
        "            gradients = self.gradients\n",
        "\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam.squeeze().cpu().numpy(), pred, class_probs\n",
        "\n",
        "def visualize_all_models(image_path, models_dict, tumor_types, device='cuda'):\n",
        "    \"\"\"\n",
        "    Visualize GradCAM for a single image across all models\n",
        "    \"\"\"\n",
        "    # Prepare image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize model-specific GradCAM\n",
        "    gradcams = {}\n",
        "    for model_name, model in models_dict.items():\n",
        "        try:\n",
        "            if model_name == 'efficientnet':\n",
        "                gradcams[model_name] = EfficientNetGradCAM(model)\n",
        "            elif model_name == 'levit':\n",
        "                gradcams[model_name] = LeViTGradCAM(model)\n",
        "            elif model_name == 'coatnet':\n",
        "                gradcams[model_name] = CoAtNetGradCAM(model)\n",
        "            elif model_name == 'xcit':\n",
        "                gradcams[model_name] = XCiTGradCAM(model)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, len(gradcams) + 1, figsize=(5*(len(gradcams) + 1), 5))\n",
        "\n",
        "    # Original image\n",
        "    original_img = cv2.imread(image_path)\n",
        "    original_img = cv2.resize(original_img, (224, 224))\n",
        "    axes[0].imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title('Original')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Generate GradCAM for each model\n",
        "    for idx, (model_name, gradcam) in enumerate(gradcams.items()):\n",
        "        try:\n",
        "            # Create a new input tensor for each model to avoid graph conflicts\n",
        "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "            cam, pred_class, class_probs = gradcam(input_tensor)\n",
        "\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "            superimposed = cv2.addWeighted(original_img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "            axes[idx + 1].imshow(cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB))\n",
        "            axes[idx + 1].set_title(f'{model_name.upper()}\\nPred: {tumor_types[pred_class]}\\n' +\n",
        "                                  '\\n'.join([f'{tumor_types[i]}: {prob:.2%}'\n",
        "                                           for i, prob in enumerate(class_probs)]))\n",
        "            axes[idx + 1].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2rUjRAJe6iW_",
      "metadata": {
        "id": "2rUjRAJe6iW_"
      },
      "outputs": [],
      "source": [
        "# Initialize models and gradcam visualizer\n",
        "models_dict = {\n",
        "    'efficientnet': cnn_model,\n",
        "    'levit': levit_model,\n",
        "    'coatnet': coatnet_model,\n",
        "    'xcit': xcit_model\n",
        "}\n",
        "\n",
        "tumor_types = ['Normal', 'Glioma', 'Meningioma', 'Pituitary']\n",
        "\n",
        "# Pick a test image\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0286.jpg'\n",
        "\n",
        "# Visualize\n",
        "visualize_all_models(image_path, models_dict, tumor_types)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qOQAD1MkbT05",
      "metadata": {
        "id": "qOQAD1MkbT05"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_attention_maps(model, x):\n",
        "    \"\"\"\n",
        "    Directly compute attention maps from LeViT layers\n",
        "    \"\"\"\n",
        "    attention_maps = []\n",
        "\n",
        "    # Process through patch embedding\n",
        "    x = model.levit.patch_embed(x)\n",
        "\n",
        "    # Process through each stage\n",
        "    for stage in model.levit.stages:\n",
        "        # Apply stage transformation\n",
        "        x = stage(x)\n",
        "        # Get attention pattern\n",
        "        attention = x.mean(1)  # Average over channels\n",
        "        attention_maps.append(attention)\n",
        "\n",
        "    return attention_maps\n",
        "\n",
        "def process_attention_maps(attention_maps, image_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Process attention maps to match image size\n",
        "    \"\"\"\n",
        "    processed_maps = []\n",
        "\n",
        "    for attn_map in attention_maps:\n",
        "        # Interpolate to match image size\n",
        "        resized_map = F.interpolate(\n",
        "            attn_map.unsqueeze(1),\n",
        "            size=image_size,\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Normalize the map\n",
        "        normalized_map = resized_map.squeeze()\n",
        "        normalized_map = (normalized_map - normalized_map.min()) / (normalized_map.max() - normalized_map.min() + 1e-8)\n",
        "        processed_maps.append(normalized_map.cpu().detach())\n",
        "\n",
        "    return processed_maps\n",
        "\n",
        "def visualize_attention(image_path, model, tumor_types):\n",
        "    \"\"\"\n",
        "    Visualize attention patterns from LeViT transformer\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ])\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    original_image = np.array(image.resize((224, 224)))\n",
        "\n",
        "    # Move to same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Get model prediction and attention maps\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get attention maps\n",
        "        attention_maps = get_attention_maps(model, input_tensor)\n",
        "\n",
        "        # Get prediction\n",
        "        output = model(input_tensor)\n",
        "        probs = torch.nn.functional.softmax(output, dim=1)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    # Process attention maps\n",
        "    processed_maps = process_attention_maps(attention_maps)\n",
        "\n",
        "    # Visualization\n",
        "    num_maps = len(processed_maps)\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # Plot original image\n",
        "    plt.subplot(2, num_maps + 1, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot combined attention\n",
        "    plt.subplot(2, num_maps + 1, num_maps + 2)\n",
        "    combined_attention = torch.stack(processed_maps).mean(0)\n",
        "    plt.imshow(original_image)\n",
        "    plt.imshow(combined_attention.numpy(), cmap='jet', alpha=0.5)\n",
        "    plt.title('Combined Attention')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot individual attention maps\n",
        "    for i, attn_map in enumerate(processed_maps):\n",
        "        # Heatmap\n",
        "        plt.subplot(2, num_maps + 1, i + 2)\n",
        "        plt.imshow(attn_map.numpy(), cmap='jet')\n",
        "        plt.title(f'Stage {i+1} Attention')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Overlay\n",
        "        plt.subplot(2, num_maps + 1, num_maps + i + 3)\n",
        "        plt.imshow(original_image)\n",
        "        plt.imshow(attn_map.numpy(), cmap='jet', alpha=0.5)\n",
        "        plt.title(f'Stage {i+1} Overlay')\n",
        "        plt.axis('off')\n",
        "\n",
        "    if tumor_types:\n",
        "        pred_label = tumor_types[predicted_class]\n",
        "        pred_prob = probs[0][predicted_class].item()\n",
        "        plt.suptitle(f'LeViT Attention Visualization\\nPrediction: {pred_label} ({pred_prob:.3f})',\n",
        "                    fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return processed_maps, predicted_class, probs\n",
        "\n",
        "# Print model structure to verify the layer names\n",
        "print(\"Model structure:\")\n",
        "print(levit_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G5u4z9ZHVxKE",
      "metadata": {
        "id": "G5u4z9ZHVxKE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.features = []\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features through forward pass\"\"\"\n",
        "        self.features = []\n",
        "\n",
        "        # Process through stem\n",
        "        x = self.model.levit.stem(x)\n",
        "        self.features.append(x)\n",
        "\n",
        "        # Process through stages\n",
        "        for stage in self.model.levit.stages:\n",
        "            x = stage(x)\n",
        "            self.features.append(x)\n",
        "\n",
        "        return self.features\n",
        "\n",
        "def process_feature_map(feature, size=(224, 224)):\n",
        "    \"\"\"Process feature map for visualization\"\"\"\n",
        "    # Move to CPU and detach\n",
        "    feature = feature.cpu().detach()\n",
        "\n",
        "    # Get mean activation across channels\n",
        "    if len(feature.shape) == 4:  # [B, C, H, W]\n",
        "        feature = feature.mean(1)\n",
        "    elif len(feature.shape) == 3:  # [B, L, C]\n",
        "        # Reshape to 2D\n",
        "        L = feature.shape[1]\n",
        "        H = W = int(np.sqrt(L))\n",
        "        feature = feature.mean(-1).view(-1, H, W)\n",
        "\n",
        "    # Take first item if batched\n",
        "    if len(feature.shape) > 2:\n",
        "        feature = feature[0]\n",
        "\n",
        "    # Normalize\n",
        "    feature = (feature - feature.min()) / (feature.max() - feature.min() + 1e-8)\n",
        "\n",
        "    # Resize\n",
        "    feature = F.interpolate(\n",
        "        feature.unsqueeze(0).unsqueeze(0),\n",
        "        size=size,\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    ).squeeze()\n",
        "\n",
        "    return feature.numpy()\n",
        "\n",
        "def visualize_features(image_path, model, tumor_types=None):\n",
        "    \"\"\"Visualize model features\"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ])\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    original_image = np.array(image.resize((224, 224)))\n",
        "\n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Get features and prediction\n",
        "    feature_extractor = FeatureExtractor(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor.get_features(input_tensor)\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "    # Process features\n",
        "    processed_features = [process_feature_map(f) for f in features]\n",
        "\n",
        "    # Visualization\n",
        "    num_features = len(processed_features)\n",
        "    rows = (num_features + 3) // 4\n",
        "    cols = min(4, num_features + 1)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 5 * rows))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(rows, cols, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Feature visualizations\n",
        "    if processed_features:\n",
        "        # Combined features\n",
        "        combined_features = np.mean(processed_features, axis=0)\n",
        "        plt.subplot(rows, cols, 2)\n",
        "        plt.imshow(original_image)\n",
        "        plt.imshow(combined_features, cmap='jet', alpha=0.5)\n",
        "        plt.title('Combined Features')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Individual features\n",
        "        for idx, feature in enumerate(processed_features):\n",
        "            plt.subplot(rows, cols, idx + 3)\n",
        "            plt.imshow(original_image)\n",
        "            plt.imshow(feature, cmap='jet', alpha=0.5)\n",
        "            plt.title(f'Stage {idx+1} Features')\n",
        "            plt.axis('off')\n",
        "\n",
        "    if tumor_types:\n",
        "        pred_label = tumor_types[predicted_class]\n",
        "        pred_prob = probs[0][predicted_class].item()\n",
        "        plt.suptitle(f'LeViT Feature Visualization\\nPrediction: {pred_label} ({pred_prob:.3f})',\n",
        "                    fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return processed_features, predicted_class, probs\n",
        "\n",
        "def plot_individual_feature(image_path, model, feature_idx=None):\n",
        "    \"\"\"Plot a specific feature map in detail\"\"\"\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ])\n",
        "\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    original_image = np.array(image.resize((224, 224)))\n",
        "\n",
        "    # Get features\n",
        "    extractor = FeatureExtractor(model)\n",
        "    with torch.no_grad():\n",
        "        features = extractor.get_features(input_tensor)\n",
        "\n",
        "    # Process feature\n",
        "    if feature_idx is None:\n",
        "        # Use combined features\n",
        "        processed_features = [process_feature_map(f) for f in features]\n",
        "        feature_map = np.mean(processed_features, axis=0)\n",
        "        title = 'Combined Features'\n",
        "    else:\n",
        "        feature_map = process_feature_map(features[feature_idx])\n",
        "        title = f'Stage {feature_idx + 1} Features'\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(feature_map, cmap='jet')\n",
        "    plt.title(f'{title} Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(original_image)\n",
        "    plt.imshow(feature_map, cmap='jet', alpha=0.5)\n",
        "    plt.title(f'{title} Overlay')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Try running:\n",
        "# features, pred_class, probabilities = visualize_features(\n",
        "#     image_path='/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0286.jpg',\n",
        "#     model=levit_model,\n",
        "#     tumor_types=tumor_types\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLYsEdUUcaWA",
      "metadata": {
        "id": "KLYsEdUUcaWA"
      },
      "outputs": [],
      "source": [
        "features, pred_class, probabilities = visualize_features(\n",
        "    image_path='/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0286.jpg',\n",
        "    model=levit_model,\n",
        "    tumor_types=tumor_types\n",
        ")\n",
        "\n",
        "# To see a specific stage in detail:\n",
        "plot_individual_feature(\n",
        "    image_path='/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0286.jpg',\n",
        "    model=levit_model,\n",
        "    feature_idx=1  # Try different indices\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clgwOFYrdkUK",
      "metadata": {
        "id": "clgwOFYrdkUK"
      },
      "outputs": [],
      "source": [
        "# Load the image again for the heatmap\n",
        "image = Image.open('/content/drive/MyDrive/Colab Notebooks/Resized_Testing/glioma/Te-gl_0286.jpg').convert('RGB')\n",
        "original_image = np.array(image.resize((224, 224)))\n",
        "\n",
        "# Plot combined attention heatmap\n",
        "plot_attention_heatmap(attention_maps, original_image)\n",
        "\n",
        "# Plot specific layer (e.g., layer 0)\n",
        "plot_attention_heatmap(attention_maps, original_image, layer_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fp9AyFSGNp7o",
      "metadata": {
        "id": "fp9AyFSGNp7o"
      },
      "source": [
        "# Saliency Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-dbPouOmOdeu",
      "metadata": {
        "id": "-dbPouOmOdeu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "class SaliencyMap:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_saliency_map(self, input_image, target_class):\n",
        "        input_image.requires_grad_()\n",
        "\n",
        "        # Forward pass\n",
        "        output = self.model(input_image)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of the target class score with respect to the input image\n",
        "        target_score = output[0][target_class]\n",
        "        target_score.backward()\n",
        "\n",
        "        # Generate saliency map\n",
        "        saliency_map = input_image.grad.data.abs().sum(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        # Normalize the saliency map\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "def apply_saliency_map(image, saliency_map):\n",
        "    saliency_map = cv2.resize(saliency_map, (image.shape[1], image.shape[0]))\n",
        "    saliency_map = np.uint8(255 * saliency_map)\n",
        "    heatmap = cv2.applyColorMap(saliency_map, cv2.COLORMAP_JET)\n",
        "    superimposed_img = cv2.addWeighted(image, 0.6, heatmap, 0.4, 0)\n",
        "    return superimposed_img\n",
        "\n",
        "def visualize_saliency(model, image_path, target_class, tumor_types):\n",
        "    # Preprocess the image\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = preprocess(img).unsqueeze(0)\n",
        "\n",
        "    # Initialize SaliencyMap\n",
        "    saliency = SaliencyMap(model)\n",
        "\n",
        "    # Generate saliency map\n",
        "    saliency_map = saliency.generate_saliency_map(input_tensor, target_class)\n",
        "\n",
        "    # Apply saliency map to original image\n",
        "    original_image = cv2.imread(image_path)\n",
        "    if original_image is None:\n",
        "        print(f\"Error: Could not read image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    result = apply_saliency_map(original_image, saliency_map)\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f'Saliency Map: {tumor_types[target_class]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_random_images(model, test_folder, tumor_types):\n",
        "    for i, tumor_type in enumerate(tumor_types):\n",
        "        tumor_folder = os.path.join(test_folder, tumor_type)\n",
        "        image_files = [f for f in os.listdir(tumor_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "        if image_files:\n",
        "            random_image = random.choice(image_files)\n",
        "            image_path = os.path.join(tumor_folder, random_image)\n",
        "            visualize_saliency(model, image_path, i, tumor_types)\n",
        "        else:\n",
        "            print(f\"No images found for {tumor_type}\")\n",
        "\n",
        "# Usage\n",
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Assuming 'model' is your loaded ResNet50 model\n",
        "visualize_random_images(model, test_folder, tumor_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PNOW7HqVEtON",
      "metadata": {
        "collapsed": true,
        "id": "PNOW7HqVEtON"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p2WybMsBEcjl",
      "metadata": {
        "id": "p2WybMsBEcjl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "class LIMEExplainer:\n",
        "    def __init__(self, model, preprocess):\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def predict(self, images):\n",
        "        batch = torch.stack([self.preprocess(Image.fromarray(img.astype('uint8'))) for img in images])\n",
        "        logits = self.model(batch)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        return probs.detach().numpy()\n",
        "\n",
        "    def explain(self, image, target_class):\n",
        "        explainer = lime_image.LimeImageExplainer()\n",
        "        explanation = explainer.explain_instance(\n",
        "            image,\n",
        "            self.predict,\n",
        "            top_labels=5,\n",
        "            hide_color=0,\n",
        "            num_samples=1000\n",
        "        )\n",
        "\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            target_class,\n",
        "            positive_only=True,\n",
        "            num_features=5,\n",
        "            hide_rest=True\n",
        "        )\n",
        "\n",
        "        return mark_boundaries(temp / 255.0, mask)\n",
        "\n",
        "def visualize_lime(model, image_path, target_class, tumor_types, preprocess):\n",
        "    # Load and preprocess the image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = preprocess(img).unsqueeze(0)\n",
        "\n",
        "    # Initialize LIMEExplainer\n",
        "    lime_explainer = LIMEExplainer(model, preprocess)\n",
        "\n",
        "    # Generate LIME explanation\n",
        "    image_array = np.array(img)\n",
        "    lime_explanation = lime_explainer.explain(image_array, target_class)\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(lime_explanation)\n",
        "    plt.title(f'LIME Explanation: {tumor_types[target_class]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_random_images(model, test_folder, tumor_types, preprocess):\n",
        "    for i, tumor_type in enumerate(tumor_types):\n",
        "        tumor_folder = os.path.join(test_folder, tumor_type)\n",
        "        image_files = [f for f in os.listdir(tumor_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "        if image_files:\n",
        "            random_image = random.choice(image_files)\n",
        "            image_path = os.path.join(tumor_folder, random_image)\n",
        "            visualize_lime(model, image_path, i, tumor_types, preprocess)\n",
        "        else:\n",
        "            print(f\"No images found for {tumor_type}\")\n",
        "\n",
        "# Usage\n",
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Define preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Assuming 'model' is your loaded ResNet50 model\n",
        "visualize_random_images(model, test_folder, tumor_types, preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "etaghXfbH5WI",
      "metadata": {
        "id": "etaghXfbH5WI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "class IntegratedGradients:\n",
        "    def __init__(self, model, preprocess):\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def generate_integrated_gradients(self, input_image, target_class, steps=50):\n",
        "        self.model.eval()\n",
        "\n",
        "        # Generate alpha values\n",
        "        alphas = torch.linspace(0, 1, steps)\n",
        "\n",
        "        # Generate scaled inputs\n",
        "        scaled_inputs = [input_image * alpha for alpha in alphas]\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = []\n",
        "        for scaled_input in scaled_inputs:\n",
        "            scaled_input.requires_grad_(True)\n",
        "            output = self.model(scaled_input)\n",
        "            score = output[0][target_class]\n",
        "            self.model.zero_grad()\n",
        "            score.backward()\n",
        "            grads.append(scaled_input.grad.cpu().detach().numpy())\n",
        "            scaled_input.requires_grad_(False)\n",
        "\n",
        "        # Compute integral\n",
        "        avg_grads = np.average(grads, axis=0)\n",
        "        integrated_gradients = (input_image.cpu().detach().numpy() - scaled_inputs[0].cpu().detach().numpy()) * avg_grads\n",
        "\n",
        "        return integrated_gradients[0].sum(axis=0)\n",
        "\n",
        "def normalize_image(image):\n",
        "    return (image - image.min()) / (image.max() - image.min())\n",
        "\n",
        "def apply_colormap(image):\n",
        "    color_image = cv2.applyColorMap(np.uint8(255 * image), cv2.COLORMAP_JET)\n",
        "    return cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def visualize_integrated_gradients(model, image_path, target_class, tumor_types, preprocess):\n",
        "    # Load and preprocess the image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = preprocess(img).unsqueeze(0)\n",
        "\n",
        "    # Initialize IntegratedGradients\n",
        "    ig = IntegratedGradients(model, preprocess)\n",
        "\n",
        "    # Generate Integrated Gradients\n",
        "    attributions = ig.generate_integrated_gradients(input_tensor, target_class)\n",
        "\n",
        "    # Normalize and apply colormap\n",
        "    normalized_attributions = normalize_image(attributions)\n",
        "    attribution_heatmap = apply_colormap(normalized_attributions)\n",
        "\n",
        "    # Resize heatmap to match original image size\n",
        "    original_image = cv2.imread(image_path)\n",
        "    attribution_heatmap = cv2.resize(attribution_heatmap, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Superimpose heatmap on original image\n",
        "    superimposed_img = cv2.addWeighted(original_image, 0.6, attribution_heatmap, 0.4, 0)\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(attribution_heatmap)\n",
        "    plt.title('Integrated Gradients')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f'Superimposed: {tumor_types[target_class]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_random_images(model, test_folder, tumor_types, preprocess):\n",
        "    for i, tumor_type in enumerate(tumor_types):\n",
        "        tumor_folder = os.path.join(test_folder, tumor_type)\n",
        "        image_files = [f for f in os.listdir(tumor_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "        if image_files:\n",
        "            random_image = random.choice(image_files)\n",
        "            image_path = os.path.join(tumor_folder, random_image)\n",
        "            visualize_integrated_gradients(model, image_path, i, tumor_types, preprocess)\n",
        "        else:\n",
        "            print(f\"No images found for {tumor_type}\")\n",
        "\n",
        "# Usage remains the same as before\n",
        "\n",
        "# Usage\n",
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/Resized_Testing'\n",
        "tumor_types = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Define preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Assuming 'model' is your loaded ResNet50 model\n",
        "visualize_random_images(model, test_folder, tumor_types, preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xHYbSpo1430Y",
      "metadata": {
        "id": "xHYbSpo1430Y"
      },
      "source": [
        "## Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WFRBRjiKCT-y",
      "metadata": {
        "id": "WFRBRjiKCT-y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import time\n",
        "\n",
        "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, num_epochs=1, patience=1):\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        # ... (training loop code here)\n",
        "\n",
        "        model.eval()\n",
        "        # ... (validation loop code here)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_metrics = evaluate_model(model, test_loader)\n",
        "\n",
        "    return model, best_val_acc, training_time, test_metrics\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    avg_loss = total_loss / total\n",
        "\n",
        "    return {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'Cross-entropy Loss': avg_loss\n",
        "    }\n",
        "\n",
        "# Assuming you have your data loaders and model initialization set up\n",
        "results = []\n",
        "k_folds = 2\n",
        "\n",
        "for fold in range(k_folds):\n",
        "    model = initialize_model(num_classes)\n",
        "    train_loader, val_loader = get_fold_loaders(fold)  # You'll need to implement this function\n",
        "    model, val_acc, train_time, _ = train_and_evaluate_model(model, train_loader, val_loader, val_loader)\n",
        "    results.append({\n",
        "        'Fold': fold + 1,\n",
        "        'Validation Accuracy': val_acc,\n",
        "        'Training Time (s)': train_time\n",
        "    })\n",
        "\n",
        "# Final training on entire dataset\n",
        "full_train_loader = get_full_train_loader()  # You'll need to implement this function\n",
        "test_loader = get_test_loader()  # You'll need to implement this function\n",
        "final_model = initialize_model(num_classes)\n",
        "final_model, final_val_acc, final_train_time, final_test_metrics = train_and_evaluate_model(final_model, full_train_loader, test_loader, test_loader)\n",
        "\n",
        "results.append({\n",
        "    'Fold': 'Final',\n",
        "    'Validation Accuracy': final_val_acc,\n",
        "    'Training Time (s)': final_train_time,\n",
        "    **final_test_metrics\n",
        "})\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Add model size and number of parameters\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/final_mri_classification_model.pth'\n",
        "model_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
        "num_params = sum(p.numel() for p in final_model.parameters())\n",
        "\n",
        "df.loc[df['Fold'] == 'Final', 'Number of Parameters'] = num_params\n",
        "df.loc[df['Fold'] == 'Final', 'Model Size (MB)'] = model_size\n",
        "\n",
        "# Reorder columns for better readability\n",
        "column_order = ['Fold', 'Validation Accuracy', 'Accuracy', 'F1 Score', 'Cross-entropy Loss',\n",
        "                'Training Time (s)', 'Number of Parameters', 'Model Size (MB)']\n",
        "df = df.reindex(columns=[col for col in column_order if col in df.columns])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}